Index,Research areas,Bio,Research interests,Achievements and awards,Full name
aaron-sloman,Mathematical physics; Robotics; Control theory; Systems theory; Human computer interface; Operating systems; Computer vision; Natural language processing; Cognitive science; Ethics; Linguistics; Causality; Geometry & topology; Number theory; ,"Aaron Sloman's first degree was in mathematics and physics; at CapeTown in 1956. A Rhodes Scholarship brought him to Oxford in 1957; where; after meeting philosophers with incorrect beliefs about mathematics; he switched and in 1962 completed a DPhil in philosophy of mathematics; defending Kant's view of mathematics. He taught philosophy at Hull then; from 1964; at Sussex University. In 1969 he was introduced to AI by Max Clowes; and decided that designing working AI systems was the best way to make progress in epistemology and philosophy of mind; language; mathematics; science etc. In 1971 he criticised the 1969 logicist AI manifesto by McCarthy and Hayes; and was invited to spend 1972-3 among AI researchers in Edinburgh; one of the few AI centres at the time; where he learnt about computer science; AI; cognitive science; started programming in earnest; and met leading researchers in AI; including international visitors.; Back at Sussex; he collaborated with Max Clowes; Margaret Boden and others; developing teaching and research in AI and computational cognitive science; and contributed to the Poplog AI development system; used for teaching and research in academe and industry. In 1978; he published The Computer Revolution in Philosophy: Philosophy science and models of mind (revised edition freely available online) He also helped to found the School of Cognitive and Computing Sciences (COGS). In 1991 he moved to Computer Science in Birmingham; helping to build up teaching and research in AI; including modelling of motivation and emotional architectures; and robotics. Retired in 2002; but continued full time research.; After encountering AI in 1969 Sloman suspected Kant's philosophy of mathematics could be defended using a design for a ""baby"" robot that could grow up to be a mathematician; like Archimedes; or Euclid; using mechanisms consistent with; but more precisely defined; than Kant's proposals. Nearly half a century later it's clear that nobody knows how to do that. An invitation to contribute to the Elsevier Turing Centenary Volume (https://goo.gl/9eN8Ks); including commenting on Turing's amazing Morphogenesis paper; led him to ask why Turing had written about chemical morphogenesis; and what he would have done had he lived several more decades.; The conjectured answer was a proposed multi-strand Meta-Morphogenesis (M-M) project on which Sloman has been working (unfunded) ever since; trying to understand how a physical universe can make it possible for biological evolution to produce the minds of ancient mathematicians (long before modern logic; algebra and cartesian representations of geometry); and other species with high levels of spatial intelligence; e.g. nest-building birds; squirrels; elephants and many others; abilities still completely lacking in current AI and unexplained by psychology and neuroscience (though often mis-described).; ""Meta-"" indicates that evolution's products repeatedly extend the powers of evolution; mainly through production of new construction-kits; all based on; but different from; the fundamental physical construction kit. Turing suggested in 1938 that mathematical discovery needs both intuition and ingenuity and suggested that Turing-machines can provide ingenuity (if suitably programmed) but not intuition. Sloman's research project will explore ways of modifying the Turing-Machine specification to fill the gap: a multi-membrane-Super-Turing machine; perhaps using sub-neural chemistry.; Sloman's 1962 thesis remains relevant; originally unpublished; but now freely available online; along with papers in philosophy of science; meta-ethics; semantics; philosophy of language; philosophy of information; philosophy of mathematics; philosophy of mind; vision (e.g. including functions of biological vision); affective phenomena - emotions; moods; etc. (topics seriously misconstrued by AI modellers and others); consciousness; evolution; epigenomics (jointly with Jackie Chappell) and programming.; His 1978 book (CRP); a progress report; made predictions about uses of computers that have recently come true; and discussed philosophical and psychological problems to be solved through (not yet achieved) developments in AI. The Epilogue disposed of the ""AI Singularity"" problem (still not on the horizon; despite misinformed dire predictions!). The Poplog development system; to which he contributed ideas; code; documentation and management; eventually became a successful commercial product; winning a UK ""Smart"" award for $5Million sales: now a free; open source system; including the Birmingham SimAgent toolkit.; CRP and work on Poplog led to invitations and collaborations; including a two year professorial fellowship from GEC. He was elected a fellow of AAAI; then a fellow of AISB; and a fellow of ECAI (Now EURAI). In 2008 Sussex University awarded him an honorary DSc. Since starting the Turing-inspired Meta-Morphogenesis project in 2012; working alone; and unfunded; he has received international conference/workshop invitations; has given tutorials (e.g. Edinburgh ESSENCE school; IJCAI 2016; the PACS Symposium Seoul 2016; Diagrams 2018) and published progress reports on the project. There remain huge problems. Deep learning won't help. Perhaps chemistry will??; ","After encountering AI in 1969 Sloman suspected Kant's philosophy of mathematics could be defended using a design for a ""baby"" robot that could grow up to be a mathematician; like Archimedes; or Euclid; using mechanisms consistent with; but more precisely defined; than Kant's proposals. Nearly half a century later it's clear that nobody knows how to do that. An invitation to contribute to the Elsevier Turing Centenary Volume (https://goo.gl/9eN8Ks); including commenting on Turing's amazing Morphogenesis paper; led him to ask why Turing had written about chemical morphogenesis; and what he would have done had he lived several more decades.; The conjectured answer was a proposed multi-strand Meta-Morphogenesis (M-M) project on which Sloman has been working (unfunded) ever since; trying to understand how a physical universe can make it possible for biological evolution to produce the minds of ancient mathematicians (long before modern logic; algebra and cartesian representations of geometry); and other species with high levels of spatial intelligence; e.g. nest-building birds; squirrels; elephants and many others; abilities still completely lacking in current AI and unexplained by psychology and neuroscience (though often mis-described).; ""Meta-"" indicates that evolution's products repeatedly extend the powers of evolution; mainly through production of new construction-kits; all based on; but different from; the fundamental physical construction kit. Turing suggested in 1938 that mathematical discovery needs both intuition and ingenuity and suggested that Turing-machines can provide ingenuity (if suitably programmed) but not intuition. Sloman's research project will explore ways of modifying the Turing-Machine specification to fill the gap: a multi-membrane-Super-Turing machine; perhaps using sub-neural chemistry.; ","Sloman's 1962 thesis remains relevant; originally unpublished; but now freely available online; along with papers in philosophy of science; meta-ethics; semantics; philosophy of language; philosophy of information; philosophy of mathematics; philosophy of mind; vision (e.g. including functions of biological vision); affective phenomena - emotions; moods; etc. (topics seriously misconstrued by AI modellers and others); consciousness; evolution; epigenomics (jointly with Jackie Chappell) and programming.; His 1978 book (CRP); a progress report; made predictions about uses of computers that have recently come true; and discussed philosophical and psychological problems to be solved through (not yet achieved) developments in AI. The Epilogue disposed of the ""AI Singularity"" problem (still not on the horizon; despite misinformed dire predictions!). The Poplog development system; to which he contributed ideas; code; documentation and management; eventually became a successful commercial product; winning a UK ""Smart"" award for $5Million sales: now a free; open source system; including the Birmingham SimAgent toolkit.; CRP and work on Poplog led to invitations and collaborations; including a two year professorial fellowship from GEC. He was elected a fellow of AAAI; then a fellow of AISB; and a fellow of ECAI (Now EURAI). In 2008 Sussex University awarded him an honorary DSc. Since starting the Turing-inspired Meta-Morphogenesis project in 2012; working alone; and unfunded; he has received international conference/workshop invitations; has given tutorials (e.g. Edinburgh ESSENCE school; IJCAI 2016; the PACS Symposium Seoul 2016; Diagrams 2018) and published progress reports on the project. There remain huge problems. Deep learning won't help. Perhaps chemistry will??; ",Professor Aaron Sloman 
adam-brentnall,Pattern recognition; Causality; Simulation; ,Dr Adam Brentnall is a statistician at the Centre for Cancer Prevention; Queen Mary University of London. Much of his research is focused on statistical modelling of cancer risk and related strategies for early detection and prevention. Methodological interests include standardization; accounting for unplanned crossover in clinical trials; and measures of model performance.; Dr Brentnall's research at Turing will include investigating ways to provide a better comprehensive cancer risk assessment than current models; by building on collaborative work in the area. Improvements are likely involve combining information from genetic factors; imaging; biomarkers and other risk factors. A major interest is to assess the potential for Artificial Intelligence or other algorithmic methods to improve risk assessment; particularly using imaging data. Methodological work to help better understand evidence supporting risk predictions or new treatments will also be undertaken.; ,Dr Brentnall's research at Turing will include investigating ways to provide a better comprehensive cancer risk assessment than current models; by building on collaborative work in the area. Improvements are likely involve combining information from genetic factors; imaging; biomarkers and other risk factors. A major interest is to assess the potential for Artificial Intelligence or other algorithmic methods to improve risk assessment; particularly using imaging data. Methodological work to help better understand evidence supporting risk predictions or new treatments will also be undertaken.; ,['N/A'],Dr Adam Brentnall 
adam-sanborn,Multi-agent reasoning; Neuroscience; Human computer interface; Cognitive science; Research methods; High dimensional inference; Monte Carlo methods; ,Adam Sanborn is Associate Professor of Psychology at the University of Warwick. He gained his PhD in psychological & brain sciences and cognitive science at Indiana University; and did his postdoctoral work at the Gatsby Computational Neuroscience Unit at UCL. Adam is interested in the rationality of human behaviour; which he studies with Bayesian models; approximations to Bayesian models; and behavioural experiments. His research has been published in leading psychology journals such as Psychological Review; Trends in Cognitive Sciences; and Cognitive Psychology; and he has won best paper awards in both psychology and computer science. He recently completed a three-year project funded by the ESRC and serves as Associate Editor for the Journal of Experimental Psychology: Learning; Memory; and Cognition.; Eliciting human beliefs in uncertain situations is a key challenge for data science and artificial intelligence; as people are often biased when expressing their subjective probabilities of events occurring. Many methods of elicitation have been developed by psychologists and statisticians that attempt to avoid these biases; but these methods do not scale well to eliciting multivariate beliefs. Adam's past work has adapted a sampling algorithm from computer science and statistics; Markov Chain Monte Carlo; to elicit people's multivariate beliefs by asking them to make a series of choices.; Adam has successfully applied this approach to a range of domains; such as eliciting what people believe different animals or facial expressions look like; or what people mean when they say they have had a good night's sleep. At the Turing; he plans to explore additional domains in economic data science in order to relate people's employment choices to their beliefs about the future. He also plans to develop new elicitation methods for efficiently combining human expertise with that of artificial intelligence.; ,Eliciting human beliefs in uncertain situations is a key challenge for data science and artificial intelligence; as people are often biased when expressing their subjective probabilities of events occurring. Many methods of elicitation have been developed by psychologists and statisticians that attempt to avoid these biases; but these methods do not scale well to eliciting multivariate beliefs. Adam's past work has adapted a sampling algorithm from computer science and statistics; Markov Chain Monte Carlo; to elicit people's multivariate beliefs by asking them to make a series of choices.; Adam has successfully applied this approach to a range of domains; such as eliciting what people believe different animals or facial expressions look like; or what people mean when they say they have had a good night's sleep. At the Turing; he plans to explore additional domains in economic data science in order to relate people's employment choices to their beliefs about the future. He also plans to develop new elicitation methods for efficiently combining human expertise with that of artificial intelligence.; ,['N/A'],Dr Adam Sanborn 
adam-tsakalidis,Natural language processing; Social media; ,"Adam is a PhD candidate in Urban Science with the Warwick Institute for the Science of Cities; supervised by Dr. Alexandra I. Cristea and Dr. Maria Liakata. His main research interests are in the areas of Social Media Mining and Natural Language Processing; with a primary focus on sentiment analysis from social media and their implications in real-world events.; Over 70% of the global population is expected to live in big cities by the next 30 years. At the same time; the use of social media is growing at tremendous rates. Motivated by these basic facts; Adam's work focuses on opinion mining of social media users; as an attempt to model and predict urban-related indices (e.g.; elections; wellbeing; etc.). The effective modelling of such domains is a challenging and important task; as it can provide additional ""sensors"" for informing citizens and policy makers on the current state of an urban environment; but also on possible consequences of their actions.; ","Over 70% of the global population is expected to live in big cities by the next 30 years. At the same time; the use of social media is growing at tremendous rates. Motivated by these basic facts; Adam's work focuses on opinion mining of social media users; as an attempt to model and predict urban-related indices (e.g.; elections; wellbeing; etc.). The effective modelling of such domains is a challenging and important task; as it can provide additional ""sensors"" for informing citizens and policy makers on the current state of an urban environment; but also on possible consequences of their actions.; ",['N/A'], Adam Tsakalidis 
adria-gascon,Cryptography (Algorithms); Cryptography (Privacy & trust); ,Adria Gascon is a computer scientist with research interests in formal languages and compression; automated reasoning; cryptography; and machine learning. He earned his PhD from the Technical University of Catalonia; and has held positions at SRI International and the University of Edinburgh.; Data is a valuable asset; as it is crucial to many decision making processes. In this paradigm; privacy is an important aspect of each component of a data analysis pipeline. Similarly to general security; privacy has proven to be a slippery concept; that requires a robust; mathematically rigorous approach.; Adria's research focus is on finding efficient solutions to the problem of computing on private data; that provide formal guarantees regarding information disclosure. This research touches on several areas of expertise and application domains; such as machine learning and statistics; cryptography; formal methods; databases; and systems security.; ,Data is a valuable asset; as it is crucial to many decision making processes. In this paradigm; privacy is an important aspect of each component of a data analysis pipeline. Similarly to general security; privacy has proven to be a slippery concept; that requires a robust; mathematically rigorous approach.; Adria's research focus is on finding efficient solutions to the problem of computing on private data; that provide formal guarantees regarding information disclosure. This research touches on several areas of expertise and application domains; such as machine learning and statistics; cryptography; formal methods; databases; and systems security.; ,['N/A'],Dr Adrià Gascón 
adrian-bevan,Neural networks; Deep learning; Modelling (Statistical methods & theory); ,"Dr Adrian Bevan is head of the Particle Physics Research Centre at Queen Mary University of London. He uses a variety of data science techniques for his day-to day research in order to learn about the sub-atomic nature of particles. His research interests include understanding the matter-antimatter asymmetry in the Universe and developing a deeper understanding of the Higgs boson. He also works on the development of semiconductor sensor devices for operation in novel experiments and the Large Hadron Collider at CERN.; Dr Adrian Bevan participates in Big Data-based experiments at international laboratories and specialises in parameter estimation through complex modelling and simulation of data; and has almost two decades of experience in this area. Having worked on experiments at international laboratories in Switzerland (CERN and PSI); the US (SLAC National Accelerator Laboratory) and Italy (Frascati) Many of the research topics of interest have significant amounts of background ""noise"" obscuring the signal and as a result Adrian has had to use machine learning techniques in order to identify signal for subsequent analysis. ; Dr Adrian Bevan has written a book on Statistical Data Analysis and edited a book on The Physics of the B Factories. ; ","Dr Adrian Bevan participates in Big Data-based experiments at international laboratories and specialises in parameter estimation through complex modelling and simulation of data; and has almost two decades of experience in this area. Having worked on experiments at international laboratories in Switzerland (CERN and PSI); the US (SLAC National Accelerator Laboratory) and Italy (Frascati) Many of the research topics of interest have significant amounts of background ""noise"" obscuring the signal and as a result Adrian has had to use machine learning techniques in order to identify signal for subsequent analysis. ; ",Dr Adrian Bevan has written a book on Statistical Data Analysis and edited a book on The Physics of the B Factories. ; ,Dr Adrian Bevan 
adriane-chapman,Data structures; Data science of government & politics; Privacy & trust; Identity management; Databases; ,Adriane Chapman is an Associate Professor in Electronics and Computer Science at the University of Southampton. She earned her PhD at the University of Michigan in 2008; and joined The MITRE Corporation; a non-profit Federally Funded Research and Development Company (FFRDC) that researched and applied solutions to technology problems in the US government. She joined the University of Southampton in 2016. She is Co-Director of the ECS Centre for Health Technologies; and is passionate about applying technology for society's benefit. She is currently working on the following projects: digital predictive technologies and conduits of algorithmic bias in policing and penal services; enabling personalized expression of privacy requirements. ; Adriane's research interests include provenance; data science pipelines; and dataset search. ; ,Adriane's research interests include provenance; data science pipelines; and dataset search. ; ,['N/A'],Dr Adriane Chapman 
aidan-osullivan,Multi-agent systems; Robotics; Neural networks; Neuroscience; Natural language processing; Pattern recognition; Reinforcement learning; Graph theory; Identity management; Uncertainty quantification; Probability; ,Aidan O'Sullivan is a lecturer at the UCL Energy Institute; University College London. He is course director for the MSc. in Energy Systems and Data Analytics and head of the Energy and Artificial Intelligence group. Prior to joining UCL he was awarded a PhD. from the department of Mathematics in Imperial College London in 2013 for his thesis on Bayesian statistics. He joined MIT as a postdoc in the Intelligent Transport Systems group where he worked on transport modelling problems using advanced artificial intelligence methods. He has been co-investigator on a number of large scale research projects; collaborations involving multiple universities; including the UK Research Council funded projects. 'System Aspects of Electric Commercial Aircraft (SAECA)'; project value £250;000; and 'Airport Capacity Consequences Leveraging Aviation Integrated Modelling (ACCLAIM)'; total project value £1;500;000.; His publications span a wide range of areas but with the common focus of how advanced data science and AI methodologies can be deployed in aerospace and the energy system for sustainability. Examples include the development of models for the global aviation system; the future of electric aircraft and the role of big data and AI in energy systems modelling His work has also been featured in a recent article on Artificial Intelligence in the Times Raconteur; 'How AI can help meet global energy demand'.; Aidan's research at the Turing will be focused on how artificial intelligence methods can be deployed to improve the efficiency of the energy system; from how electricity is generated to how the National grid is operated.; ,Aidan's research at the Turing will be focused on how artificial intelligence methods can be deployed to improve the efficiency of the energy system; from how electricity is generated to how the National grid is operated.; ,['N/A'],Dr Aidan O'Sullivan 
alain-zemkoho,Complexity (Algorithms); Numerical (Algorithms); Applied mathematics; Operations research; Neural networks; Machine learning; Applications (Machine learning); Deep learning; Reinforcement learning; Semi-supervised learning; Supervised learning; Unsupervised learning; Optimisation; Convex programming; Nonlinear programming; Time series; ,Alain is a Lecturer within the Southampton School of Mathematical Sciences; where he is affiliated to CORMSIS - Centre for Operational Research; Management Science & Information Systems and the Operational Research Group. Prior to joining the University of Southampton; he was a Research Fellow at the University of Birmingham and had previously worked as a Research Associate at the Technical University of Freiberg; where he also completed his PhD in Mathematics. He is Fellow of the Institute of Mathematics and its Applications; a Member of the EPSRC Mathematical Sciences Early Career Forum (2017 - 2019); a regular member of the Society for Industrial and Applied Mathematics (SIAM); the Mathematical Optimization Society (MOS); and The Operational Research Society. ; Alain's research interests are in continuous optimization; with special focus on bilevel optimization and its applications to transportation; data analysis; forecasting; and phase retrieval. He has developed optimality conditions and stability/sensitivity analysis results for such problems and is currently developing numerical algorithms that can capture the optimistic and pessimistic features of bilevel optimization problems.; ,Alain's research interests are in continuous optimization; with special focus on bilevel optimization and its applications to transportation; data analysis; forecasting; and phase retrieval. He has developed optimality conditions and stability/sensitivity analysis results for such problems and is currently developing numerical algorithms that can capture the optimistic and pessimistic features of bilevel optimization problems.; ,['N/A'],Dr Alain Zemkoho 
alan-brown,Privacy & trust; Software framework development; Data science of government & politics; ,Alan W. Brown is Professor in Digital Economy at the University of Exeter’s Business School and co-founder of the Initiative in Digital Economy at Exeter (INDEX). After receiving a PhD in Computational Science at the University of Newcastle-upon-Tyne; Alan spent almost 20 years in the USA in commercial high-tech companies leading R&D teams; building leading-edge solutions; and driving innovation in software product delivery. He then spent 5 years in Madrid leading enterprise strategy as European CTO for IBM’s Software group. Most recently Alan co-founded the Surrey Centre for the Digital Economy (CoDE) at the University of Surrey where he led research activities in 4 EPSRC-funded research projects.; The impact of Alan’s research is seen in his consulting activities where he advises several startups; and works with clients in the public and private sector including the UK National Audit Office; Centrica; SAP; Resonate; McLaren; and several UK Government agencies. Additionally; he has published 5 books and numerous papers on software engineering; systems design; and digital business transformation. ; Alan’s research is focused on agile approaches to business transformation; and the relationship between technology innovation and business innovation in today’s rapidly evolving digital economy. His current research projects involve the adoption of digital technology in business; the effect of digital disruption in emerging business models; and the development of new management strategies and practices for data-driven innovation.; ,Alan’s research is focused on agile approaches to business transformation; and the relationship between technology innovation and business innovation in today’s rapidly evolving digital economy. His current research projects involve the adoption of digital technology in business; the effect of digital disruption in emerging business models; and the development of new management strategies and practices for data-driven innovation.; ,['N/A'],Professor Alan Brown 
aleksandar-mijatovic,Numerical analysis; Stochastic optimisation; Simulation; Probability; Geometry & topology; ,Alex Mijatovic is the Strategic Leader for the Mathematical Foundations theme of the Turing's programme in data-centric engineering and a Turing Fellow. Alex is a Professor in the Statistics Department of the University of Warwick.; He has broad research interests ranging from probability to topology; and including numerical stochastics; optimisation; and simulation. He is a pure mathematician with a background in industry and a long-standing focus on applications: in financial markets; energy systems; and engineering.; ,He has broad research interests ranging from probability to topology; and including numerical stochastics; optimisation; and simulation. He is a pure mathematician with a background in industry and a long-standing focus on applications: in financial markets; energy systems; and engineering.; ,['N/A'],Professor Aleksandar Mijatović 
ales-leonardis,Computer vision; Cognitive science; ,Ales Leonardis is Chair of Robotics at the School of Computer Science; University of Birmingham and Co-Director of the Computational Neuroscience and Cognitive Robotics Centre at the University of Birmingham. He is also Professor of Computer and Information Science at the University of Ljubljana and an Adjunct Professor at the Faculty of Computer Science; Graz University of Technology. He was a visiting researcher at the GRASP Laboratory at the University of Pennsylvania; post-doctoral fellow at PRIP Laboratory; Vienna University of Technology; and visiting professor at ETH Zurich and University of Erlangen.; He has extensive experience in participating and managing multi-site international projects including eight European Commission (FP5; FP6; FP7 and H2020) projects; one DARPA project (Neovision2) and one ONR MURI project. He is actively involved in applied robotics and computer vision projects with nuclear (National Nuclear Laboratory) and industrial robotics industry (KUKA Robotics UK).; He was a Program Co-chair of the European Conference on Computer Vision 2006 and he has been an Associate Editor of the IEEE PAMI and IEEE Robotics and Automation Letters; an editorial board member of Pattern Recognition and Image and Vision Computing; and an editor of the Springer book series Computational Imaging and Vision. In 2002; he coauthored a paper; Multiple Eigenspaces; which won the 29th Annual Pattern Recognition Society award. He is a fellow of the IAPR and in 2004 he was awarded one of the two most prestigious national (SI) awards for his research achievements.; His research interests include robust and adaptive methods for computer/cognitive vision; object and scene recognition and categorisation; statistical visual learning; object tracking; and biologically motivated vision - all in a broader context of artificial cognitive systems and robotics.; ,His research interests include robust and adaptive methods for computer/cognitive vision; object and scene recognition and categorisation; statistical visual learning; object tracking; and biologically motivated vision - all in a broader context of artificial cognitive systems and robotics.; ,['N/A'],Professor Ales Leonardis 
alessandro-abate,['N/A'],Alessandro Abate is Professor of Verification and Control in the Department of Computer Science at the University of Oxford. Previously he has done research at SRI International and at Stanford University; and has been an Assistant Professor at the Delft Center for Systems and Control; TU Delft. He has received a Laurea degree from the University of Padova and a MS/PhD at UC Berkeley. ; For more details; please see his departmental page or his research group page.; Alessandro's research interests lie in the analysis; verification; and optimal control of heterogeneous and complex dynamical models -- in particular of stochastic hybrid systems -- and in their applications in cyber-physical systems (particularly involving energy networks) and in the life sciences (systems biology). He is interested in a principled integration of model-based mathematical techniques with data-driven learning algorithms.; ,Alessandro's research interests lie in the analysis; verification; and optimal control of heterogeneous and complex dynamical models -- in particular of stochastic hybrid systems -- and in their applications in cyber-physical systems (particularly involving energy networks) and in the life sciences (systems biology). He is interested in a principled integration of model-based mathematical techniques with data-driven learning algorithms.; ,['N/A'],Professor Alessandro Abate 
alessandro-vatri,['N/A'],Alessandro is working on a seed funded project which focuses on the computational models of meaning change in natural language texts. The project focuses on Bayesian models for change-point detection in multiple languages; stimulating collaborations with British Library and National Grid. Alessandro joins us from Oxford where he has been Postdoctoral Research Assistant in Comparative Philology and Non-Stipendiary Junior Research Fellow of Wolfson College since 2014.; ,['N/A'],['N/A'], Alessandro Vatri 
alex-bespalov,Dynamical systems & differential equations; Numerical analysis; Deep learning; Calculus & analysis; ,Dr Alex Bespalov is a Senior Lecturer in Applied Mathematics at the University of Birmingham. Alex obtained his PhD in Computational Mathematics from the Russian Academy of Sciences in 1999. Before joining the University of Birmingham; he held postdoctoral research positions at Universidad de Concepción (Chile); Brunel University; and the University of Manchester.; Alex specialises in numerical analysis and scientific computing. He is particularly interested in developing robust and accurate numerical algorithms for solving mathematical problems arising in uncertainty quantification and machine learning. His recent research is focused on the design; mathematical analysis; and efficient implementation of the state-of-the-art adaptive algorithms for numerical approximation of high-dimensional functions - solutions of partial differential equations with random input data.; ,Alex specialises in numerical analysis and scientific computing. He is particularly interested in developing robust and accurate numerical algorithms for solving mathematical problems arising in uncertainty quantification and machine learning. His recent research is focused on the design; mathematical analysis; and efficient implementation of the state-of-the-art adaptive algorithms for numerical approximation of high-dimensional functions - solutions of partial differential equations with random input data.; ,['N/A'],Dr Alex Bespalov 
alex-shestopaloff,['N/A'],Alex received his PhD in Statistics from the University of Toronto in 2016; with a thesis on Markov Chain Monte Carlo (MCMC) methods for Bayesian inference in complex stochastic models. Alex's research was supervised by Prof. Radford M. Neal.; Alex plans to continue his research in MCMC and related methods to further improve their efficiency and broaden their applicability. In particular; Alex is interested in looking at novel Sequential Monte Carlo methods for high-dimensional state space models as well as MCMC methods for sampling in discrete spaces.; ,Alex plans to continue his research in MCMC and related methods to further improve their efficiency and broaden their applicability. In particular; Alex is interested in looking at novel Sequential Monte Carlo methods for high-dimensional state space models as well as MCMC methods for sampling in discrete spaces.; ,['N/A'], Alex Shestopaloff 
alexander-harris,Data science of government & politics; ,Prior to joining the Turing; Alex held a joint appointment as a research analyst at a data science consultancy and at Tech Against Terrorism; a project working on behalf of the United Nations Counter-Terrorism Committee Executive Directorate (UN CTED) to support the global technology industry to tackle terrorist exploitation of their technologies.; Alex has a BA in History from King's College London and an MSc in International Relations from the London School of Economics and Political Science (LSE); awarded with distinction. During his time at the LSE; Alex interned at the Royal United Services Institute (RUSI) and the International Institute of Strategic Studies (IISS).; ,['N/A'],['N/A'], Alexander Harris 
alexandros-beskos,['N/A'],Dr Beskos obtained his BSc in Statistics at the Department of Statistics; Athens University in Economics and Business; in 2000. He received a PhD in Statistics from the Department of Mathematics and Statistics; Lancaster University; in 2005. He worked as a Post-Doctoral Researcher at the Mathematics Institute and the Department of Statistics from 2005 to 2008. Dr Beskos was appointed as a Lecturer in Statistics at the Department of Statistical Science; UCL in 2008; he was promoted to Senior Lecturer in 2013 and Reader in 2015.; His research interests span the areas of Computational Statistics; Bayesian Methods; Monte-Carlo Algorithms; Applied Mathematics; Inverse Problems; and Biostatistics. He has worked with a number of collaborators in projects involving applications in Finance; Econometrics; Physical Sciences; Genetics; Statistical Ecology and Energy.; Alexandros is a Turing Fellow through his supervision of Shouto Yonekura; a doctoral student at the Turing.; ,His research interests span the areas of Computational Statistics; Bayesian Methods; Monte-Carlo Algorithms; Applied Mathematics; Inverse Problems; and Biostatistics. He has worked with a number of collaborators in projects involving applications in Finance; Econometrics; Physical Sciences; Genetics; Statistical Ecology and Energy.; Alexandros is a Turing Fellow through his supervision of Shouto Yonekura; a doctoral student at the Turing.; ,['N/A'],Dr Alexandros Beskos 
alfredo-nazabal,['N/A'],Alfredo's work at the Turing is focused on the 'Artificial intelligence for data analytics' project. He completed his PhD at the Department of Signal Theory and Communications at the University Carlos III of Madrid. His work has focused on probabilistic machine learning and signal processing for time series data applied to human activity recognition. ; ,['N/A'],['N/A'], Alfredo Nazabal 
alison-heppenstall,Social networks; Probabilistic programming; Causality; Uncertainty quantification; Optimisation; Neural & evolutionary computing; Artificial intelligence; Neural networks; Pattern formation; Multi-agent systems; Graph theory; Dynamic/static (Mathematical modelling); Machine learning; Reinforcement learning; Pattern recognition; Deep learning; ,Alison Heppenstall is Professor of Geocomputation at the University of Leeds and an ESRC-Turing Fellow. She has a PhD in Artificial Intelligence; a focus of which was developing machine learning algorithms (e.g. neural networks) and AI approaches (agent-based models) that were applied to solving complex spatial problems. Most of her current research is focused on developing and adapting ML approaches to understanding social phenomena. She has particular interests in data analytics; developing approaches for detecting 'hidden' spatio-temporal patterns in 'big data'; quantifying uncertainty in simulations and building more robust individual-based models through probabilistic programming and reinforcement learning.; Professor Heppenstall is involved in several strands of work at the Turing. Her ESRC-Turing Fellowship is focused on (i) developing new ML tools for detecting hidden patterns and structures within cities and (ii) creating robust agent-based modelling simulations for Smart Cities; for example understanding the impact of new infrastructure or air pollution.  ; Professor Heppenstall is also involved in the two other Turing funded projects: The first project will develop methods that can be used to better understand uncertainty in individual-level models. In particular; it will explore and extend the state-of-the-art in two related areas: ensemble modelling and emulators for use in individual-level models. The second project will investigate linking together casual inference modelling with agent-based modelling. Can we improve our agent-based models through a better understanding of the relationships in large; dynamic data sets?; Professor Heppenstall holds an ESRC-Turing Fellowship.  ; ,Professor Heppenstall is involved in several strands of work at the Turing. Her ESRC-Turing Fellowship is focused on (i) developing new ML tools for detecting hidden patterns and structures within cities and (ii) creating robust agent-based modelling simulations for Smart Cities; for example understanding the impact of new infrastructure or air pollution.  ; Professor Heppenstall is also involved in the two other Turing funded projects: The first project will develop methods that can be used to better understand uncertainty in individual-level models. In particular; it will explore and extend the state-of-the-art in two related areas: ensemble modelling and emulators for use in individual-level models. The second project will investigate linking together casual inference modelling with agent-based modelling. Can we improve our agent-based models through a better understanding of the relationships in large; dynamic data sets?; ,Professor Heppenstall holds an ESRC-Turing Fellowship.  ; ,Professor Alison Heppenstall 
amy-wilson,Data science of government & politics; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Simulation; Time series; ,Amy Wilson is a researcher in statistics in the School of Mathematics at the University of Edinburgh. She previously worked as a research associate in the quantification of uncertainty for large scale energy models at Durham University. She has an MMath from the University of Cambridge and a PhD from the University of Edinburgh in statistical modelling for the evaluation of forensic evidence.; Amy Wilson is an applied statistician interested in decision-making under uncertainty. She has worked mainly on applications in energy and forensic science. In energy; she is interested in statistical methodology for long term planning of the electricity system. This has included work on modelling extremes for assessing the risk of electricity shortfalls and modelling uncertainty in large scale computer models. In forensic science she is interested in statistical modelling for the interpretation of forensic data and has developed techniques for the analysis of drug traces on banknotes.; ,Amy Wilson is an applied statistician interested in decision-making under uncertainty. She has worked mainly on applications in energy and forensic science. In energy; she is interested in statistical methodology for long term planning of the electricity system. This has included work on modelling extremes for assessing the risk of electricity shortfalls and modelling uncertainty in large scale computer models. In forensic science she is interested in statistical modelling for the interpretation of forensic data and has developed techniques for the analysis of drug traces on banknotes.; ,['N/A'],Dr Amy Wilson 
anastasia-papavasiliou,['N/A'],Dr Anastasia Papavasiliou got her PhD from Princeton University in 2002; working on stability questions for stochastic filtering and particle filters. Before coming to Warwick; she taught at Columbia University for a couple of years and spent another year in Princeton; working on efficient simulation methods for multiscale stochastic systems. Stochastic filtering; multiscale systems and; stochastic simulations are still some of her research interests; while recently she has working on applying ideas coming from the theory of rough paths to statistics.; Dr Papavasiliou is interested in investigating signature-based representations of data streams as alternative ways of describing information; possibly more efficiently.; ,Dr Papavasiliou is interested in investigating signature-based representations of data streams as alternative ways of describing information; possibly more efficiently.; ,['N/A'],Dr Anastasia Papavasiliou 
andre-carlos-ponce-de-leon-ferreira-de-carvalho,Robotics; Deep learning; Pattern recognition; Data science of government & politics; Ethics; Research methods; Causality; ,Full Professor in the Institute of Mathematics and Computer Sciences; University of São Paulo; Brazil. He was Associate Professor in the University of Guelph; Canada; Visiting Researcher in the University of Porto; Portugal and Visiting Professor in the University of Kent; UK. His main research interests are data science and machine learning. He is the vice-director of the Center for Mathematical Sciences Applied to Industry; University of São Paulo. He is in the Steering Committee of the International Network for Government Science Advice (INGSA) Latin American and Caribbean Chapter.; Andre's research areas are data science and machine learning; in particular AutoML; automating of machine learning; and data stream mining; for novelty and concept drift detection. The project to be carried out at The Alan Turing Institute will investigate current studies regarding transparency and fairness of decision-making by models induced by machine learning algorithms.; ,Andre's research areas are data science and machine learning; in particular AutoML; automating of machine learning; and data stream mining; for novelty and concept drift detection. The project to be carried out at The Alan Turing Institute will investigate current studies regarding transparency and fairness of decision-making by models induced by machine learning algorithms.; ,['N/A'], Andre Carlos Ponce de Leon Ferreira de Carvalho 
andre-neto-bradley,['N/A'],['N/A'],['N/A'],['N/A'], André Neto-Bradley 
andrea-baronchelli,Graph theory; ,Andrea Baronchelli is a Senior Lecturer in Mathematics at City University of London; a research fellow at the UCL Centre for Blockchain Technologies and a fellow at the ISI Foundation in Turin. He currently serves as Associate Editor at EPJ Data Science; PLoS ONE and Frontiers in Blockchain. Prior to joining City in 2013; he was at Northeastern University and at the Technical University of Catalonia (UPC) in Barcelona. He received his BS and MS in Theoretical Physics from the Sapienza University of Rome and his PhD in Physics from the same university.; His research in computational socio-economic science and network science has appeared in a wide range of journals; from Science; PNAS and Nature Human Behaviour to Physical Review Letters and Social Networks; and has been recognized by the 2019 “Young Scientist Award for Socio and Econophysics” of the German Physical Society (DPG). In London; he organise Data Natives and co-organises Databeers London and the Computational Social Science Initiative London.; ,His research in computational socio-economic science and network science has appeared in a wide range of journals; from Science; PNAS and Nature Human Behaviour to Physical Review Letters and Social Networks; and has been recognized by the 2019 “Young Scientist Award for Socio and Econophysics” of the German Physical Society (DPG). In London; he organise Data Natives and co-organises Databeers London and the Computational Social Science Initiative London.; ,['N/A'],Dr Andrea Baronchelli 
andrea-cavallaro,['N/A'],Andrea Cavallaro is Professor in Multimedia Signal Processing & Turing Fellow in the School of Electronic Engineering and Computer Science at Queen Mary University of London.  He is also Principal investigator on a new project funded by the Alan Turing Institute: 'Privacy-preserving Multimodal Learning for Activity Recognition' (PRIMULA).; PRIMULA will study; design and validate efficient on-device models from multimodal sensor available on a body camera for privacy preserving aggregation; processing and interpretation. The study and analysis of novel algorithms and techniques will enable us to deepen the scientific understanding of the core problems in the rapidly emerging landscape and application domains of body cameras.; Professor Cavallaro is Area Editor for the Institute of Electrical and Electronics Engineers (IEEE) Signal Processing Magazine and Associate Editor for the IEEE Transactions on Image Processing. He is an elected member of the IEEE Signal Processing Society; Image; Video; and Multidimensional Signal Processing Technical Committee; and chair of its Awards committee. He served as an elected member of the IEEE Signal Processing Society; Multimedia Signal Processing Technical Committee; as Associate Editor for the IEEE Transactions on Multimedia and the IEEE Transactions on Signal Processing; and as Guest Editor for seven international journals. He has published more than 130 journal and conference papers and three edited books and main research interests are image and video analysis; video compression and visual information description.; Camera networks; Target detection and tracking; Behaviour and identity recognition; Bayesian visual tracking; ,Camera networks; Target detection and tracking; Behaviour and identity recognition; Bayesian visual tracking; ,['N/A'],Professor Andrea Cavallaro 
andrea-pizzoferrato,Mathematical physics; ,Andrea is working on a seed funded project on Predictive Graph Analytics and Propagation of Information networks. Andrea has previously been at the University of Warwick where he was completing his mathematics PhD and was also a Teaching Assistant. ; ,['N/A'],['N/A'],Dr Andrea Pizzoferrato 
andrew-beggs,Databases; Deep learning; ,Andrew Beggs is a Reader in cancer genetics & surgery at the University of Birmingham. He obtained his PhD in 2011 from research undertaken in the laboratory of Professor Ian Tomlinson (University of Oxford) and then moved to Birmingham in 2012 where he spent five years undertaking a Wellcome Trust postdoctoral fellowship for clinician scientists in the Institute of Cancer & Genomic Sciences. He obtained a Cancer Research UK Advanced Clinician Scientist award in 2017 and runs a mixed wet/dry lab that examines novel determinants of response to treatment in cancer. He also practices clinically as a Consultant Colorectal & General Surgeon at the Queen Elizabeth Hospital Birmingham.; Next generation sequencing data presents a particular challenge in the field of data analysis and integration. With the advent of the 100;000 Genomes project (100KG); a large volume of next generation sequencing data will become available for analysis along with detailed clinical phenotype data. Up to this point; methods for analysis of this data are relatively sparse and are limited to interactions between two dimensions of data; i.e. mutational and a clinical phenotype (e.g. tumour stage). This fails to take account of the richness of the available data; both genomic and clinical and also fails to account for the interactions at multiple levels that can occur within these datasets.; This diversity of data takes multiple forms. Firstly; although the 100KG project will output mutations and structural variants derived from whole genome sequencing data; there are advanced plans to add layers to this data by measurement of other genetic information. This data will include RNA-seq (to measure gene expression; alternate splicing etc); methylation; chromatin structure (via Hi-C and ATAC-seq) and tumour heterogeneity (via single cell sequencing). Finally; the clinical dataset that has been taken as part of this project will allow correction and modelling of many different environmental influences between genomic data and clinical outcomes. There are likely to be profound interactions with host and tumour genetics and the environment that must be accounted for in any model of disease.; ,Next generation sequencing data presents a particular challenge in the field of data analysis and integration. With the advent of the 100;000 Genomes project (100KG); a large volume of next generation sequencing data will become available for analysis along with detailed clinical phenotype data. Up to this point; methods for analysis of this data are relatively sparse and are limited to interactions between two dimensions of data; i.e. mutational and a clinical phenotype (e.g. tumour stage). This fails to take account of the richness of the available data; both genomic and clinical and also fails to account for the interactions at multiple levels that can occur within these datasets.; This diversity of data takes multiple forms. Firstly; although the 100KG project will output mutations and structural variants derived from whole genome sequencing data; there are advanced plans to add layers to this data by measurement of other genetic information. This data will include RNA-seq (to measure gene expression; alternate splicing etc); methylation; chromatin structure (via Hi-C and ATAC-seq) and tumour heterogeneity (via single cell sequencing). Finally; the clinical dataset that has been taken as part of this project will allow correction and modelling of many different environmental influences between genomic data and clinical outcomes. There are likely to be profound interactions with host and tumour genetics and the environment that must be accounted for in any model of disease.; ,['N/A'],Dr Andrew Beggs 
andrew-dowsey,Multi-agent reasoning; Pattern formation; Databases; Computer vision; Deep learning; Natural language processing; Pattern recognition; Cryptography (Privacy & trust); Differential privacy; Uncertainty quantification; High dimensional inference; Time series; ,Andrew Dowsey is Chair of Population Health Data Science at the University of Bristol; a joint post between the Department of Population Health Sciences and Bristol Veterinary School; where he is Research Director for Bristol Veterinary School. His data science expertise lies at the interface of health sciences and engineering; having previously held positions in the Department of Computing and Institute of Biomedical Engineering; Imperial College London (EPSRC Postdoctoral Fellowship); the Institute of Human Development; University of Manchester (Lecturer); and the Department of Electrical Engineering and Electronics; University of Liverpool (Reader); together with visiting appointments at the University of Texas MD Anderson Cancer Centre and the Conway Institute of Biomolecular and Biomedical Research; University College Dublin.Andrew’s research focus is on the facilitation and acceleration of health sciences research through novel statistical modelling and machine learning methodology; as well as data collection and management platforms. His team works in highly multidisciplinary environments to both support and lead investigations across three strands of research: Profiling the protein and metabolite content of biological fluids and tissues using mass spectrometry proteomics and metabolomics for understanding disease mechanism; and to discover biomarkers for clinical diagnostics; Constructing secure research platforms from population-level agricultural; environmental and health data to inform policy on One Health challenges such as global food security and the transmission of anti-microbial resistance; Intensive longitudinal health; activity and behavioural monitoring at the level of individual animals and groups of animals.; Andrew is leading implementation of the John Oldacre Centre for Sustainability and Welfare in Dairy Production at Bristol Veterinary School; with the aim of create the world’s most intensively monitored livestock cohort at our dairy farm. The Centre will provide blanket 24/7 surveillance of each animal through video tracking; activity wearables; measurements of individual feed and emissions; environmental monitoring; and recording of veterinary assessments. While such infrastructure is key for developing future ‘precision farming’ approaches for optimising farm management in the livestock industry; our Centre is particularly focussed on harnessing this data to tackle fundamental grand challenges affecting human and animal health globally. These include the need to better understand what constitutes and predicts animal resilience; health and welfare; and what factors damage sustainability by increasing the total economic and social burden with respect to competing land use; climate change; and the effects of antimicrobial resistance.The aim of Andrew’s work at the Turing is to build the foundations for a comprehensive open research data platform for the John Oldacre Centre. This involves several interesting data science challenges around the robust monitoring of individual animals and the integration of heterogenous data streams; with many more opening up once the platform is achieved. In this Turing project; we will work with Dr Tilo Burghardt; Department of Computer Science; University of Bristol; on extending his deep learning methodology for species localisation and individual animal biometrics robust to the movements and interactions of our 200+ Holstein cattle.; ,Andrew is leading implementation of the John Oldacre Centre for Sustainability and Welfare in Dairy Production at Bristol Veterinary School; with the aim of create the world’s most intensively monitored livestock cohort at our dairy farm. The Centre will provide blanket 24/7 surveillance of each animal through video tracking; activity wearables; measurements of individual feed and emissions; environmental monitoring; and recording of veterinary assessments. While such infrastructure is key for developing future ‘precision farming’ approaches for optimising farm management in the livestock industry; our Centre is particularly focussed on harnessing this data to tackle fundamental grand challenges affecting human and animal health globally. These include the need to better understand what constitutes and predicts animal resilience; health and welfare; and what factors damage sustainability by increasing the total economic and social burden with respect to competing land use; climate change; and the effects of antimicrobial resistance.The aim of Andrew’s work at the Turing is to build the foundations for a comprehensive open research data platform for the John Oldacre Centre. This involves several interesting data science challenges around the robust monitoring of individual animals and the integration of heterogenous data streams; with many more opening up once the platform is achieved. In this Turing project; we will work with Dr Tilo Burghardt; Department of Computer Science; University of Bristol; on extending his deep learning methodology for species localisation and individual animal biometrics robust to the movements and interactions of our 200+ Holstein cattle.; ,['N/A'],Professor Andrew Dowsey 
andrew-duncan,Numerical (Algorithms); Multi-agent systems; Stochastic (Mathematical modelling); Uncertainty quantification; Monte Carlo methods; Time series; Asymptotic (Statistical methods & theory); Modelling (Statistical methods & theory); Probability; ,Andrew Duncan is RAEng Lecturer (Assistant Professor) in the Department of Mathematics at Imperial College London and group leader for the Data-Centric Engineering Programme at The Alan Turing Institute.; Andrew's research interests span applied stochastic modelling; modelling and inference in a variety of applications including aerospace engineering; energy systems and predictive health monitoring for complex industrial processes.; His theoretical research pertains to methods for approximate inference; anomaly and change-point detection for complex statistical models based on Stein’s method and related methodology. His recent work includes developing methods for assessing convergence of MCMC samplers and for learning intractable generative models.; Andrew's interests lie generally within the intersection between computation; analysis and probability; with a particular focus on applications in biology and chemistry; particularly systems involving stochasticity and/or multiple scales. These include the analysis and construction of MCMC-based methods for sampling from probability distributions; coarse graining of stochastic models involving multiple scales; classical and stochastic homogenisation of PDEs and SDEs; and the Bayesian formulation of inverse problems.; ,Andrew's interests lie generally within the intersection between computation; analysis and probability; with a particular focus on applications in biology and chemistry; particularly systems involving stochasticity and/or multiple scales. These include the analysis and construction of MCMC-based methods for sampling from probability distributions; coarse graining of stochastic models involving multiple scales; classical and stochastic homogenisation of PDEs and SDEs; and the Bayesian formulation of inverse problems.; ,['N/A'],Dr Andrew Duncan 
andrew-elliott,['N/A'],Andrew is working on a project with Accenture to develop new approaches that improve the identification of fraud. Andrew was previously a Research Assistant on the Charity Network Project at Said Business School; University of Oxford.; ,['N/A'],['N/A'], Andrew Elliott 
andrew-holding,Neural networks; Deep learning; ,Dr Andrew Holding is a Senior Research Associate at Cancer Research UK's (CRUK) Cambridge Institute and Fellow of Downing College; Cambridge. Andrew originally studied chemistry at the University of Oxford before he went on to apply these skills to investigate how life works at a molecular level. During his PhD at the University of Cambridge; he focused on how nature makes the antibiotics we use to treat antibiotic-resistant infections including MRSA. In 2009; he moved to the MRC Laboratory of Molecular Biology as a Career Development Fellow; here; he investigated how the machinery within the cells interacts to undertake basic processes including the replication of DNA.; Then in 2013; Andrew joined CRUK's Cambridge Institute where he now leads a team focusing on the molecular processes that drive cancer; in particular the role of nuclear receptors in breast cancer and leukaemia. In 2015 Andrew was appointed a Fellow of Natural Sciences at Downing College; University of Cambridge; where he teaches Biochemistry and Molecular Biology and was awarded the inaugural CRUK Rising Star in Public Engagement Prize by CRUK in recognition of his work communicating with public.; Andrew's research project brings together several cutting-edge technologies: combining single cell sequencing and CRISPR gene editing with neural networks to establish a computational model that predicts therapeutic response to combination drug therapies. Each year there are about 750 cases of Acute Lymphoblastic Leukaemia (ALL); predominantly in those of age 0-4; where cure rates are high. However; in older patients and children who relapse; survival is dismal; leading to around 240 deaths annually. Glucocorticoid Receptor (GR) activation by dexamethasone (Dex) has been one of the mainstay therapies for ALL for over three decades. The caveat is drug resistance: 60% of adult patients are unresponsive to therapy at relapse and is therefore a critical problem for survival.; Combinatorial therapies have the potential to bypass resistance. Screening human cancer cells for drug response has been successful in predicting single therapeutic options from a patient's genotype but does not resolve the challenge of identifying particularly efficacious combinations of drugs. Andrew proposes an alternative strategy: first; establishing how cancer cells respond to combination therapies by experimentally targeting combinations of genes in a high-throughput manner; and secondly; integrating the results through machine learning. By predicting therapies that bypass resistance; Andrew's work will play a key role in improving the current five-year survival rate in adults which; at only 40%; is considerably short of public targets; including CRUK's own 75% 10-year survival goal.; ,Andrew's research project brings together several cutting-edge technologies: combining single cell sequencing and CRISPR gene editing with neural networks to establish a computational model that predicts therapeutic response to combination drug therapies. Each year there are about 750 cases of Acute Lymphoblastic Leukaemia (ALL); predominantly in those of age 0-4; where cure rates are high. However; in older patients and children who relapse; survival is dismal; leading to around 240 deaths annually. Glucocorticoid Receptor (GR) activation by dexamethasone (Dex) has been one of the mainstay therapies for ALL for over three decades. The caveat is drug resistance: 60% of adult patients are unresponsive to therapy at relapse and is therefore a critical problem for survival.; Combinatorial therapies have the potential to bypass resistance. Screening human cancer cells for drug response has been successful in predicting single therapeutic options from a patient's genotype but does not resolve the challenge of identifying particularly efficacious combinations of drugs. Andrew proposes an alternative strategy: first; establishing how cancer cells respond to combination therapies by experimentally targeting combinations of genes in a high-throughput manner; and secondly; integrating the results through machine learning. By predicting therapies that bypass resistance; Andrew's work will play a key role in improving the current five-year survival rate in adults which; at only 40%; is considerably short of public targets; including CRUK's own 75% 10-year survival goal.; ,['N/A'], Andrew Holding 
angela-wood,Data structures; Research methods; High dimensional inference; Simulation; ,Angela Wood is a University Senior Lecturer in Biostatistics at the Department of Public Health and Primary care; University of Cambridge. She joined the department in 2006 after spending 6 years as a Research Scientist at the Medical Research Council's Biostatistics Unit in Cambridge. She gained her PhD in Medical Statistics from the University of Lancaster in 2001. Angela's research interests are centred on the development and application of statistical methods for advancing epidemiological research.; She has focused on developing statistical methodology for handling measurement error; using repeated measures of risk factors; missing data problems; multiple imputation; risk prediction and meta-analysis. She has developed statistical methods and led analyses for major population resources to advance the study of cardiovascular disease; including analyses of the 2.5 million-participant Emerging Risk Factors Collaboration and EPIC-CVD (the world's largest genomic case-cohort study of incident CVD).; The aim of Angela's research is to develop statistical and machine learning frameworks for the development of dynamic risk prediction models that leverage repeated measurements and handle informative observation times in routinely recorded risk factors in electronic health records (EHRs). Two established statistical methods for developing dynamic risk prediction models are joint and landmark modelling; which could be combined with machine-learning approaches to help address current limitations and achieve improved risk assessment. The methods will be used to identify individuals at future risk of experiencing cardiovascular disease (CVD).; Despite reductions in age-specific cardiovascular diseases death rates in the UK in recent decades; the overall future CVD burden is predicted to increase as a result of the ageing population and increasing prevalence of risk factors such as obesity and diabetes. Indeed it has been estimated that tens of thousands of additional CVD outcomes could be prevented per year by better screening and management of risk. These potential gains are more likely to be realised if more compelling evidence emerge to support future approaches to primary prevention of CVD. For example; although the UK has introduced a national CVD screening programme (NHS Health Check) for adults without a history of CVD many of its features have not been founded on robust evidence and could be optimised (or re-designed) with emergence of new data.; Key unresolved questions include: What is the value of using repeated information on CVD risk factors already available in primary care records? How can we identify individuals with a greatest risk of experiencing certain type of CVDs? Such questions are potentially important because; if one could earlier predict which individuals were likely to suffer certain types of CVD events; then preventive treatments could be better targeted on these people and avoided for those at low risk.; ,The aim of Angela's research is to develop statistical and machine learning frameworks for the development of dynamic risk prediction models that leverage repeated measurements and handle informative observation times in routinely recorded risk factors in electronic health records (EHRs). Two established statistical methods for developing dynamic risk prediction models are joint and landmark modelling; which could be combined with machine-learning approaches to help address current limitations and achieve improved risk assessment. The methods will be used to identify individuals at future risk of experiencing cardiovascular disease (CVD).; Despite reductions in age-specific cardiovascular diseases death rates in the UK in recent decades; the overall future CVD burden is predicted to increase as a result of the ageing population and increasing prevalence of risk factors such as obesity and diabetes. Indeed it has been estimated that tens of thousands of additional CVD outcomes could be prevented per year by better screening and management of risk. These potential gains are more likely to be realised if more compelling evidence emerge to support future approaches to primary prevention of CVD. For example; although the UK has introduced a national CVD screening programme (NHS Health Check) for adults without a history of CVD many of its features have not been founded on robust evidence and could be optimised (or re-designed) with emergence of new data.; Key unresolved questions include: What is the value of using repeated information on CVD risk factors already available in primary care records? How can we identify individuals with a greatest risk of experiencing certain type of CVDs? Such questions are potentially important because; if one could earlier predict which individuals were likely to suffer certain types of CVD events; then preventive treatments could be better targeted on these people and avoided for those at low risk.; ,['N/A'],Dr Angela Wood 
angelo-cangelosi,Robotics; Multi-agent reasoning; Neural networks; Neuroscience; Neural & evolutionary computing; Deep learning; Natural language processing; ,Angelo Cangelosi is Professor of Machine Learning and Robotics at the University of Manchester. Previously he was Professor of Artificial Intelligence and Cognition; and founding director; at the Centre for Robotics and Neural Systems at Plymouth University. Cangelosi studied Psychology and Cognitive Science at the Universities of Rome La Sapienza and at the University of Genoa; and was visiting scholar at the University of California San Diego and the University of Southampton. Currently; he is the coordinator of the EU H2020 Marie Sklodowska-Curie European Industrial Doctorate “APRIL: Applications of Personal Robotics through Interaction and Learning” (2016-2019).; He is also Principal Investigator for the ongoing projects “THRIVE++” (US Air Force Office of Science and Research; 2014-2022); the H2020 project MoveCare; and the Marie Curie projects SECURE and DCOMM. He has secured over £30m of research grants as coordinator/PI. Cangelosi has produced more than 250 scientific publications. In 2012-13 he was Chair of the IEEE Technical Committee on Autonomous Mental Development. He has been Visiting Professor at Waseda University in Japan and at Sassari and Messina Universities in Italy. Cangelosi is Editor of the following journals “Interaction Studies” and “IET Cognitive Computation and Systems”; and in 2015 was Editor-in-Chief of IEEE Transactions on Autonomous Development. His latest book “Developmental Robotics: From Babies to Robots” (MIT Press; co-authored with Matt Schlesinger) was published in January 2015; and recently translated in Chinese and Japanese.; Cangelosi's main research interests are in artificial intelligence and cognitive robotics. He is one of the pioneers in the field of developmental robotics; with his main scientific work on neuro-robotic modelling of the grounding of language and of embodies cognition. Application areas include social robot companion for health and social care; and trust and acceptability in human-robot interaction.; ,Cangelosi's main research interests are in artificial intelligence and cognitive robotics. He is one of the pioneers in the field of developmental robotics; with his main scientific work on neuro-robotic modelling of the grounding of language and of embodies cognition. Application areas include social robot companion for health and social care; and trust and acceptability in human-robot interaction.; ,['N/A'],Professor Angelo Cangelosi 
anjali-mazumder,['N/A'],Anjali Mazumder is an Assistant Research Professor in the Department of Statistics and Data Science at Carnegie Mellon University (CMU). She has over 15 years’ experience at the interface of research; policy and practice in the UK; US; and Canadian institutions; having held positions at the Medical Research Council (UK); University of Warwick; Forensic Science Service (UK); and the Institute for Work & Health (Canada). At CMU; she co-ordinates the research activity of the CMU branch of the NIST Centre of Statistics and its Applications to Forensic Evidence (CSAFE); and an affiliate of the Analytics and AI for social good initiative of the new CMU Heinz Block Center for Technology & Society.; She was appointed to Canada’s National DNA Databank Advisory Committee in 2012; actively contributes to initiatives that promote the understanding of uncertainty and data science informed decision-making in policy; and fosters collaboration to strengthen knowledge capacity and scientific innovation; particularly in the realms of data science for social good.; Anjali holds a doctorate in Statistics from the University of Oxford and two masters’ degrees: one in Measurement and Evaluation and one in Statistics; both from the University of Toronto. Anjali is motivated by a desire to solve real and fundamental data problems of societal importance; particularly in justice (criminal and social); human rights and the law; global development; public safety and security; health and education. Anjali’s research interests are in probabilistic graphical models; latent variable models; causality; decision analysis; information theory; Bayesian inference; clustering and trajectory models; change point analysis; social network analysis and their applications.; In particular; her research focuses on developing and applying statistical methods to quantify the value of evidence; determine the optimal subset of information for inference or decision-making; combine models involving different scientific processes or disparate sources of data; characterize and assess change in processes or scenarios; and build decision support systems. Her research involves using data-driven approaches; developing and applying statistical methods and probabilistic reasoning to inform decision-making in policy and practice.; Anjali aims to work closely with the Defence and Security programme researchers and others at the Turing to further her current research work in building peace; security and trust in conflict areas. Perceptions of security; safety and trust in police/security forces can change rapidly in the midst of terrorist attacks; racially-charged violent or criminal outbreaks; and ethnic conflicts. Poor governance and mishandling of such ethnically and racially charged conflicts or events erode trust between security forces and communities; particularly with mob mentality or opinion spread; impeding peace building efforts and giving rise to social; economic and political instability of communities.; With international collaborators; I she aims to (1) understand the complex data structure and causal mechanisms of on-going and potential conflict areas; and (2) develop data-driven approaches and decision-support systems to build peace; security and trust in conflict areas. The work will include accounting for complex spatial and temporal processes and causal mechanisms; identifying change point events and interaction forces; and combining (perhaps) disparate data sources in such a way that can inform actionable tasks for decision-making.; She is interested to develop collaborations with other researchers to develop data science approaches (including understanding causal; spatial-temporal processes; mining unstructured data; combining data systems; building decision support systems) that not only contribute to building security and trust for defence and public safety but that can also be used to help other government services.; ,Anjali aims to work closely with the Defence and Security programme researchers and others at the Turing to further her current research work in building peace; security and trust in conflict areas. Perceptions of security; safety and trust in police/security forces can change rapidly in the midst of terrorist attacks; racially-charged violent or criminal outbreaks; and ethnic conflicts. Poor governance and mishandling of such ethnically and racially charged conflicts or events erode trust between security forces and communities; particularly with mob mentality or opinion spread; impeding peace building efforts and giving rise to social; economic and political instability of communities.; With international collaborators; I she aims to (1) understand the complex data structure and causal mechanisms of on-going and potential conflict areas; and (2) develop data-driven approaches and decision-support systems to build peace; security and trust in conflict areas. The work will include accounting for complex spatial and temporal processes and causal mechanisms; identifying change point events and interaction forces; and combining (perhaps) disparate data sources in such a way that can inform actionable tasks for decision-making.; She is interested to develop collaborations with other researchers to develop data science approaches (including understanding causal; spatial-temporal processes; mining unstructured data; combining data systems; building decision support systems) that not only contribute to building security and trust for defence and public safety but that can also be used to help other government services.; ,['N/A'],Dr Anjali Mazumder 
anna-fitzmaurice,Social data science; Data science of government & politics; Social networks; Social media; Mathematical modelling; Applications (Machine learning); ,Anna is a postdoctoral research fellow on the Women in Data Science and AI project. Prior to joining the Turing; she worked as the Machine Learning and Data Scientist at the Montreal-based non-profit eQualit.ie; which runs an open source effort to protect civil society and independent news organisations from cyber attacks. She holds a PhD from Princeton University in Atmospheric and Oceanic Sciences; with a focus on modelling ice-ocean interactions under future climate change scenarios; and an MMath in Mathematics (first class) from the University of Oxford.; Academically; Anna is interested in how the modern explosion of data and advances in machine learning can be utilised to understand and address current humanitarian issues. Her research at the Turing Institute sits at the intersection of technology and society; taking a data-driven approach to investigating the systematic exclusion of women from tech; and the impact this is having on the development of AI.; ,Academically; Anna is interested in how the modern explosion of data and advances in machine learning can be utilised to understand and address current humanitarian issues. Her research at the Turing Institute sits at the intersection of technology and society; taking a data-driven approach to investigating the systematic exclusion of women from tech; and the impact this is having on the development of AI.; ,['N/A'],Dr Anna FitzMaurice 
anna-hadjitofi,['N/A'],['N/A'],['N/A'],['N/A'], Anna Hadjitofi 
anthony-cohn,Robotics; Multi-agent reasoning; Pattern formation; Computer vision; Deep learning; Natural language processing; Pattern recognition; Cognitive science; Ethics; Uncertainty quantification; Time series; ,Tony Cohn holds BSc and PhD degrees from the University of Essex where he studied under Pat Hayes. He spent 10 years at the University of Warwick before moving to Leeds in 1990 where he founded  a research group working on knowledge representation and reasoning with a particular focus on qualitative spatial/spatio-temporal  reasoning; the best known being the well cited region connection calculus (RCC).  ; He is Editor-in-Chief Spatial Cognition and Computation and has been Chairman/President of the UK AI Society SSAISB; the European Association for Artificial Intelligence  (EurAI);  KR inc; the IJCAI Board of Trustees and was the Editor-in-Chief for  Artificial Intelligence 2007-2014 and of the AAAI Press 2004-14. He remains a Director of KR Inc. ; He is the recipient of the 2015 IJCAI Donald E Walker Distinguished Service Award which honours senior scientists in AI for contributions and service to the field during their careers; as well as the 2012 AAAI Distinguished Service Award for “extraordinary and sustained service to the artificial intelligence community”. He is a Fellow of the Royal Academy of Engineering;  and is also a Fellow of AAAI; AISB;  EurAI (Founding Fellow); the BCS; and the IET.  He was a member of the UK Research Excellence Framework (REF) 2014 Sub Panel 11 (Computer Science and Informatics) of Panel B.; Theme 1: decision support systems (DSS)  for urban infrastructure: One such DSS has been developed in the EPSRC funded assessing the underworld project has been gaining industrial interest. The DSS relies on a number of AI technologies; including ontologies and reasoning with rules which encode uncertain and vague knowledge; a demo of the system can be viewed at http://bit.ly/2pV7k3Z. ; Theme 2: activity recognition: A particular focus is the use of high level qualitative spatio-temporal representations; rather than relying on low level visual features as in much of the other literature. Advantages of this qualitative approach are that it helps abstract away from noise; that instances of activities that are metrically different; are more alike when described qualitatively (which facilitates activity class learning); and that the resulting model can be made to be human-introspectable. Theme 3: grounding language to vision: Recent work has been addressing how descriptions of objects; names; properties; spatial relations and actions can be learnt simultaneously; and incrementally; as well as learning the grammar of the language. The approach is developmental; inspired by acquiring concepts and grounding as a young child might. ; Theme 4: qualitative spatial reasoning for the humanities Tony is involved in a new AHRC network investigating the application of qualitative spatial representations in the digital humanities as an alternative to GIS. The aim is to help find patterns in the information; through techniques including visualisation of the data. These patterns can generate new questions or new perspectives on the world from which the data came. ; ,Theme 1: decision support systems (DSS)  for urban infrastructure: One such DSS has been developed in the EPSRC funded assessing the underworld project has been gaining industrial interest. The DSS relies on a number of AI technologies; including ontologies and reasoning with rules which encode uncertain and vague knowledge; a demo of the system can be viewed at http://bit.ly/2pV7k3Z. ; Theme 2: activity recognition: A particular focus is the use of high level qualitative spatio-temporal representations; rather than relying on low level visual features as in much of the other literature. Advantages of this qualitative approach are that it helps abstract away from noise; that instances of activities that are metrically different; are more alike when described qualitatively (which facilitates activity class learning); and that the resulting model can be made to be human-introspectable. Theme 3: grounding language to vision: Recent work has been addressing how descriptions of objects; names; properties; spatial relations and actions can be learnt simultaneously; and incrementally; as well as learning the grammar of the language. The approach is developmental; inspired by acquiring concepts and grounding as a young child might. ; Theme 4: qualitative spatial reasoning for the humanities Tony is involved in a new AHRC network investigating the application of qualitative spatial representations in the digital humanities as an alternative to GIS. The aim is to help find patterns in the information; through techniques including visualisation of the data. These patterns can generate new questions or new perspectives on the world from which the data came. ; ,['N/A'],Professor Anthony Cohn 
anthony-constantinou,Game theory; Graph theory; Uncertainty quantification; Causality; ,Dr Anthony Constantinou is a Lecturer (Assistant Prof) in Machine Learning and Data Mining at Queen Mary University of London. He is Head of the Bayesian Artificial Intelligence Research Lad; and Principal Investigator on EPSRC UKRI Fellowship 'Bayesian Artificial Intelligence for Decision Making under Uncertainty'.; Anthony's research interests are in Bayesian Artificial Intelligence for causal discovery and intelligent decision making under uncertainty. He collaborates with academics and industrial organisations world-wide; and applies his research to a wide range of areas including sports; economics; medicine; finance; and gaming.; ,Anthony's research interests are in Bayesian Artificial Intelligence for causal discovery and intelligent decision making under uncertainty. He collaborates with academics and industrial organisations world-wide; and applies his research to a wide range of areas including sports; economics; medicine; finance; and gaming.; ,['N/A'],Dr Anthony Constantinou 
anthony-kennedy,['N/A'],Tony Kennedy has been Professor of Computational Science in the School of Physics and Astronomy at the University of Edinburgh since 1998; before which he spent 19 years in the United States working at the University of Maryland; the Institute for Theoretical Physics (ITP) at the University of California at Santa Barbara (UCSB); and the Supercomputer Computations Research Institute (SCRI) at Florida State University (FSU).  ; His research interests span several disciplines within Theoretical Physics; Mathematics; and Computer Science.; His principal research area has been developing algorithms for non-perturbative computations in relativistic quantum field theory; particularly in Quantum Chromodynamics (the theory of the strong nuclear force).  This involves evaluating infinite-dimensional (functional) integrals numerically using Markov Chain Monte Carlo (MCMC) methods.  He introduced the Hybrid (or Hamiltonian) Monte Carlo algorithm in 1987; and several subsequent developments based on this such as the Rational HMC algorithm (which uses optimal rational approximations to improve stochastic estimates for functional determinants); and methods for using Shadow Hamiltonians to automate the determination of optimal parameters for symplectic integrators for use in HMC computations.; He is especially interested in applying MCMC and other statistical methods to other areas; such as machine learning (ML); as well as developing a better understanding the theoretical basis of ML in general.; He has also interested in high-performance computer architecture and in ways of developing efficient; modular; reusable software for such systems. He was part of the collaboration that built the QCDSP computer for lattice QCD computations (which won a Gordon Bell prize in 1998); and in the subsequent QCDOC computer which directly led to IBM's Blue Gene supercomputers.; Other interests include various areas of theoretical and mathematical physics (such as renormalization theory); algorithms for computations using representations of groups and algebras; algorithms for computer algebra and related category theoretic ideas for computer languages.; ,His research interests span several disciplines within Theoretical Physics; Mathematics; and Computer Science.; His principal research area has been developing algorithms for non-perturbative computations in relativistic quantum field theory; particularly in Quantum Chromodynamics (the theory of the strong nuclear force).  This involves evaluating infinite-dimensional (functional) integrals numerically using Markov Chain Monte Carlo (MCMC) methods.  He introduced the Hybrid (or Hamiltonian) Monte Carlo algorithm in 1987; and several subsequent developments based on this such as the Rational HMC algorithm (which uses optimal rational approximations to improve stochastic estimates for functional determinants); and methods for using Shadow Hamiltonians to automate the determination of optimal parameters for symplectic integrators for use in HMC computations.; He is especially interested in applying MCMC and other statistical methods to other areas; such as machine learning (ML); as well as developing a better understanding the theoretical basis of ML in general.; He has also interested in high-performance computer architecture and in ways of developing efficient; modular; reusable software for such systems. He was part of the collaboration that built the QCDSP computer for lattice QCD computations (which won a Gordon Bell prize in 1998); and in the subsequent QCDOC computer which directly led to IBM's Blue Gene supercomputers.; Other interests include various areas of theoretical and mathematical physics (such as renormalization theory); algorithms for computations using representations of groups and algebras; algorithms for computer algebra and related category theoretic ideas for computer languages.; ,['N/A'],Professor Anthony Kennedy 
antoine-jacquier,Stochastic (Mathematical modelling); Asymptotic (Statistical methods & theory); Probability; ,Dr Antoine Jacquier is a Senior Lecturer in the Department of Mathematics at Imperial College London.; His research interests are in probability and mathematical finance. He is particularly interested in large deviations methods and asymptotic expansions for stochastic processes; and their applications to volatility modelling.; ,His research interests are in probability and mathematical finance. He is particularly interested in large deviations methods and asymptotic expansions for stochastic processes; and their applications to volatility modelling.; ,['N/A'],Dr Antoine Jacquier 
anuj-dawar,Complexity (Algorithms); Logic (Theoretical mathematics); ,Anuj Dawar is Professor of Logic and Algorithms at the University of Cambridge; where he has been a member of the faculty since 1999.He obtained a Bachelor's degree from IIT; Delhi and a Master's degree from the University of Delaware before going on to get his PhD from the University of Pennsylvania in 1993. Before coming to Cambridgehe worked for several years as a post-doc and a lecturer at Swansea University.; Anuj has an interest in computational complexity theory; which seeks to understand the fundamental limitations of some of our most powerful algorithmic techniques. He has worked for many years on approaches to complexity based on logic - by relating computational complexity to problems of definability. Recently he has been focusing on the role of symmetry in computation; both as a resource and a limitation.; ,Anuj has an interest in computational complexity theory; which seeks to understand the fundamental limitations of some of our most powerful algorithmic techniques. He has worked for many years on approaches to complexity based on logic - by relating computational complexity to problems of definability. Recently he has been focusing on the role of symmetry in computation; both as a resource and a limitation.; ,['N/A'],Professor Anuj Dawar 
anya-skatova,Neuroscience; Reinforcement learning; Semi-supervised learning; Differential privacy; Cognitive science; Data science of government & politics; Ethics; Research methods; Social media; Social psychology; ,Anya Skatova is Vice-Chancellor's Fellow in Digital Innovation and Wellbeing at the School of Psychological Science; University of Bristol. She gained her PhD in Psychology at the University of Nottingham; and worked as a postdoctoral researcher at the University of Nottingham and the University of Warwick. She has extensive network of industry contacts and always open for new collaborations. Currently Anya focuses on using large transactional (e.g.; banking and retail) datasets to study individual difference in decision-making; wellbeing and personality. She is also working on linking banking and retail loyalty card datasets with data collected through longitudinal population studies (LPS) through joining behavioural patterns that can be learned from transactional data with rich medical; genetic; early life environment and other records collected by LPS. Finally; she works on a range of projects related to public attitudes to data sharing including whether individuals can assign value to their personal data and what are perceived risks and benefits of sharing personal data with various organisations.; The proliferation of digital technology has ushered in a new era of understanding individual choices and decision-making. An ever-increasing amount of machine recorded information is routinely generated as we traverse our daily lives; touching on a range of human behaviours from financial activity (via banking records); to eating habits (via supermarket loyalty cards and digital food diaries). The patterns of these real-world choices could aid understanding of the causes and consequences of public health issues such as obesity; diabetes and mental health issues. Currently; this data is collected and used mainly by private companies to fit their own benefit (e.g.; to track sales of their products; target promotions); with the general public having little say in the way these data are shared and linked with other datasets. Through unlocking these data sets for academic use; we can benefit population health research. Anya's work at the Turing focuses on two research questions: (1) what are the mostly widely held attitudes towards using transactional data for public health research? (2) what are the publicly acceptable and ethical pathways of using transactional data for public health research?; ,The proliferation of digital technology has ushered in a new era of understanding individual choices and decision-making. An ever-increasing amount of machine recorded information is routinely generated as we traverse our daily lives; touching on a range of human behaviours from financial activity (via banking records); to eating habits (via supermarket loyalty cards and digital food diaries). The patterns of these real-world choices could aid understanding of the causes and consequences of public health issues such as obesity; diabetes and mental health issues. Currently; this data is collected and used mainly by private companies to fit their own benefit (e.g.; to track sales of their products; target promotions); with the general public having little say in the way these data are shared and linked with other datasets. Through unlocking these data sets for academic use; we can benefit population health research. Anya's work at the Turing focuses on two research questions: (1) what are the mostly widely held attitudes towards using transactional data for public health research? (2) what are the publicly acceptable and ethical pathways of using transactional data for public health research?; ,['N/A'],Dr Anya Skatova 
aretha-teckentrup,['N/A'],Dr Teckentrup is a lecturer in data science in the School of Mathematics. Before coming to Edinburgh; she held postdoctoral research positions at Florida State University and the University of Warwick. Dr Teckentrup received her PhD in applied mathematics from the University of Bath in 2013.; Aretha's research interests are at the interface of numerical analysis; statistics and data science. She is particularly interested in uncertainty quantification in simulation with complex computer models; with recent research focussing on multilevel sampling methods; Bayesian inverse problems; Gaussian process regression; approximation theory and deep Gaussian processes.; ,Aretha's research interests are at the interface of numerical analysis; statistics and data science. She is particularly interested in uncertainty quantification in simulation with complex computer models; with recent research focussing on multilevel sampling methods; Bayesian inverse problems; Gaussian process regression; approximation theory and deep Gaussian processes.; ,['N/A'],Dr Aretha Teckentrup 
arnaud-doucet,['N/A'],He obtained his PhD from the University of Paris XI (Orsay) in 1997. He previously held academic positions at Cambridge University; Melbourne University; The Institute of Statistical Mathematics in Tokyo and the University of British Columbia where he was a Canada Research Chair in Stochastic Computation.; Professor Arnaud Doucet’s research concerns numerical methods for the analysis of complex data sets. In particular; he has contributed to the development and study of sequential Monte Carlo and Markov chain Monte Carlo methods.; ,Professor Arnaud Doucet’s research concerns numerical methods for the analysis of complex data sets. In particular; he has contributed to the development and study of sequential Monte Carlo and Markov chain Monte Carlo methods.; ,['N/A'],Professor Arnaud Doucet 
arshad-jhumka,['N/A'],Arshad Jhumka leads the Fault-Tolerant and Reliable Systems group in the Department of Computer Science at Warwick. He received his PhD in Computer Science in 2003 from TU-Darmstadt; Germany. In 2004; he worked as a postdoctoral research associate at Chalmers University of Technology; Sweden. He joined Warwick in 2005 as a lecturer and is now an Associate Professor.; His main interest is the design and validation of reliable systems; ranging from small embedded systems to large-scale distributed systems such as cluster systems. Arshad has used both formal and experimental approaches to the design of such systems but; increasingly; he has been using a data-centric approach for both the design and the validation of these systems. Dr Jhumka is interested in the application of machine learning techniques to both reliability and security issues such as intrusion detection; malware detection; design and validation of reliable SW and failure diagnosis in supercomputers; among many others.; Arshad is a Turing Fellow through his supervision of Edward Chuah; a doctoral student at the Turing.; ,His main interest is the design and validation of reliable systems; ranging from small embedded systems to large-scale distributed systems such as cluster systems. Arshad has used both formal and experimental approaches to the design of such systems but; increasingly; he has been using a data-centric approach for both the design and the validation of these systems. Dr Jhumka is interested in the application of machine learning techniques to both reliability and security issues such as intrusion detection; malware detection; design and validation of reliable SW and failure diagnosis in supercomputers; among many others.; Arshad is a Turing Fellow through his supervision of Edward Chuah; a doctoral student at the Turing.; ,['N/A'],Dr Arshad Jhumka 
arzu-kibris,Data science of government & politics; Research methods; Social psychology; Causality; Simulation; Time series; Estimation theory; Probability; ,Arzu Kibris is an Associate Professor of Politics at the Department of Politics and International Studies at University of Warwick. Her main research interests include the dynamics and social; political and economic consequences of civil conflicts as well as formal analyses of political and economic behaviour. Her work has been published in the Journal of Conflict Resolution; Social Science & Medicine; Public Choice; and Studies in Conflict and Terrorism; among other journals. She received the prestigious ERC Starting Grant in 2016 for her project EXPOVIBE: Exposure to Political Violence and Individual Behavior in which she explores the individual level effects of being exposed to political violence in a civil conflict context.; Her research focuses on studying how violent events evolve over time and space within a civil conflict context and on analysing whether there is a predictable pattern to the diffusion of violence that we can decipher. Needless to say; these are very important questions. The typical civil conflict lasts about 10 years and wreaks havoc to the host country. If we can figure out the spatial and temporal dynamics of civil conflicts; pre-emptive actions and policies can then be devised to prevent violence from ever happening or to nip these humanitarian disasters in the bud. This is a rather ambitious goal that requires significant advancements in data availability; theoretical understanding; empirical methodology and predictive capability. She aims to contribute to those advancements in a significant way by introducing a new; comprehensive and detailed conflict event dataset and by building theoretically sound statistical models with good fit and high predictive capability.; ,Her research focuses on studying how violent events evolve over time and space within a civil conflict context and on analysing whether there is a predictable pattern to the diffusion of violence that we can decipher. Needless to say; these are very important questions. The typical civil conflict lasts about 10 years and wreaks havoc to the host country. If we can figure out the spatial and temporal dynamics of civil conflicts; pre-emptive actions and policies can then be devised to prevent violence from ever happening or to nip these humanitarian disasters in the bud. This is a rather ambitious goal that requires significant advancements in data availability; theoretical understanding; empirical methodology and predictive capability. She aims to contribute to those advancements in a significant way by introducing a new; comprehensive and detailed conflict event dataset and by building theoretically sound statistical models with good fit and high predictive capability.; ,['N/A'], Arzu Kibris 
ashley-scillitoe,['N/A'],Research Associate within the Data-Centric Engineering programme; focusing on applying machine learning (ML) to data-driven Computational Fluid Dynamics (CFD) methods.; ,['N/A'],['N/A'],Dr Ashley Scillitoe 
ashwini-venkatasubramaniam,Spatial analytics; Time series; Modelling (Statistical methods & theory); Non-parametric & semi-parametric methods; Causality; ,"Ashwini is a Research Associate at the Institute and joined in April 2019. She gained her PhD in Statistics at the University of Glasgow and her thesis was titled ""Nonparametric clustering for spatio-temporal datasets"". This thesis focussed on the development of a flexible Bayesian model that identifies contiguous clusters in a data-driven manner and is able to accommodate varied (spatial; temporal and network) dependency structures. Previously; she also earned a M.S. in Biostatistics from the University of Minnesota; Twin Cities; a B.Sc. in Mathematics with Economics from the University of East Anglia and a Diploma in Economics from the University of London.; She is currently affiliated with the Health programme at the Institute and appreciates the collaborative nature of associated projects. Her research interests in this domain are diverse and include machine learning and causal inference with particular focus on subgroup identification; characterisation and variable selection.; ",She is currently affiliated with the Health programme at the Institute and appreciates the collaborative nature of associated projects. Her research interests in this domain are diverse and include machine learning and causal inference with particular focus on subgroup identification; characterisation and variable selection.; ,['N/A'],Dr Ashwini Venkatasubramaniam 
ata-kaban,Computer vision; Pattern recognition; High dimensional inference; ,Ata Kaban is professor in Computer Science at the University of Birmingham. She holds a PhD in Computer Science (2001) and a PhD in Musicology (1999). She is a member of the IEEE CIS Technical Committee on Data Mining and Big Data Analytics; and vice-chair of the IEEE CIS Task Force on High Dimensional Data Mining.; Ata Kaban works in high-dimensional data analytics and statistical machine learning; currently focusing on the problems of the 'curse of dimensionality'; and the gap between theory and practice. She has a major interest in techniques and methodologies for large scale data analysis; in particular random projections and other dimensionality reduction methods; and their potential use to better explain how high-dimensional learning works. For instance; pixel representation of images becomes increasingly high-dimensional due to advances of high-resolution measurement devices. At the same time their information content may increase at a somewhat slower rate. This makes image data sets particularly suited to form testbeds for the problem of detecting low-complexity geometric structures that facilitate learning in high dimensions.; ,Ata Kaban works in high-dimensional data analytics and statistical machine learning; currently focusing on the problems of the 'curse of dimensionality'; and the gap between theory and practice. She has a major interest in techniques and methodologies for large scale data analysis; in particular random projections and other dimensionality reduction methods; and their potential use to better explain how high-dimensional learning works. For instance; pixel representation of images becomes increasingly high-dimensional due to advances of high-resolution measurement devices. At the same time their information content may increase at a somewhat slower rate. This makes image data sets particularly suited to form testbeds for the problem of detecting low-complexity geometric structures that facilitate learning in high dimensions.; ,['N/A'],Professor Ata Kaban 
awais-rashid,Robotics; Multi-agent reasoning; Evolution & adaptation; Human computer interface; Applications (Machine learning); Natural language processing; Semi-supervised learning; Supervised learning; Identity management; Verification; Ethics; ,Awais Rashid is Professor of Cyber Security at University of Bristol; where he heads the Bristol Cyber Security Group. His research is focused on cyber security of connected critical infrastructure systems. This involves large-scale analysis of security and resilience of the cyber-physical systems and software underpinning critical infrastructure systems; human factors as well as adversarial behaviours based on real-world datasets of cyber criminal activities.; He leads multiple projects as part of two EPSRC-NCSC Research Institutes: Research Institute in Trustworthy; Interconnected; Cyber- Physical Systems (RITICS) and Research Institute in Science of Cyber Security (RISCS); co-leads the Security & Safety Stream within PETRAS: the EPSRC Research Hub on Cyber Security of Internet of Things; heads the National Cyber Security Programme funded Cyber Security Body of Knowledge (CyBOK) project developing interdisciplinary foundations for the field and is a Co-Investigator of the ESRC Centre for Research and Evidence on Security Threats (CREST).; Rashid’s research has an empirical; experimental ethos based on experimentation on real-world critical infrastructure testbeds that the Bristol Cyber Security Group has setup and runs.; As a Turing Fellow; Rashid is focusing on novel data science techniques to reason about security and resilience of connected critical infrastructure at scale – befitting the size; connectivity and heterogeneity of platforms; devices and stakeholders in such infrastructures. He will be investigating a number of key research problems including: (1) the type information from the infrastructure and applications/services that is pertinent to reasoning about its security state; (2) novel automated and semi-automated techniques that can ensure resilience and continued operation of the infrastructure when under attack; and (3) the balance between automated reasoning and decision-making and the role of human actors in this regard.; Rashid was the Laureate of the Pays de la Loire Chaire Regionale; Ecole des Mines de Nantes; France; 2008-2011. He is a member of the EPSRC Digital Economy Programme Advisory Board and is also on the advisory group of the DFG SFB Collaborative Research Centre: CROSSING (Cryptography-Based Security Solutions: Enabling Trust in New and Next Generation Computing Environments) at TU Darmstadt; Germany. He is also a member of the Scientific Advisory Board of the EPSRC-NCSC Research Institute on Science of Cyber Security.; ,As a Turing Fellow; Rashid is focusing on novel data science techniques to reason about security and resilience of connected critical infrastructure at scale – befitting the size; connectivity and heterogeneity of platforms; devices and stakeholders in such infrastructures. He will be investigating a number of key research problems including: (1) the type information from the infrastructure and applications/services that is pertinent to reasoning about its security state; (2) novel automated and semi-automated techniques that can ensure resilience and continued operation of the infrastructure when under attack; and (3) the balance between automated reasoning and decision-making and the role of human actors in this regard.; ,Rashid was the Laureate of the Pays de la Loire Chaire Regionale; Ecole des Mines de Nantes; France; 2008-2011. He is a member of the EPSRC Digital Economy Programme Advisory Board and is also on the advisory group of the DFG SFB Collaborative Research Centre: CROSSING (Cryptography-Based Security Solutions: Enabling Trust in New and Next Generation Computing Environments) at TU Darmstadt; Germany. He is also a member of the Scientific Advisory Board of the EPSRC-NCSC Research Institute on Science of Cyber Security.; ,Professor Awais Rashid 
barbara-mcgillivray,Social data science; Linguistics; Research methods; Natural language processing; ,Barbara McGillivray is Turing Research Fellow at The Alan Turing Institute and the University of Cambridge and Editor in Chief of the Journal of Open Humanities Data. She has always been passionate about how Sciences and Humanities can meet. She completed a PhD in Computational Linguistics from the University of Pisa in 2010 after a degree in Mathematics and one in Classics from the University of Florence (Italy). Before joining the Turing; she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature.; Language changes all the time in fascinating ways. Barbara's research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek; Latin; eighteen-century English) and in contemporary texts (Twitter; web archives). Her interdisciplinary contribution covers Data Science; Natural Language Processing; Historical Linguistics and other humanistic fields; to push the boundaries of what academic disciplines separately have achieved so far on this topic. ; Barbara convenes the Data Science and Digital Humanities Special Interest Group at the Turing.; She has organised the Workshop on automatic methods for lexical semantic change; ,Language changes all the time in fascinating ways. Barbara's research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek; Latin; eighteen-century English) and in contemporary texts (Twitter; web archives). Her interdisciplinary contribution covers Data Science; Natural Language Processing; Historical Linguistics and other humanistic fields; to push the boundaries of what academic disciplines separately have achieved so far on this topic. ; ,Barbara convenes the Data Science and Digital Humanities Special Interest Group at the Turing.; She has organised the Workshop on automatic methods for lexical semantic change; ,Dr Barbara McGillivray 
beatrice-alex,Machine learning; Applications (Machine learning); Natural language processing; ,Dr. Beatrice Alex is Chancellor's Fellow at the Edinburgh Futures Institute and the School of Literatures; Languages and Cultures and Turing Fellow at The Alan Turing Institute and the School of Informatics at the University of Edinburgh.  Her research interests center around information extraction and text mining.  She has worked on a many projects involving natural language processing applied to different domains; including healthcare biomedicine; astronomy; news; recruitment; social media; history and literature.; Accessibility; interpretability and linking of data have become the focus of many data providers and users. Most research in this area has been directed towards making textual resources more accessible. Audio and video archives need to be regarded as equally important. Cataloguing and transcription of speech archives is extremely labour-intensive and time-consuming. The goal of Beatrice's work is to mine transcripts of large speech archives automatically to generate metadata for such collections. Beatrice is working closely with the British Library and applying her work to TV and radio recordings held at their archive.; ,Accessibility; interpretability and linking of data have become the focus of many data providers and users. Most research in this area has been directed towards making textual resources more accessible. Audio and video archives need to be regarded as equally important. Cataloguing and transcription of speech archives is extremely labour-intensive and time-consuming. The goal of Beatrice's work is to mine transcripts of large speech archives automatically to generate metadata for such collections. Beatrice is working closely with the British Library and applying her work to TV and radio recordings held at their archive.; ,['N/A'],Dr Beatrice Alex 
ben-caldecott,Applications (Machine learning); Data science of government & politics; ,Dr Ben Caldecott is the founding Director of the Oxford Sustainable Finance Programme and a Senior Research Fellow at the University of Oxford Smith School of Enterprise and the Environment. He is concurrently an Academic Visitor at the Bank of England and a Visiting Scholar at Stanford University. Ben is the Organiser of the Sustainable Finance Interest Group at The Alan Turing Institute.He has conceived and initiated a number of initiatives related to sustainable finance. Ben founded and co-chairs the Global Research Alliance for Sustainable Finance and Investment (GRASFI); an alliance of global research universities promoting rigorous and impactful academic research on sustainable finance. He initiated the Spatial Finance Initiative (SFI) that aims to mainstream geospatial capabilities enabled by space technology and data science into financial decision-making globally; as well as the Asset-level Data Initiative (ADI); which aims to make accurate; comparable; and comprehensive asset-level data tied to ownership publicly available across key sectors and geographies. Ben also chairs the City of London Green Finance Initiative (GFI) Working Group on Data; Disclosure; and Risk. In his capacity as a Member of the UK Green Finance Taskforce; he chaired its Workstream on Task Force on Climate-related Disclosures (TCFD) Implementation. Prior to joining the University of Oxford; he was a Vice President at investment bank Climate Change Capital; one of the early leading asset management and advisory firms focused on the low carbon transition; where he ran the firm's research centre and advised clients and funds on the development of policy-driven markets. Ben has previously worked as Research Director for Environment and Energy at the think tank Policy Exchange; as Head of Government Advisory at Bloomberg New Energy Finance; as a Deputy Director in the Strategy Directorate of the UK's Department of Energy and Climate Change; as an Advisor to The Prince of Wales's International Sustainability Unit; and as Sherpa to the UK Green Investment Bank Commission.Ben holds a doctorate in economic geography from the University of Oxford. He initially read economics and specialised in development and China at the University of Cambridge and the School of Oriental and African Studies; University of London. He has been a Visiting Scholar at Peking University and held Visiting Fellowships at the University of Oxford; the University of Sydney; and the University of Melbourne. He is an Associate Editor of the Journal of Sustainable Finance & Investment. Ben is also a Non-Resident Fellow at the Payne Institute for Earth Resources at the Colorado School of Mines; a Fellow of the Royal Asiatic Society and Royal Geographical Society; and a Member of the Senior Common Room at Oriel College; Oxford.; ,['N/A'],['N/A'],Dr Ben Caldecott 
ben-leimkuhler,['N/A'],"Ben studies the design; analysis and implementation of algorithms for time-dependent phenomena and modelling for problems in engineering and the sciences. His previous works has helped to establish the foundations of molecular simulation; providing efficient deterministic and stochastic numerical methods for an exploding field of application. A recent line of research has focused on stochastic algorithms for Bayesian inference from data and has demonstrated the potential for crossover of work from molecular science to data analytics (in this case molecular dynamics temperature controls were used to stabilise sampling-based parameterisation schemes).; Data science disrupts the traditional mathematical model by replacing physical law with empirical law; through the need to incorporate inference based on massive data sets or streams; and by destroying smooth structures (ODE/PDE solutions) that underpin numerical analysis. Data science is grounded in statistics and optimization; but to be effective in the engineering setting; data analysis methods such as Bayesian inference must be merged with models built up over many centuries on a foundation of physical law (e.g. quantum mechanics and thermodynamics) and be compatible with dynamical principles and geometric (or topological) constraints. During Ben's fellowship he plans to explore the interplay between ""naive"" data science approaches and models informed by physical law and mathematical structure. Access to The Alan Turing Institute is already providing him with connections to the statistics; operational research and machine learning communities. He looks forward to expanding these connections in the coming years.; ","Data science disrupts the traditional mathematical model by replacing physical law with empirical law; through the need to incorporate inference based on massive data sets or streams; and by destroying smooth structures (ODE/PDE solutions) that underpin numerical analysis. Data science is grounded in statistics and optimization; but to be effective in the engineering setting; data analysis methods such as Bayesian inference must be merged with models built up over many centuries on a foundation of physical law (e.g. quantum mechanics and thermodynamics) and be compatible with dynamical principles and geometric (or topological) constraints. During Ben's fellowship he plans to explore the interplay between ""naive"" data science approaches and models informed by physical law and mathematical structure. Access to The Alan Turing Institute is already providing him with connections to the statistics; operational research and machine learning communities. He looks forward to expanding these connections in the coming years.; ",['N/A'],Professor Ben Leimkuhler 
ben-macarthur,Applied mathematics; Dynamical systems & differential equations; Nonlinear dynamics; Machine learning; Applications (Machine learning); Pattern recognition; Supervised learning; Unsupervised learning; Mathematical modelling; Graph theory; Stochastic (Mathematical modelling); Social networks; Time series; Information theory (Statistical methods & theory); ,Ben MacArthur is professor of quantitative biomedicine at the University of Southampton. In accordance with his interdisiplinary research interests he holds a joint between the Faculty of Medicine and Mathematical Sciences.; He obtained a PhD in applied mathematics (Southampton; 2003) before training in experimental cell biology; first in the Faculty of Medicine at Southampton (2003-2008) and then at Mount Sinai School of Medicine; USA (2008-2010).; He is a visiting professor at the International Research Centre for Medical Sciences at Kumamoto University; Japan.; Ben's work combines computational; mathematical and experimental methods to better understand human health and disease. In particular; he uses machine learning methods to analyse and combine complex biomedical datasets; including time-series; genomics/proteomics; single cell expression; and heterogeneous clinical datasets. He is also interested in how complex patterns of social interactions (for example; from social networks) interact with physiology to affect human health; and well-being.  His work often combines machine learning with mechanistic mathematical modelling to better understand the causes of any observed patterns.  ;  ;  ; ,Ben's work combines computational; mathematical and experimental methods to better understand human health and disease. In particular; he uses machine learning methods to analyse and combine complex biomedical datasets; including time-series; genomics/proteomics; single cell expression; and heterogeneous clinical datasets. He is also interested in how complex patterns of social interactions (for example; from social networks) interact with physiology to affect human health; and well-being.  His work often combines machine learning with mechanistic mathematical modelling to better understand the causes of any observed patterns.  ;  ;  ; ,['N/A'],Professor Ben MacArthur 
benjamin-guedj,Supervised learning; Unsupervised learning; Uncertainty quantification; ,Dr. Benjamin Guedj obtained his PhD in Mathematics in 2013 from Université Pierre & Marie Curie (now Sorbonne Université; France). Since 2014; he is a tenured researcher at Inria (Lille - Nord Europe research centre) and since 2018; he also is a principal researcher at University College London (Department of Computer Science).; He has recently obtained two competitive grants from the French Agency for Research; is an elected member of the board of the French Statistical Society and of Inria's Evaluation Committee. Benjamin's areas of expertise are machine learning; statistical learning theory; and computational statistics. Keywords: PAC-Bayes; Bayesian learning; supervised and unsupervised learning; concentration inequalities; deep learning; online learning; sparsity and high dimensional statistics; ensemble learning.; As a visiting researcher to the Data-Centric Engineering programme; Benjamin Guedj is planning to carry on research on variational inference for complex systems (based on his expertise on PAC-Bayes); scalable sampling algorithms with theoretical guarantees; high-dimensional and sequential clustering algorithms; and reinterpret deep learning architectures from an information-theoretic perspective.; ,As a visiting researcher to the Data-Centric Engineering programme; Benjamin Guedj is planning to carry on research on variational inference for complex systems (based on his expertise on PAC-Bayes); scalable sampling algorithms with theoretical guarantees; high-dimensional and sequential clustering algorithms; and reinterpret deep learning architectures from an information-theoretic perspective.; ,['N/A'],Dr Benjamin Guedj 
bernie-hogan,['N/A'],Bernie Hogan completed his BA(hons) at the Memorial University of Newfoundland in Canada; where he received the University Medal in Sociology. Since then he has been working on Internet use and social networks at the University of Toronto under social network analysis pioneer Barry Wellman. Bernie received his Masters of Arts at Toronto in 2003; and defended his PhD Dissertation in the Fall of 2008. His dissertation examines how the use of ICTs alters the way people maintain their relationships in everyday life. In 2005 he was an intern at Microsoft’s Community Technologies Lab; working with Danyel Fisher on new models for email management. ; Bernie Hogan’s research focuses on the creation; maintenance and analysis of personal social networks; with a particular focus on the relation between online and offline networks. Hogan’s work has demonstrated the utility of visualisation for network members; how the addition of new social media can complicate communication strategies; and how the uneven distribution of media globally can affect the ability of people to participate online. Currently; Hogan is working on techniques to simplify the deployment of personal network studies for newcomers as well as social-theoretical work on the relationship between naming conventions and identities.; ,Bernie Hogan’s research focuses on the creation; maintenance and analysis of personal social networks; with a particular focus on the relation between online and offline networks. Hogan’s work has demonstrated the utility of visualisation for network members; how the addition of new social media can complicate communication strategies; and how the uneven distribution of media globally can affect the ability of people to participate online. Currently; Hogan is working on techniques to simplify the deployment of personal network studies for newcomers as well as social-theoretical work on the relationship between naming conventions and identities.; ,['N/A'],Dr Bernie Hogan 
bertie-vidgen,Social data science; Data science of government & politics; Social media; ,Bertie Vidgen is a Research Associate within the public policy programme. His main research is focused on detecting; analysing; and countering online hate speech; examining it in the context of both news and social media. In his work; he primarily uses computational social science methods; including machine learning; natural language processing; and statistical modelling.; Before he joined the Turing; Bertie studied for a DPhil at the University of Oxford's Oxford Internet Institute; where he researched Islamophobic hate speech among followers of UK political parties on Twitter. He is particularly interested in understanding how Islamophobia manifests across the political spectrum (including both mainstream and far right parties) as well as its temporal dynamics. He is actively interested in the use of computational methods in social science; the ethical challenges of using social media data; and the role of social media within politics. He holds a BA from the University of Warwick in History and Politics and an MA from the University of Essex in Ideology and Discourse Analysis. Bertie's other work includes consulting; advising; and teaching.; Blog post; 'Four ways social media platforms could stop the spread of hateful content in aftermath of terror attacks'; ,Blog post; 'Four ways social media platforms could stop the spread of hateful content in aftermath of terror attacks'; ,['N/A'],Dr Bertie Vidgen 
biao-cai,['N/A'],Biao obtained a BSc and MSc in Materials from Central South University in China in 2009 and 2011; respectively. He went on to study for a PhD in Materials at the University of Manchester; and was awarded the degree in 2015. After that; he worked for two years as a post-doc at the University of Manchester on various projects ranging from alloy solidification to magma flow mainly; using state-of-the-art high speed synchrotron X-ray tomography. He was based at the Research Complex at Harwell; Rutherford Appleton Laboratory; Harwell Campus; Oxfordshire during his PhD and post-doc.; He became a Lecturer in the School of Metallurgy and Materials at the University of Birmingham in October 2017. His main role is to co-ordinate the Diamond-Birmingham Collaboration; which is a long-term collaboration established between the University of Birmingham and Diamond Light Source (UK’s synchrotron X-ray source).; His current research focuses on developing and utilising in situ synchrotron; and neutron methods; to gain new understanding in micro-mechanics and physical metallurgy - feeding back into the development of new materials and manufacturing techniques.; ,His current research focuses on developing and utilising in situ synchrotron; and neutron methods; to gain new understanding in micro-mechanics and physical metallurgy - feeding back into the development of new materials and manufacturing techniques.; ,['N/A'],Dr Biao Cai 
bill-byrne,['N/A'],Bill Byrne is a member of the Machine Intelligence Lab in the Cambridge University Engineering Department. His background is in information theory and statistical signal processing. Bill's research is focused on the statistical modelling of speech and language with a particular current interest in statistical machine translation.; Bill hopes to study hybrid symbolic and neural modelling approaches for language processing. His aim will be to develop techniques that combine the fluency and naturalness of `neural` models with the robustness and descriptive power of symbolic approaches.; ,Bill hopes to study hybrid symbolic and neural modelling approaches for language processing. His aim will be to develop techniques that combine the fluency and naturalness of `neural` models with the robustness and descriptive power of symbolic approaches.; ,['N/A'],Professor Bill Byrne 
blanka-horvath,Numerical analysis; Machine learning; Stochastic (Mathematical modelling); Simulation; ,Blanka Horvath is a Lecturer at King's College London in the Financial Mathematics group; and an Honorary Lecturer in the Department of Mathematics at Imperial College London.; Blanka holds a PhD in Financial Mathematics from ETH Zurich; a postgraduate degree (Diplom) in Mathematics from the University of Bonn; and an MSc in Economics from The University of Hong Kong. In her research she lays a particular emphasis on the applicability of her research and maintains close collaborations with the industry; including: JP Morgan; Deutsche Bank; Zeliade Systems and AXA.; Her research interests are in the area of stochastic analysis and mathematical finance. They include (but not limited to); ,Her research interests are in the area of stochastic analysis and mathematical finance. They include (but not limited to); ,['N/A'],Dr Blanka Horvath 
bo-wang,Cognitive science; Information retrieval; Neural networks; Natural language processing; ,['N/A'],['N/A'],['N/A'], Bo Wang 
brad-love,['N/A'],Brad Love is Professor of Cognitive and Decision Sciences at UCL. He works at the intersection of Neuroscience; Experimental Psychology; and Machine Learning. His current interests include using brain imaging data to select between competing models of cognitive function; and combining big data and psychological theory to understand consumer behaviour. ; He is interested in understanding consumer behaviour using large datasets; such as loyalty card data. Topics include how people explore product options and construe product categories. He is also interested in relating deep learning networks to brain function. Additionally; he would like to pursue a machine learning collaboration in which lessons from neural computation could be used to improve the performance of these models on a variety of tasks; including object recognition.; ,He is interested in understanding consumer behaviour using large datasets; such as loyalty card data. Topics include how people explore product options and construe product categories. He is also interested in relating deep learning networks to brain function. Additionally; he would like to pursue a machine learning collaboration in which lessons from neural computation could be used to improve the performance of these models on a variety of tasks; including object recognition.; ,['N/A'],Professor Brad Love 
brent-mittelstadt,['N/A'],Brent Mittelstadt is a Research Fellow and British Academy Postdoctoral Fellow in data ethics at the Oxford Internet Institute and a member of the UK National Statistician’s Data Ethics Advisory Committee. His research addresses the ethics of algorithms; machine learning; artificial intelligence and data analytics (‘Big Data’). Over the past five years his focus has broadly been on the ethics and governance of emerging information technologies; including a special interest in medical applications.  ; Dr Mittelstadt's research focuses on ethical auditing of algorithms; including the development of standards and methods to ensure fairness; accountability; transparency; interpretability and group privacy in complex algorithmic systems. His work addresses norms and methods for prevention and systematic identification of discriminatory and ethically problematic outcomes in decisions made by algorithmic and artificially intelligent systems. A recent paper on the legally dubious 'right to explanation' and the lack of meaningful and accountability and transparency mechanisms for automated decision-making in the General Data Protection Regulation; co-authored with Dr Sandra Wachter and Prof. Luciano Floridi; highlights the pressing need for work in these areas.; ,Dr Mittelstadt's research focuses on ethical auditing of algorithms; including the development of standards and methods to ensure fairness; accountability; transparency; interpretability and group privacy in complex algorithmic systems. His work addresses norms and methods for prevention and systematic identification of discriminatory and ethically problematic outcomes in decisions made by algorithmic and artificially intelligent systems. A recent paper on the legally dubious 'right to explanation' and the lack of meaningful and accountability and transparency mechanisms for automated decision-making in the General Data Protection Regulation; co-authored with Dr Sandra Wachter and Prof. Luciano Floridi; highlights the pressing need for work in these areas.; ,['N/A'],Dr Brent Mittelstadt 
brooks-paige,['N/A'],Brooks Paige moved to the UK in 2013 to undertake a D.Phil. in Engineering Science at the University of Oxford. He previously lived in New York City; where he completed an M.A. in Statistics at Columbia University and worked professionally for several years as a software developer and web designer. In the distant past; he was an undergraduate at Amherst College; and remains a strong proponent of a liberal arts science education.; The emerging field of probabilistic programming aims to reduce the technical and cognitive overhead for writing and designing novel probabilistic models; by introducing a specialised programming language as an abstraction barrier between modelling and inference. Brooks' research focus is on developing general-purpose Bayesian inference algorithms which can be applied automatically to generative models written as probabilistic programs; with a particular emphasis on sequential Monte Carlo methods and scalable variational inference; ,The emerging field of probabilistic programming aims to reduce the technical and cognitive overhead for writing and designing novel probabilistic models; by introducing a specialised programming language as an abstraction barrier between modelling and inference. Brooks' research focus is on developing general-purpose Bayesian inference algorithms which can be applied automatically to generative models written as probabilistic programs; with a particular emphasis on sequential Monte Carlo methods and scalable variational inference; ,['N/A'],Dr Brooks Paige 
burcin-becerik-gerber,['N/A'],Dr Becerik-Gerber is an associate professor at the Astani Department of Civil and Environmental Engineering of University of Southern California. Her research falls at the intersection of built environments; machine intelligence; and systems thinking. Specifically; her work focuses on the development of novel methods for the acquisition; modelling; and analysis of the data needed for cognitive (responsive and adaptive) built environments that can perceive; sense; reason and collaborate with their users; and support decision-making; problem solving; and management of resources.; Using multi-dimensional data; she develops algorithms; frameworks and visualisation techniques to improve built-environment resiliency; efficiency; sustainability; and maintainability while increasing user satisfaction. She is the founding director of the Innovation in Integrated Informatics Lab: http://i-lab.usc.edu/ .; Her work has received support worth approximately USD $5 million from a variety of sources. She serves as an associate editor for ASCE’s Journal of Computing in Civil Engineering since 2011. In 2012; she was appointed as the inaugural holder of the Stephen Schrank Early Career Chair in Civil and Environmental Engineering.; She is also the recipient of MIT Technology Review’s TR35 Recognition (2012); NSF CAREER Award (2014); Viterbi Junior Research Award (2016); Mellon Mentoring Award (2017); and Celebration of Engineering & Technology Innovation Award (CETI) in the Outstanding Early Career Researcher category from FIATECH (2018).;  ; Burcin will explore data-driven disaster-prepared buildings while at the Turing. We see an increasing number of man-made and natural disasters striking our built infrastructure (e.g.; building fires; acts of extreme violence; earthquakes). There is little insight in how people behave when exposed to these stressors; how behaviour is influenced by the building’s design; building type; person’s past experiences; people around them; people they are responsible for and so on.; Most theories and models assume behaviour is similar under different stressors. However; the impact on a building’s structure; physical conditions inside the building and the surrounding environment are distinct from one another in different emergency scenarios. Work to date used traditional methods; such as unannounced fire drills; post-surveys; video-recordings; and so on. However; these methods do not trigger natural responses (like trauma; panic; etc.); they lack realistic scenario features (fire; smoke; explosion) or do not provide opportunities for controlled experiments. On the other hand; we cannot expose people to unsafe conditions for moral and legal reasons. Thus; she proposes to study behaviour in different building conditions and under different extreme events using immersive virtual environments and agent based modelling.; The outcomes of this work will inform computational models that attempt to simulate emergency behaviour by predicting actions like evacuation; sheltering; decisions made during performing these actions; and the time it takes to take these actions more accurately. With more accurate data driven models; engineers and architects can develop safer and more secure building designs and operational procedures that are driven by empirical data.; ,Burcin will explore data-driven disaster-prepared buildings while at the Turing. We see an increasing number of man-made and natural disasters striking our built infrastructure (e.g.; building fires; acts of extreme violence; earthquakes). There is little insight in how people behave when exposed to these stressors; how behaviour is influenced by the building’s design; building type; person’s past experiences; people around them; people they are responsible for and so on.; Most theories and models assume behaviour is similar under different stressors. However; the impact on a building’s structure; physical conditions inside the building and the surrounding environment are distinct from one another in different emergency scenarios. Work to date used traditional methods; such as unannounced fire drills; post-surveys; video-recordings; and so on. However; these methods do not trigger natural responses (like trauma; panic; etc.); they lack realistic scenario features (fire; smoke; explosion) or do not provide opportunities for controlled experiments. On the other hand; we cannot expose people to unsafe conditions for moral and legal reasons. Thus; she proposes to study behaviour in different building conditions and under different extreme events using immersive virtual environments and agent based modelling.; The outcomes of this work will inform computational models that attempt to simulate emergency behaviour by predicting actions like evacuation; sheltering; decisions made during performing these actions; and the time it takes to take these actions more accurately. With more accurate data driven models; engineers and architects can develop safer and more secure building designs and operational procedures that are driven by empirical data.; ,['N/A'], Burcin Becerik-Gerber 
caitriona-jackman,Neural networks; Machine learning; Applications (Machine learning); Supervised learning; Time series; ,Dr. Jackman is an Associate Professor of Space Physics. She holds an STFC Ernest Rutherford Fellowship to study planetary and stellar magnetospheres.; Dr. Jackman obtained a BSc in Applied Physics from the University of Limerick in 2003 and a PhD in Planetary Physics from the University of Leicester in 2006. She has held research positions at Imperial College London; and two consecutive fellowships (Leverhulme Trust and Royal Astronomical Society) at University College London. She moved to Southampton in 2013.; Her research interests include large-scale structure of giant planet magnetospheres; the energy budget of Earth’s magnetosphere (with implications for ground-based technology and services); machine learning and complexity science.  She is also very active in outreach and public engagement.; Dr. Jackman's Turing related research is in the field of machine learning - developing automated feature identification methods for use in Space Physics. She has worked for many years on the identification and characterisation of signatures of magnetic reconnection in spacecraft data; and her work with Turing will employ state-of-the-art machine learning methods to ensure the search for evidence of reconnection is quick; unbiased; and repeatable. This work will have wide-reaching implications for our ability to analyse large datasets from spacecraft throughout our solar system.; ,Dr. Jackman's Turing related research is in the field of machine learning - developing automated feature identification methods for use in Space Physics. She has worked for many years on the identification and characterisation of signatures of magnetic reconnection in spacecraft data; and her work with Turing will employ state-of-the-art machine learning methods to ensure the search for evidence of reconnection is quick; unbiased; and repeatable. This work will have wide-reaching implications for our ability to analyse large datasets from spacecraft throughout our solar system.; ,['N/A'],Dr Caitriona Jackman 
camila-rangel-smith,Numerical (Algorithms); Mathematical physics; Visualisation (Computer systems & architectures); Research methods; High dimensional inference; Monte Carlo methods; Simulation; Modelling (Statistical methods & theory); Probability; ,Camila is a Research Data Scientist at The Alan Turing Institute. She holds a PhD in Particle Physics from Université Paris Diderot where she worked on the ATLAS experiment at the Large Hadron Collider at CERN. During her PhD she participated on the discovery of the Higgs Boson particle announced by CERN in 2012. She continued working on ATLAS as a postdoc for Uppsala University where she focused on searches for physics beyond the Standard Model of Particle Physics.; Right before joining the Turing; she worked as Data Scientist in the EdTech sector developing innovative products focused on the assessment process in education.; She is passionate about impactful research  that create public value. Currently she is currently contributing to the Artificial Intelligence for Data Analytics (AIDA) project. ; ,She is passionate about impactful research  that create public value. Currently she is currently contributing to the Artificial Intelligence for Data Analytics (AIDA) project. ; ,['N/A'],Dr Camila Rangel Smith 
carl-rasmussen,['N/A'],Professor Carl Rasmussen is a lecturer in the Machine Learning Group of the Computational and Biological Learning Lab in the Division of Information Engineering at the Department of Engineering in Cambridge.  ; Carl has very broad interests in probabilistic inference in machine learning; covering both unsupervised; supervised and reinforcement learning. He is particularly interested in design and evaluation of non-parametric methods such as Gaussian processes and Dirichlet processes. Exact inference in these models is often intractable; so we need to resort to approximation methods; such as variational techniques or Markov chain Monte Carlo.    ; ,Carl has very broad interests in probabilistic inference in machine learning; covering both unsupervised; supervised and reinforcement learning. He is particularly interested in design and evaluation of non-parametric methods such as Gaussian processes and Dirichlet processes. Exact inference in these models is often intractable; so we need to resort to approximation methods; such as variational techniques or Markov chain Monte Carlo.    ; ,['N/A'],Professor Carl Rasmussen 
carola-bibiane-schonlieb,['N/A'],Carola-Bibiane Schönlieb is a Reader in Applied and Computational Analysis at the Department of Applied Mathematics and Theoretical Physics (DAMTP); University of Cambridge since 2015. There; she is head of the Cambridge Image Analysis group; Director of the Cantab Capital Institute for Mathematics of Information; Co-Director of the EPSRC Centre for Mathematical and Statistical Analysis of Multimodal Clinical Imaging; and since 2011 a fellow of Jesus College Cambridge. Her current research interests focus on variational methods and partial differential equations for image analysis; image processing and inverse imaging problems.; Her research has been acknowledged by scientific prizes; among them the LMS Whitehead Prize 2016; and by invitations to give plenary lectures at several renowned applied mathematics conference; among them the SIAM conference on Imaging Science in 2014; the SIAM conference on Partial Differential Equations in 2015; the IMA Conference on Challenges of Big Data in 2016 and the SIAM annual meeting in 2017. Carola graduated from the Institute for Mathematics; University of Salzburg (Austria) in 2004. From 2004 to 2005 she held a teaching position in Salzburg. She received her PhD degree from the University of Cambridge in 2009. After one year of postdoctoral activity at the University of Göttingen (Germany); she became a Lecturer in at DAMTP in 2010; promoted to Reader in 2015.; She is interested in the interaction of mathematical sciences and imaging. She studies non-smooth and possibly non-convex variational methods and nonlinear partial differential equations for image analysis and inverse imaging problems; among them image reconstruction and restoration; object segmentation; and dynamic image reconstruction and analysis such as fast flow imaging; object tracking and motion analysis in videos.; Moreover; she works on computational methods for large-scale and high-dimensional problems appearing in; e.g. image classification and 3D and 4D imaging. Within this context she is interested in both the rigorous theoretical and computational analysis of the problems considered as well as their practical implementation and their use for real-world applications. Currently; her research focuses on customising variational image analysis and image reconstruction models to applications by learning their setup from real-world data training sets.; To this end she investigates so-called bilevel optimisation techniques in which the solution is typically constrained to a non-smooth variational problem or a nonlinear PDE. She has active interdisciplinary collaborations with clinicians; biologists and physicists on biomedical imaging topics; chemical engineers and plant scientists on image sensing; as well as collaborations with artists and art conservators on digital art restoration.; ,She is interested in the interaction of mathematical sciences and imaging. She studies non-smooth and possibly non-convex variational methods and nonlinear partial differential equations for image analysis and inverse imaging problems; among them image reconstruction and restoration; object segmentation; and dynamic image reconstruction and analysis such as fast flow imaging; object tracking and motion analysis in videos.; Moreover; she works on computational methods for large-scale and high-dimensional problems appearing in; e.g. image classification and 3D and 4D imaging. Within this context she is interested in both the rigorous theoretical and computational analysis of the problems considered as well as their practical implementation and their use for real-world applications. Currently; her research focuses on customising variational image analysis and image reconstruction models to applications by learning their setup from real-world data training sets.; To this end she investigates so-called bilevel optimisation techniques in which the solution is typically constrained to a non-smooth variational problem or a nonlinear PDE. She has active interdisciplinary collaborations with clinicians; biologists and physicists on biomedical imaging topics; chemical engineers and plant scientists on image sensing; as well as collaborations with artists and art conservators on digital art restoration.; ,['N/A'],Professor Carola-Bibiane Schönlieb 
caroline-jay,Human computer interface; Visualisation (Programming languages); Cognitive science; Research methods; ,Caroline Jay is a Chartered Psychologist and Computer Scientist at the University of Manchester. She is the Research Director of the UK Software Sustainability Institute and an advocate for open and reproducible research. She leads the University of Manchester Arm of the BBC Data Science Research Partnership.; Caroline leads research examining the relationship between human health and the environment; with a particular focus on air quality. Poor air quality is responsible for millions of deaths across the world. Tackling this issue is complex. Whilst a solution is to emit fewer pollutants; achieving this is difficult; due to economic and political pressures. The research is examining how to gather the data we need to understand where to focus reductions; how to modify the environment to mitigate the worst effects of pollutants; and how to influence human behaviour to ensure individuals lower their own risk of suffering ill effects. The project builds on symptom data collected via mobile phones in the Britain Breathing and CityVerve projects; extending the approach to Brazil and China.; ,Caroline leads research examining the relationship between human health and the environment; with a particular focus on air quality. Poor air quality is responsible for millions of deaths across the world. Tackling this issue is complex. Whilst a solution is to emit fewer pollutants; achieving this is difficult; due to economic and political pressures. The research is examining how to gather the data we need to understand where to focus reductions; how to modify the environment to mitigate the worst effects of pollutants; and how to influence human behaviour to ensure individuals lower their own risk of suffering ill effects. The project builds on symptom data collected via mobile phones in the Britain Breathing and CityVerve projects; extending the approach to Brazil and China.; ,['N/A'],Dr Caroline Jay 
carsten-maple,Privacy & trust; ,Professor Carsten Maple is Professor of Cyber Systems Engineering at the University of Warwick's Cyber Security Centre (CSC). He is the director of research in Cyber Security working with organisations in key sectors such as manufacturing; healthcare; financial services and the broader public sector to address the challenges presented by today's global cyber environment.; Professor Maple was previously Professor of Applicable Computing and Pro Vice Chancellor (Research and Enterprise) at the University of Bedfordshire. He has an international research reputation and extensive experience of institutional strategy development and interacting with external agencies. He has published over 200 peer reviewed papers and is co-author of the UK Security Breach Investigations Report 2010; supported by the Serious Organised Crime Agency and the Police Central e-crime Unit. Carsten is also co-author of Cyberstalking in the UK; a report supported by the Crown Prosecution Service and Network for Surviving Stalking.; He has given evidence to government committees on issues of anonymity; child safety online. He works with various departments such as the Association of Chief Police Officers; the College of Policing; Interpol; the Equality and Human Rights Commission; the Department of Business Innovation and Skills; to name a few. Additionally he has advised executive and non-executive directors of public sector organisations and multibillion pound private organisations.; Professor Maple is a Fellow of the British Computer Society and Vice chair of the Council of Professors and Heads of Computing; UK.; ,['N/A'],['N/A'],Professor Carsten Maple 
catalina-vallejos,Statistical methods & theory; Uncertainty quantification; ,Catalina is a Chancellor's Fellow at the MRC Human Genetics Unit; where she leads the Biomedical Data Science research group. ; Before moving to Edinburgh; Catalina was part of the first cohort of Turing Research Fellows. As part of her Fellowship; Catalina was also a Group Leader within the Lloyds Register Foundation-Turing Programme on Data-Centric Engineering. ; Between 2014 and 2016; Catalina was a Postdoctoral Fellow in a joint appointment between the MRC Biostatistics Unit (MRC-BSU) and the EMBL European Bioinformatics Institute (EMBL-EBI); both located in Cambridge (UK). In this position; she was a member of the Statistical Genomics research group (MRC-BSU) and the Marioni group (EMBL-EBI) which are respectively lead by Professor Sylvia Richardson and Dr John Marioni. ; Education; Catalina completed a PhD in Statistics at the Department of Statistics of the University of Warwick; under the supervision of Professor Mark Steel. Her PhD thesis covered theoretical and practical aspects of Bayesian inference and survival analysis. She completed her undergraduate and MSc studies in Chile: BSc in Mathematics (Statistics track) and MSc in Statistics at the Faculty of Mathematics of the Pontificia Universidad Católica de Chile. During her BSc studies; Catalina also completed a Certificate in Economics. Her MSc dissertation project was in the area of long memory time times; under the supervision of Dr Wilfredo Palma. ; Catalina's main area of research is on Bayesian statistical methodology; mostly driven by applications in biomedicine. An important area of Catalina's research programme is to translate the methods she develops into open-source analysis tools that can reach the wider community. Currently; Catalina's group focuses on two areas of application: single-cell genomics and electronic health records. ; In terms of methodology; her main interests include:; ,Catalina's main area of research is on Bayesian statistical methodology; mostly driven by applications in biomedicine. An important area of Catalina's research programme is to translate the methods she develops into open-source analysis tools that can reach the wider community. Currently; Catalina's group focuses on two areas of application: single-cell genomics and electronic health records. ; In terms of methodology; her main interests include:; ,['N/A'],Dr Catalina Vallejos 
cecilia-mascolo,['N/A'],Cecilia Mascolo is Full Professor of Mobile Systems in the Computer Laboratory; University of Cambridge; UK. Prior joining Cambridge in 2008; she has been a faculty member in the Department of Computer Science at University College London. Her research interests are in human mobility modelling; mobile and sensor systems and networking and spatio-temporal data analysis.  ; At The Alan Turing Institute; Professor Mascolo hopes to research on aspects related related to interpretation and inference of mobile and wearable sensor data efficiently on devices; on how to make sense of this kind of data in ways which respect its fine grained spatial and temporal granularity. She hopes then to look at how to use the interpretation to improve systems and interventions for users. Her research is highly interdisciplinary and applications of this research will span various other disciplines like; to name a few; urban data science; mobile health and organisation analytics.; ,At The Alan Turing Institute; Professor Mascolo hopes to research on aspects related related to interpretation and inference of mobile and wearable sensor data efficiently on devices; on how to make sense of this kind of data in ways which respect its fine grained spatial and temporal granularity. She hopes then to look at how to use the interpretation to improve systems and interventions for users. Her research is highly interdisciplinary and applications of this research will span various other disciplines like; to name a few; urban data science; mobile health and organisation analytics.; ,['N/A'],Professor Cecilia Mascolo 
charisma-choudhury,Data structures; Multi-agent systems; Robotics; Multi-agent reasoning; Game theory; Neural networks; Neuroscience; Pattern formation; Databases; Human computer interface; Deep learning; Natural language processing; Pattern recognition; Speech recognition; Cognitive science; Social media; Social psychology; Simulation; Estimation theory; Probability; ,Charisma Choudhury is an Associate Professor of the University of Leeds where she is currently serving as the Deputy-Director of the Choice Modelling Centre. Prior to that; she has worked as an Assistant Professor at Bangladesh University of Engineering and Technology (BUET); as a Postdoctoral Research Associate at Massachusetts Institute of Technology (MIT) and as Analysts in RAND Europe; UK and Cambridge Systematics; USA. Charisma holds a PhD and MSc from Massachusetts Institute of Technology (MIT). In recognition of her doctoral research; she has received the Gordon Newell Best Dissertation Prize from the HKSTS and an Honourable Mention from the IATBR.; At the Turing; Charisma plans to work towards bridging choice modelling (CM) with data science and artificial intelligence Travel behaviour models have traditionally relied on manually collected survey data; which are expensive to obtain and thereby generally have limited sample sizes and lower update frequencies. On the other hand; over the last decade; passively collected data sources; commonly termed as Big Data or Ubiquitous Data; have emerged as a very promising source of activity and travel information. The applications of such data have however been primarily limited to visualizations and pattern identifications using machine-learning (ML) techniques.; However; there are criticism about the effectiveness of the ML algorithms due to the black-box approach and over-reliance on the training data; which questions their applicability in human behaviour prediction in the context of disruptive changes (e.g. radically new technologies; collapse of economy; etc.). Bridging ML with choice modelling holds the promise to make the best use of the data. CM techniques; which are based on principles of economic and psychology also have significant potential to augment AI. A particularly topical example is the set of AI models for Connected and Autonomous Vehicles (CAVs). Building on her previous research on driving behaviour modelling using CM techniques; Charisma is keen to augment AI models with behavioural underpinning.; 2011 Faculty for the Future Fellowship; Schlumberger Foundation Awarded each year to 100 exceptionally talented female STEM researchers from developing and emerging economies for pursuing postdoctoral research or doctoral studies at leading universities.; 2010 Gordon Newell Memorial Prize; Hong Kong Society for Transport Studies 1st prize of the annual dissertation competition launched in 2003 to laud the best doctoral dissertation in transportation conducted in any country by a student of Asian origin.; 2007 Honourable Mention; Eric Pas Dissertation Award; International Association of Travel Behaviour Research (IATBR) 2nd prize of the annual dissertation competition launched in 1998 to recognise the outstanding doctoral dissertations in the area of travel behaviour research.; 2004 Martin Family Society for Sustainability Fellowship; MIT Awarded annually to 20 outstanding doctoral students at MIT pursuing sustainability research.; ,At the Turing; Charisma plans to work towards bridging choice modelling (CM) with data science and artificial intelligence Travel behaviour models have traditionally relied on manually collected survey data; which are expensive to obtain and thereby generally have limited sample sizes and lower update frequencies. On the other hand; over the last decade; passively collected data sources; commonly termed as Big Data or Ubiquitous Data; have emerged as a very promising source of activity and travel information. The applications of such data have however been primarily limited to visualizations and pattern identifications using machine-learning (ML) techniques.; However; there are criticism about the effectiveness of the ML algorithms due to the black-box approach and over-reliance on the training data; which questions their applicability in human behaviour prediction in the context of disruptive changes (e.g. radically new technologies; collapse of economy; etc.). Bridging ML with choice modelling holds the promise to make the best use of the data. CM techniques; which are based on principles of economic and psychology also have significant potential to augment AI. A particularly topical example is the set of AI models for Connected and Autonomous Vehicles (CAVs). Building on her previous research on driving behaviour modelling using CM techniques; Charisma is keen to augment AI models with behavioural underpinning.; ,2011 Faculty for the Future Fellowship; Schlumberger Foundation Awarded each year to 100 exceptionally talented female STEM researchers from developing and emerging economies for pursuing postdoctoral research or doctoral studies at leading universities.; 2010 Gordon Newell Memorial Prize; Hong Kong Society for Transport Studies 1st prize of the annual dissertation competition launched in 2003 to laud the best doctoral dissertation in transportation conducted in any country by a student of Asian origin.; 2007 Honourable Mention; Eric Pas Dissertation Award; International Association of Travel Behaviour Research (IATBR) 2nd prize of the annual dissertation competition launched in 1998 to recognise the outstanding doctoral dissertations in the area of travel behaviour research.; 2004 Martin Family Society for Sustainability Fellowship; MIT Awarded annually to 20 outstanding doctoral students at MIT pursuing sustainability research.; ,Dr Charisma Choudhury 
charles-raab,['N/A'],Professor Charles Raab is a political scientist; conducts research on privacy; data protection; surveillance; security; and on regulatory policy and practice. He has published extensively on those topics. He is a Director of CRISP (Centre for Research into Information; Surveillance and Privacy); and co-Chairs IDEPP (Independent Digital Ethics Panel for Policing). He is a member of the Turing's Data Ethics Group.; Professor Raab aims to stimulate discussion and awareness of ethical and related legal and social issues in data science among Turing researchers; and to learn about their approaches to these subjects in their work. He will organise workshops and seminars on these issues.; ,Professor Raab aims to stimulate discussion and awareness of ethical and related legal and social issues in data science among Turing researchers; and to learn about their approaches to these subjects in their work. He will organise workshops and seminars on these issues.; ,['N/A'],Professor Charles Raab 
charles-sutton,['N/A'],Charles Sutton is a Reader (equivalent to Associate Professor in Machine Learning at the University of Edinburgh. He has over 50 publications in a broad range of applications of probabilistic machine learning. His work in machine learning for software engineering has won an ACM Distinguished Paper Award. His PhD is from the University of Massachusetts Amherst; and he has done postdoctoral work at the University of California Berkeley. He is currently Director of the EPSRC Centre for Doctoral Training in Data Science at the University of Edinburgh.; Charles's research focuses on developing new machine learning methods that are motivated by the demands of new; cutting-edge practical problems. He develops new techniques in probabilistic machine learning; approximate inference in graphical models; and more recently deep learning. In general; these motivating applications come from a wide range of areas; including natural language processing; analysis of computer systems; software engineering; sustainable energy; and exploratory data analysis. Most recently; he is particularly fascinated by two important application areas: machine learning and NLP methods to improve software development; and machine learning and artificial intelligence methods to support the full practical workflow of data science.; ,Charles's research focuses on developing new machine learning methods that are motivated by the demands of new; cutting-edge practical problems. He develops new techniques in probabilistic machine learning; approximate inference in graphical models; and more recently deep learning. In general; these motivating applications come from a wide range of areas; including natural language processing; analysis of computer systems; software engineering; sustainable energy; and exploratory data analysis. Most recently; he is particularly fascinated by two important application areas: machine learning and NLP methods to improve software development; and machine learning and artificial intelligence methods to support the full practical workflow of data science.; ,['N/A'],Dr Charles Sutton 
charlotte-deane,['N/A'],Charlotte has a wide range of research interests concerning protein structure prediction and protein interaction networks; combining both theoretical work and empirical analyses. Originally a Chemist she has moved across many traditional research disciplines and her research group is now based in Statistics. Charlotte completed her PhD in Cambridge and after a Wellcome trust fellowship in California she returned to Oxford as a University Lecturer in 2002.  ; Charlotte’s group works in the protein bioinformatics area. Currently the research focuses on understanding protein structure and improving our ability to model and design proteins. To this end; the group is developing a rigorous definition of evolutionary relationships between proteins of differing structures using; in the initial stages; genomic data and building novel protein structure prediction software. In a separate thread the group has been examining protein-protein interaction networks in terms of both their quality and use as a predictive tool.; ,Charlotte’s group works in the protein bioinformatics area. Currently the research focuses on understanding protein structure and improving our ability to model and design proteins. To this end; the group is developing a rigorous definition of evolutionary relationships between proteins of differing structures using; in the initial stages; genomic data and building novel protein structure prediction software. In a separate thread the group has been examining protein-protein interaction networks in terms of both their quality and use as a predictive tool.; ,['N/A'],Professor Charlotte Deane 
chenlei-leng,['N/A'],Chenlei obtained his BSc in mathematics from the University of Science and Technology of China and his PhD in statistics from the University of Wisconsin-Madison. He held regular and visiting faculty positions in China; Germany and Singapore before joining the University of Warwick as a professor of statistics in 2013.; Chenlei is a statistician working mainly on developing novel statistical methods for analysing complex data. His recent research interests have been focused on high-dimensional data analysis; correlated data analysis; network data analysis and statistical learning. His works have found applications in medicine; biology; engineering; social sciences and so on. As a faculty fellow; he looks forward to the opportunity to work on projects where statistical analysis can be useful.; ,Chenlei is a statistician working mainly on developing novel statistical methods for analysing complex data. His recent research interests have been focused on high-dimensional data analysis; correlated data analysis; network data analysis and statistical learning. His works have found applications in medicine; biology; engineering; social sciences and so on. As a faculty fellow; he looks forward to the opportunity to work on projects where statistical analysis can be useful.; ,['N/A'],Professor Chenlei Leng 
chris-dent,['N/A'],Dr. Chris Dent is Reader (Associate Professor) in Industrial Mathematics in the School of Mathematics at the University of Edinburgh and held research and academic positions at Heriot-Watt; Marburg; Edinburgh and Durham Universities. He holds an MA in Mathematics (Cambridge University); PhD in Theoretical Physics (Loughborough University); and MSc in Operational Research (Edinburgh University). Since 2007 he has worked full time in energy systems analysis; and currently concentrates on security of supply risk analysis; and use of large scale computer models in decision making. He is a Senior Member of the IEEE; a Fellow of the Operational Research Society; and a Chartered Engineer.; Energy systems are growing in complexity and there is a need to manage uncertainty on a range of timescales (from short run uncertainty in renewable generation output; to uncertainty in planning background when taking major policy and capital planning decisions). Dr. Dent is broadly interested in the application of data science across energy system and planning; and has a particular interest in how approaches from data science can productively be taken to practical application in government and industry.; ,Energy systems are growing in complexity and there is a need to manage uncertainty on a range of timescales (from short run uncertainty in renewable generation output; to uncertainty in planning background when taking major policy and capital planning decisions). Dr. Dent is broadly interested in the application of data science across energy system and planning; and has a particular interest in how approaches from data science can productively be taken to practical application in government and industry.; ,['N/A'],Dr Chris Dent 
chris-oates,['N/A'],Chris is a Professor of Statistics at Newcastle University and Group Leader for the data-centric engineering programme at the Turing.; Chris’ research aims to develop probabilistic numerical methods for simulation-based risk management. The use of computers to simulate physical governing equations introduces numerical and discretisation errors; his research aims to account for these with statistical methods; to provide a proper quantification of risk for decisions that are based on simulation output.; ,['N/A'],['N/A'],Professor Chris Oates 
chris-russell,['N/A'],['N/A'],['N/A'],['N/A'],Page not found
chris-williams,Artificial intelligence; Machine learning; Computer vision; Supervised learning; Unsupervised learning; Time series; ,Chris Williams is Professor of Machine Learning in the School of Informatics; University of Edinburgh. He obtained his MSc (1990) and PhD (1994) at the University of Toronto; under the supervision of Geoff Hinton. He was a member of the Neural Computing Research Group at Aston University from 1994 to 1998; and has been at the University of Edinburgh since 1998.; Chris is interested in a wide range of theoretical and practical issues in machine learning; statistical pattern recognition; probabilistic graphical models and computer vision. This includes theoretical foundations; the development of new models and algorithms; and applications. His main areas of research are in models for understanding time-series; visual object recognition and image understanding; unsupervised learning; and Gaussian processes. At the Turing he also has interests in improving the data analytics process; looking to address the issues of data understanding and preparation that are widely quoted as taking around 80% of the time in a typical data mining project.; ,Chris is interested in a wide range of theoretical and practical issues in machine learning; statistical pattern recognition; probabilistic graphical models and computer vision. This includes theoretical foundations; the development of new models and algorithms; and applications. His main areas of research are in models for understanding time-series; visual object recognition and image understanding; unsupervised learning; and Gaussian processes. At the Turing he also has interests in improving the data analytics process; looking to address the issues of data understanding and preparation that are widely quoted as taking around 80% of the time in a typical data mining project.; ,['N/A'],Professor Chris Williams 
christina-hitrova,Ethics; Data science of government & politics; ,Christina is a Research Assistant in Digital Ethics within the public policy programme. Her interests lie in understanding the ethical implications of new technologies and innovation and in studying the role of regulation through law and policy.; Prior to joining the Turing; Christina worked within international research consortia on the ethical; privacy; and data protection implications of new technologies; including civil drones; law enforcement investigatory tools; connected mobility; and blockchain technology. Christina’s work has sought to support responsible innovation and has covered issues such as GDPR compliance; ethical research practices; privacy and data protection by design; transparency and accountability in surveillance tools; as well as prevention of abuse and misuse of research results.; Christina brings a legal perspective to the work of the public policy programme. She has a Master of Law from the University of Zurich and the Catholic University of Leuven; where she focused her studies on International and European Law and wrote her Master’s thesis about data protection; surveillance and the EU-US international data flows. Building on this; Christina has gained experience in a diverse set of fields; including international relations; human rights; and competition law. Notably; as a graduate trainee with the Legal Service of the European Commission; Christina worked on EU trade policy and litigation.; At the Turing; Christina contributes to the Public Policy programme's research. Christina works on the Turing-ICO collaborative project ExplAIn developing a guidance for AI explainability; she explores the ethical foundations of data science in the criminal justice system; and researches the ethics of machine learning research in children's social care. Christina also organises the Public policy programme's lecture series Driving data futures that seeks to bring cutting edge research surrounding the intersection of new technologies; ethics; and policy to the attention of the general public.; Beyond the Public Policy programme; Christina enables the work of the Data Ethics Group - an interest group of cross-university experts providing strategic data ethics advice to the public sector - and coordinates and administers the Turing's internal ethics approval body - the Ethics Advisory Group. ; ,At the Turing; Christina contributes to the Public Policy programme's research. Christina works on the Turing-ICO collaborative project ExplAIn developing a guidance for AI explainability; she explores the ethical foundations of data science in the criminal justice system; and researches the ethics of machine learning research in children's social care. Christina also organises the Public policy programme's lecture series Driving data futures that seeks to bring cutting edge research surrounding the intersection of new technologies; ethics; and policy to the attention of the general public.; Beyond the Public Policy programme; Christina enables the work of the Data Ethics Group - an interest group of cross-university experts providing strategic data ethics advice to the public sector - and coordinates and administers the Turing's internal ethics approval body - the Ethics Advisory Group. ; ,['N/A'], Christina Hitrova 
christophe-andrieu,Parallel computing; Stochastic (Mathematical modelling); Stochastic optimisation; Probabilistic programming; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Time series; Asymptotic (Statistical methods & theory); Estimation theory; Modelling (Statistical methods & theory); Probability; Calculus & analysis; ,Christophe Andrieu is professor of statistical science at the University of Bristol. He gained his PhD in 1998 at Paris XV; in Signal Processing. After a 3 year postdoc in the Signal Processing group in Cambridge he joined Bristol in 2001.; Christophe's main research focus is on computational methods for statistical inference; ranging from sampling to optimisation techniques.; ,Christophe's main research focus is on computational methods for statistical inference; ranging from sampling to optimisation techniques.; ,['N/A'],Professor Christophe Andrieu 
christopher-burr,Ethics; Developmental psychology; Data science of government & politics; Cognitive science; Social psychology; ,Dr Christopher Burr is a Senior Research Associate and a philosopher of cognitive science; specialising in the ethical design and use of data-driven technologies and the interaction between human agents and artificial intelligence.; His research interests include topics in bioethics (e.g. how AI technologies should be used to support or deliver mental healthcare); cognitive science (e.g. how to understand the risks of intelligent systems influencing and shaping human judgement and choice behaviour); the study of well-being (e.g. how can we use digital technologies to measure and promote individual and social well-being); and human-computer interaction (e.g. how to design intelligent systems that promote intentional use and self-determination).; Prior to starting at The Alan Turing Institute; he held postdoctoral research posts at the Oxford Internet Institute; University of Oxford (2018–19); working as part of the Data Ethics Lab; and the Department of Computer Science; University of Bristol (2017–18). He completed his PhD in Philosophy of Cognitive Science at the University of Bristol in 2017; funded by a European Research Council scholarship.; He has previously advised organisations including the Centre for Data Ethics and Innovation; the Department of Health and Social Care; and has conducted interviews with media organisations including the New York Times; BBC; and Vox.; ,['N/A'],['N/A'],Dr Christopher Burr 
christopher-yau,Statistical methods & theory; Monte Carlo methods; High dimensional inference; Uncertainty quantification; Machine learning; Applications (Machine learning); ,Christopher is Professor of Artificial Intelligence at the University of Manchester and co-Director of the Health Data Research UK-Turing Wellcome PhD Programme in Health Data Science. He received his undergraduate degree in Engineering at the University of Cambridge in 2004 and a D.Phil in Statistics from the University of Oxford in 2009. He was previously a Lecturer in Statistics at Imperial College London; Associate Professor in Genomic Medicine at the University of Oxford; and Professor of Artificial Intelligence at the University of Birmingham.; His research is focused on issues related to the interpretation of high-dimensional data arising from modern molecular technologies and health systems and how such data can be used to give insights into the molecular basis of human disease particularly cancer. His efforts in this area span a spectrum of areas from core statistical and machine learning methodological research to wet lab-based experimental investigations to translational clinical research. He currently co-leads the Machine Learning Clinical Interpretation Partnership as part of the Genomics England 100;000 Genomes Project.; ,His research is focused on issues related to the interpretation of high-dimensional data arising from modern molecular technologies and health systems and how such data can be used to give insights into the molecular basis of human disease particularly cancer. His efforts in this area span a spectrum of areas from core statistical and machine learning methodological research to wet lab-based experimental investigations to translational clinical research. He currently co-leads the Machine Learning Clinical Interpretation Partnership as part of the Genomics England 100;000 Genomes Project.; ,['N/A'],Professor Christopher Yau 
claire-grover,['N/A'],Claire Grover a Senior Research Fellow in the Institute for Language; Cognition and Computation (ILCC) in the School of Informatics; University of Edinburgh. Her background is in linguistics and natural language processing (NLP); with a focus on the syntactic and semantic analysis of text. Most recently she has been working on text mining; a technology which helps users to make sense of vast amounts of textual data by automatically recognising relevant pieces of information and extracting them to a format which allows for further analysis.; The topic of her fellowship is text mining of clinical records. In collaboration with a neurologist and a radiologist (Dr William Whiteley and Dr Grant Mair; Centre for Clinical Brain Sciences; University of Edinburgh); her group has been developing a text mining system to analyse radiologists' reports of brain imaging (MRI and CT scans). They aim to extend our work to 1.2 million records in NHS Scotland in order to test the feasibility of our approach to clinical epidemiology and trials. For the fellowship she wants to consider text mining for a wider range of clinical reports by putting into place reusable; state-of-the-art tools and infrastructure; and exploring machine learning methods to improve performance and to enable adaptation to new clinical domains.; ,The topic of her fellowship is text mining of clinical records. In collaboration with a neurologist and a radiologist (Dr William Whiteley and Dr Grant Mair; Centre for Clinical Brain Sciences; University of Edinburgh); her group has been developing a text mining system to analyse radiologists' reports of brain imaging (MRI and CT scans). They aim to extend our work to 1.2 million records in NHS Scotland in order to test the feasibility of our approach to clinical epidemiology and trials. For the fellowship she wants to consider text mining for a wider range of clinical reports by putting into place reusable; state-of-the-art tools and infrastructure; and exploring machine learning methods to improve performance and to enable adaptation to new clinical domains.; ,['N/A'],Dr Claire Grover 
claire-haworth,Applications (Machine learning); Natural language processing; Software framework development; Ethics; Social networks; Research methods; Social media; Social psychology; Causality; High dimensional inference; Time series; ,Claire Haworth is Reader in Behavioural Genetics at the University of Bristol. Her role is split between the School of Psychological Science and the Bristol Medical School; and she is a member of the MRC Integrative Epidemiology Unit. With Dr Oliver Davis she co-directs the Dynamic Genetics Lab. Claire studied Experimental Psychology at the University of Oxford and completed her MSc and PhD at King’s College London (KCL). Following her PhD she was funded by two consecutive fellowships; an interdisciplinary fellowship from the MRC and ESRC; and a research fellowship from the British Academy. She held academic positions at KCL and the University of Warwick before moving to Bristol in 2015.; Claire's work focuses on why people are different from one another. In particular she is interested in what factors influence our mental health and wellbeing across the life course. She uses a range of techniques to investigate individual differences; including genetically informative studies that tell us about the role of nature (genes) and nurture (environments) in creating these differences between us. For example; Claire has done work on how genetic and environmental influences may change across development; in different environmental contexts; and in response to interventions.; A key new direction in Claire's research is the use of information from social media platforms; and what benefits and challenges social media use has for mental health and wellbeing. As well as the importance of considering the ethical perspective of using digital technologies to understand; explore and promote mental health.; Claire was awarded the British Psychological Society's Spearman Medal in 2017 for outstanding published work in Psychology; and a Philip Leverhulme Prize in Psychology in 2018.; ,Claire's work focuses on why people are different from one another. In particular she is interested in what factors influence our mental health and wellbeing across the life course. She uses a range of techniques to investigate individual differences; including genetically informative studies that tell us about the role of nature (genes) and nurture (environments) in creating these differences between us. For example; Claire has done work on how genetic and environmental influences may change across development; in different environmental contexts; and in response to interventions.; A key new direction in Claire's research is the use of information from social media platforms; and what benefits and challenges social media use has for mental health and wellbeing. As well as the importance of considering the ethical perspective of using digital technologies to understand; explore and promote mental health.; ,Claire was awarded the British Psychological Society's Spearman Medal in 2017 for outstanding published work in Psychology; and a Philip Leverhulme Prize in Psychology in 2018.; ,Dr Claire Haworth 
conor-houghton,Complexity (Algorithms); Numerical (Algorithms); Linguistics; Information theory (Statistical methods & theory); Uncertainty quantification; Neuroscience; Nonlinear dynamics; Dynamical systems & differential equations; Information theory (Applied mathematics); Unsupervised learning; Supervised learning; Deep learning; ,Conor Houghton is a Reader in Mathematical Neuroscience in the Computer Science Department at Bristol University. He came to Bristol in 2012 from Trinity College Dublin where he had been a lecturer in the School of Mathematics. This followed a year as a Fulbright scholar at Columbia University and a research fellowship in Cambridge; where he also did his PhD in mathematical particle physics.; In his research Conor Houghton is principally focussed on mathematical descriptions of the brain and; in particular; on models of mood and movement and on computational approaches to neurolinguistics. His work on how to quantify information in the electrophysiological data recorded during typical neuroscience experiments has led to an interest in how to apply information theory to data.; ,In his research Conor Houghton is principally focussed on mathematical descriptions of the brain and; in particular; on models of mood and movement and on computational approaches to neurolinguistics. His work on how to quantify information in the electrophysiological data recorded during typical neuroscience experiments has led to an interest in how to apply information theory to data.; ,['N/A'], Conor Houghton 
conrad-bessant,Algorithms; Numerical analysis; Neural networks; Machine learning; Pattern recognition; Supervised learning; Programming languages; Software framework development; ,Conrad Bessant is Professor of Bioinfromatics at Queen Mary; University of London. After gaining a degree in physics and a PhD in machine learning for analytical science; he began working in bioinformatics in 2000. In additional to his academic career; Prof Bessant has worked as a software developer and freelance technical journalist.; Professor Bessant is interested in leveraging the latest proteomics technologies to answer fundamental questions in biology and medicine. In particular; his lab focuses on the development of new and improved methodologies to analyse proteomic mass spectrometry data; and integrate proteomics data with other types of data in multi-omic studies. Current applications of our work include understanding the role of cell signaling in cancer (with Dr Pedro Cutillas; Barts Cancer Institute); studying protein expression during viral infection (with Dr David Matthews; University of Bristol) and large scale evidence-based genome annotation.; Key expertise in the lab includes statistics; machine learning; sequence analysis; proteome informatics and software development.; ,Professor Bessant is interested in leveraging the latest proteomics technologies to answer fundamental questions in biology and medicine. In particular; his lab focuses on the development of new and improved methodologies to analyse proteomic mass spectrometry data; and integrate proteomics data with other types of data in multi-omic studies. Current applications of our work include understanding the role of cell signaling in cancer (with Dr Pedro Cutillas; Barts Cancer Institute); studying protein expression during viral infection (with Dr David Matthews; University of Bristol) and large scale evidence-based genome annotation.; Key expertise in the lab includes statistics; machine learning; sequence analysis; proteome informatics and software development.; ,['N/A'],Professor Conrad Bessant 
coralia-cartis,['N/A'],Coralia Cartis (BSc Mathematics; Babesh-Bolyai University; Romania; PhD Mathematics; University of Cambridge (2005)) has joined the Mathematical Institute at Oxford and Balliol College in 2013 as Associate Professor in Numerical Optimization. Previously; she worked as a research scientist at Rutherford Appleton Laboratory and as a postdoctoral researcher at Oxford University. Between 2007-2013; she was a (permanent) lecturer in the School of Mathematics; University of Edinburgh.  ; Assoc. Prof. Cartis’ research is on optimisation; algorithm development; analysis and implementation for a variety of problem classes (linear; convex; non-convex; smooth/non-smooth; stochastic); suitable for large scale. In the past few years; her research focused on complexity of non-convex optimisation problems; compressed sensing; parameter estimation for climate modelling. Currently; she is investigating optimisation methods for problems with imperfect or corrupted information; such as in the case of stochastic optimisation.; ,Assoc. Prof. Cartis’ research is on optimisation; algorithm development; analysis and implementation for a variety of problem classes (linear; convex; non-convex; smooth/non-smooth; stochastic); suitable for large scale. In the past few years; her research focused on complexity of non-convex optimisation problems; compressed sensing; parameter estimation for climate modelling. Currently; she is investigating optimisation methods for problems with imperfect or corrupted information; such as in the case of stochastic optimisation.; ,['N/A'],Professor Coralia Cartis 
damon-wischik,Visualisation (Programming languages); Simulation; ,Damon completed his PhD; supervised by Frank Kelly in the Statistical Laboratory; Cambridge. He is now a research fellow at Trinity College; Cambridge.; ,['N/A'],['N/A'], Damon Wischik 
dan-olteanu,['N/A'],"Dan Olteanu is Professor of Computer Science at Oxford and Computer Scientist at RelationalAI. He also consulted for LogicBlox and taught at the universities of California Berkeley; Munich; Saarland; and Heidelberg. He received his PhD in Computer Science from University of Munich in 2005. His research interests are in databases and adjacent areas. Dan contributed to relational and XML query processing; incomplete information and probabilistic databases; and more recently to factorised databases; in-database analytics; and the LogicBlox commercial system. He co-authored the book ""Probabilistic Databases"" (2011). He served as associate editor for PVLDB'13 and IEEE TKDE; track chair for ICDE'15; group leader for SIGMOD'15; and vice chair for SIGMOD'17.  ; His research interests are in database systems and theory. Dan contributed to XML query processing; incomplete information and probabilistic databases; and more recently to factorized databases and the industrial-strength LogicBlox database and analytics system. He is a co-author of the book “Probabilistic Databases” (2011). Olteanu has served as associate editor for PVLDB’13 and IEEE TKDE; as track chair for ICDE’15; group leader for SIGMOD’15; and will serve as vice chair for SIGMOD’17. His current research is supported by awards from Amazon; Google; and LogicBlox; and an ERC consolidator grant.    ; ",His research interests are in database systems and theory. Dan contributed to XML query processing; incomplete information and probabilistic databases; and more recently to factorized databases and the industrial-strength LogicBlox database and analytics system. He is a co-author of the book “Probabilistic Databases” (2011). Olteanu has served as associate editor for PVLDB’13 and IEEE TKDE; as track chair for ICDE’15; group leader for SIGMOD’15; and will serve as vice chair for SIGMOD’17. His current research is supported by awards from Amazon; Google; and LogicBlox; and an ERC consolidator grant.    ; ,['N/A'],Professor Dan Olteanu 
dan-stowell,Multi-agent systems; Multi-agent reasoning; Neural networks; Information retrieval; Deep learning; Pattern recognition; Differential privacy; Data science of government & politics; Ethics; Research methods; ,Dan Stowell is senior researcher in machine listening - which means using computation to understand sound signals. He co-leads the Machine Listening Lab at Queen Mary University of London; based in the Centre for Digital Music; and is also a Turing Fellow at The Alan Turing Institute. Dan has worked on voice; music and environmental soundscapes; and is currently leading a five-year EPSRC fellowship project researching the automatic analysis of bird sounds. His first degree was from Cambridge University; and his PhD from Queen Mary University of London.; Machine learning and environmental sounds; including bird sound.; ,Machine learning and environmental sounds; including bird sound.; ,['N/A'],Dr Dan Stowell 
dani-arribas-bel,Data science of government & politics; Spatial analytics; Neural networks; Machine learning; Unsupervised learning; Deep learning; Computer vision; ,"Dani Arribas-Bel is senior lecturer in Geographic Data Science at the Department of Geography and Planning of the University of Liverpool. Prior to his appointment in 2015; he held positions at the University of Birmingham; the VU University in Amsterdam; Arizona State University; and Universidad de Zaragoza. He holds honorary positions at the University of Chicago's Center for Spatial Data Science; the Center for Geospatial Sciences of the University of California Riverside; and the Smart Cities Chair of Universitat the Barcelona.; His research combines urban studies; computational methods and new forms of data; and has been published in journals such as PLOS ONE; Journal of Urban Economics; Demography; Geographical Analysis; or Environment and Planning (A/B/C). He is member of the development team of PySAL; the Python library for spatial analysis; currently serves as co-editor of the journal ""Environment and Planning B - Urban Analytics & City Science” and the ""Journal of the Royal Statistical Society Series A - Statistics in Society”; and chairs the Quantitative Methods Research Group of the Royal Geographical Society.; Dani is interested in how data science and AI can help us better understand cities and their evolution. His main research integrates new forms of data and methods with traditional spatial analysis and modelling to create representations of the spatial structure of cities. At the Turing; he is focused on using AI and machine learning to leverage satellite imagery; creating high-resolution descriptions of cities that can be updated as new streams of images become available.; ",Dani is interested in how data science and AI can help us better understand cities and their evolution. His main research integrates new forms of data and methods with traditional spatial analysis and modelling to create representations of the spatial structure of cities. At the Turing; he is focused on using AI and machine learning to leverage satellite imagery; creating high-resolution descriptions of cities that can be updated as new streams of images become available.; ,['N/A'],Dr Dani Arribas-Bel 
daniel-birks,Multi-agent systems; Operations research; Pattern formation; Machine learning; Social data science; Data science of government & politics; Research methods; Simulation; ,Dr Daniel Birks is the University Academic Fellow in Quantitative Policing and Crime Data Analytics at the School of Law; University of Leeds. He holds degrees in criminology; cognitive science; artificial intelligence and computer science; and has previously held research and teaching positions at UCL; Griffith University; Brisbane; and the Australian Research Council's Centre of Excellence in Policing and Security. His research interests lie at the intersection of criminology; complexity science and AI.; Dan's research focuses on exploring the potential of data science and AI to better understand; predict; and disrupt crime problems. He has over 15 years' experience working in applied criminal justice and public policy settings; and has collaborated extensively with government and policing agencies in the UK and Australia. He is particularly interested in how commonly observed patterns of crime emerge from the individual behaviours of potential victims; offenders and crime preventers and their interactions with the urban environment. To investigate these questions; he applies methods from a variety of disciplines; including data science; agent-based computing; and epidemiology.; Dan is currently part of the N8 Police Research Partnership - a £7million research collaboration between eight research intensive universities and 11 police services across the north of England. In addition; he is investigator on projects evaluating the effectiveness of computational methods in forecasting crime patterns in Australian communities (funded by the Australian Institute of Criminology); and in analysing Automatic Number Plate Recognition data collected by UK police (N8 Police Research Partnership).; ,Dan's research focuses on exploring the potential of data science and AI to better understand; predict; and disrupt crime problems. He has over 15 years' experience working in applied criminal justice and public policy settings; and has collaborated extensively with government and policing agencies in the UK and Australia. He is particularly interested in how commonly observed patterns of crime emerge from the individual behaviours of potential victims; offenders and crime preventers and their interactions with the urban environment. To investigate these questions; he applies methods from a variety of disciplines; including data science; agent-based computing; and epidemiology.; Dan is currently part of the N8 Police Research Partnership - a £7million research collaboration between eight research intensive universities and 11 police services across the north of England. In addition; he is investigator on projects evaluating the effectiveness of computational methods in forecasting crime patterns in Australian communities (funded by the Australian Institute of Criminology); and in analysing Automatic Number Plate Recognition data collected by UK police (N8 Police Research Partnership).; ,['N/A'],Dr Daniel Birks 
daniel-williamson,Applications (Machine learning); Deep learning; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Time series; Modelling (Statistical methods & theory); Probability; ,"Daniel Williamson is Senior lecturer in statistics at the University of Exeter. He gained is PhD at Durham University in 2010 on the topic of developing Bayesian policy support for decision problems involving data from expensive computer models. Following a postdoctoral research position at Durham University; he was appointed as a lecturer at the University of Exeter in 2013. ; Dr Williamson's research involves developing methodology for integrating physical data with the output from expensive computer simulators in order to calibrate those simulators and thus learn about the real world; with specific application to climate modelling. This area of research is known as Uncertainty Quantification (UQ) and; in application to climate models; ""climate model tuning"". His other Turing related research looks to develop theory to link algorithms for Bayesian inference with big data in applications; to justifiable decision making with implications for machine decisions and artificial intelligence. ; Daniel has won an International Society for Bayesian Analysis Savage award (Hon. Men) for his PhD thesis and won the Lindley prize for outstanding research article in 2016. He won an EPSRC fellowship in 2013 for developing Bayesian methods in Uncertainty Quantification for hierarchies of models with spatio-temporal output. ; ","Dr Williamson's research involves developing methodology for integrating physical data with the output from expensive computer simulators in order to calibrate those simulators and thus learn about the real world; with specific application to climate modelling. This area of research is known as Uncertainty Quantification (UQ) and; in application to climate models; ""climate model tuning"". His other Turing related research looks to develop theory to link algorithms for Bayesian inference with big data in applications; to justifiable decision making with implications for machine decisions and artificial intelligence. ; ",Daniel has won an International Society for Bayesian Analysis Savage award (Hon. Men) for his PhD thesis and won the Lindley prize for outstanding research article in 2016. He won an EPSRC fellowship in 2013 for developing Bayesian methods in Uncertainty Quantification for hierarchies of models with spatio-temporal output. ; ,Dr Daniel Williamson 
daniel-wilson,Data science of government & politics; ,Daniel C.S. Wilson is a historian of modern Britain; with a focus on science and technology. He has degrees in History and Philosophy; and has held research fellowships in Cambridge and Paris. Prior to joining The Alan Turing Institute; Daniel taught in the Department of History and Philosophy of Science at Cambridge; where he also worked on the 'Technology & Democracy' project at CRASSH: an inquiry into the politics of the digital.; Daniel is interested in both the past and present modalities of what it means to be 'living with machines'. His current research focuses among other things on the politics of work; infrastructure; energy and 'Pandaemonium'.; ,Daniel is interested in both the past and present modalities of what it means to be 'living with machines'. His current research focuses among other things on the politics of work; infrastructure; energy and 'Pandaemonium'.; ,['N/A'],Dr Daniel Wilson 
danielle-paul,Robotics; Neural networks; Pattern formation; Applications (Machine learning); Computer vision; Deep learning; Pattern recognition; Semi-supervised learning; Supervised learning; Unsupervised learning; ,Dr Danielle Paul started her research career as a BHF funded PhD student using electron microscopy to study the structure of the filamentous proteins in cardiac muscle. She completed her PhD at Imperial College London in 2006 and then went on to the structural Biology department at the Institute of Cancer Research for her first post-doctoral position.; When her first son was born she and her family moved to Bristol and when her daughter was born in 2012 she took a career break for 2.5 years. Dr Paul returned to academia taking up a BHF Career Re-Entry fellowship in the school of Physiology; Pharmacology and Neuroscience at Bristol University in 2015. The focus of her lab’s work is to determine high resolution 3D models of the cardiac thin filament using state of the art image processing techniques and the recent advances made in the field of cryo-electron microscopy.; The recent “Cryo-EM revolution” has also produced a massive increase in the amount of data; in the form of EM images; with one session on the microscope producing up to 8TB. Powerful; well developed software exists for the analysis of data sets of this size when the object of study conforms to certain set of constraints i.e. they are either a globular protein or a helical filament. However; there are many harder to reach systems; such as the hybrid cases of globular proteins interacting with filaments. The number of biological and synthetic systems that involve such an arrangement is vast. ; To this end; Dr Paul and her team will develop software to facilitate single particle analysis of Cryo-EM images of elongated / filamentous macromolecular complexes decorated with accessory globular proteins. The main objective is to create a self-contained image processing tool ‘PROOF: Software for the identification of accessory PROteins On Filaments’; that takes high resolution Cryo-EM movie data; automatically identifies filamentous proteins; straightens them if necessary and crucially locates the position of the accessory binding proteins. Segmentation/boxing of the filament can occur and ‘particles’ can then be extracted. These selected particles can then be fed into existing single particle image processing packages.; ,The recent “Cryo-EM revolution” has also produced a massive increase in the amount of data; in the form of EM images; with one session on the microscope producing up to 8TB. Powerful; well developed software exists for the analysis of data sets of this size when the object of study conforms to certain set of constraints i.e. they are either a globular protein or a helical filament. However; there are many harder to reach systems; such as the hybrid cases of globular proteins interacting with filaments. The number of biological and synthetic systems that involve such an arrangement is vast. ; To this end; Dr Paul and her team will develop software to facilitate single particle analysis of Cryo-EM images of elongated / filamentous macromolecular complexes decorated with accessory globular proteins. The main objective is to create a self-contained image processing tool ‘PROOF: Software for the identification of accessory PROteins On Filaments’; that takes high resolution Cryo-EM movie data; automatically identifies filamentous proteins; straightens them if necessary and crucially locates the position of the accessory binding proteins. Segmentation/boxing of the filament can occur and ‘particles’ can then be extracted. These selected particles can then be fed into existing single particle image processing packages.; ,['N/A'],Dr Danielle Paul 
darren-price,Artificial intelligence; Real time computing; Deep learning; Unsupervised learning; Research methods; Statistical methods & theory; High dimensional inference; Monte Carlo methods; Simulation; ,Dr Darren Price is a Senior Research Fellow in Experimental Particle Physics at the University of Manchester. Dr Price read Mathematics at Cambridge; followed by an MSc and PhD in Experimental Particle Physics at the Universities of Manchester and Lancaster. Following his PhD he moved to Indiana University in the USA as a postdoctoral researcher with secondment to the Fermi National Accelerator Laboratory in Chicago and to the Large Hadron Collider at CERN in Geneva.; Following research in the USA and Switzerland; Dr Price joined the University of Manchester first as an EU-funded Marie Curie Research Fellow and currently as an STFC Ernest Rutherford Research Fellow. He also serves as Director of the STFC 4IR Centre for Doctoral Training in Data Intensive Science whose remit spans Particle Physics; Astrophysics; Computer Science and Nuclear Physics at the Universities of Manchester; Sheffield and Lancaster.; His research interests include study of anomalies in rare electroweak interactions in high-energy particle collisions; new approaches to the search for dark matter; and tackling the challenges of data filtering; classification; simulation and modelling associated with exploration of the high rate and large volume datasets produced at the Large Hadron Collider at CERN and other particle research facilities.; Darren Price's Turing research explores how we can best make use of the large datasets recorded by large scientific experiments like the Large Hadron Collider in the search for the unknown (such as exploring the mystery of dark matter) through the development of new semi-supervised and unsupervised machine learning tools to maximise the value of research outputs; enhance reinterpretation; reusability and reproducibility of data; and enable the automation of aspects of scientific discovery across multiple domains.; ,Darren Price's Turing research explores how we can best make use of the large datasets recorded by large scientific experiments like the Large Hadron Collider in the search for the unknown (such as exploring the mystery of dark matter) through the development of new semi-supervised and unsupervised machine learning tools to maximise the value of research outputs; enhance reinterpretation; reusability and reproducibility of data; and enable the automation of aspects of scientific discovery across multiple domains.; ,['N/A'],Dr Darren Price 
darren-wilkinson,Data structures; Dynamical systems & differential equations; Nonlinear dynamics; Parallel computing; Probabilistic programming; Uncertainty quantification; Monte Carlo methods; Simulation; Time series; ,Darren Wilkinson is Professor of Stochastic Modelling within the School of Mathematics; Statistics and Physics at Newcastle University. He studied at Durham University; where he did a degree in Mathematics followed by a PhD in Bayesian Statistics. After a year as a research associate at Durham; he took up a Lectureship at Newcastle; being promoted to Professor in 2007. He began working on problems in statistical bioinformatics and computational systems biology shortly after moving to Newcastle. From 2008-2011 he held a BBSRC Research Development Fellowship to study stochasticity and heterogeneity in model biological systems.; He continues to be motivated by challenging inferential problems arising from life science research; and is active in the biological sciences research community; currently serving on BBSRC's Strategy Advisory Panel for Enabling New Ways of Working. In addition to being an investigator on several large RCUK grants; he currently co-directs Newcastle's EPSRC Centre for Doctoral Training in Cloud Computing for Big Data; building on his existing interests in modelling large and complex data sets in molecular biology and genomics.; His current research interests involve applications of Bayesian statistics to a variety of challenging big data problems in molecular biology and engineering. He is especially interested in parameter inference for dynamic models; on-line inference for high-velocity time series data; probabilistic programming; and the use of approximate models and emulators for rendering computationally prohibitive algorithms for expensive models more tractable.; ,His current research interests involve applications of Bayesian statistics to a variety of challenging big data problems in molecular biology and engineering. He is especially interested in parameter inference for dynamic models; on-line inference for high-velocity time series data; probabilistic programming; and the use of approximate models and emulators for rendering computationally prohibitive algorithms for expensive models more tractable.; ,['N/A'],Professor Darren Wilkinson 
dave-smith,Data structures; Dynamical systems & differential equations; Numerical analysis; Neural networks; Nonlinear dynamics; Parallel computing; Computer vision; Deep learning; Pattern recognition; Uncertainty quantification; High dimensional inference; Simulation; Time series; ,Professor Dave Smith is an applied mathematician who works across the interfaces of biomedical research and modelling. Following a PhD with Professors John Blake and Eamonn Gaffney; he undertook postdoctoral research on male fertility diagnostics in collaboration with Birmingham Women's Hospital Centre for Human Reproductive Science. He has also held an honorary positions at the University of Warwick and in the Institute for Metabolism and Systems Research; Birmingham. His main interests are very low Reynolds number biological fluid dynamics; and he has co-authored Annual Review of Fluid Mechanics articles on Mammalian Sperm Motility; and Left-Right Symmetry Breaking Flows in Embryogenesis.; In addition he has worked with teams researching topics as diverse as steroid metabolome cancer diagnosis; the molecular and physical pathogenesis of lung disease; and synthetic biology techniques for flow and pathogen detection. He and his multidisciplinary research group develop and apply tools based on regularised singularity solutions; the finite element method; and model-based image analysis.; The project 'Mechanistic learning for biomedical data science' will develop the concept of mechanistic learning; i.e. making sense of data from individual cells to people through models of the underlying biological processes. The concept; which is being developed as part of a multi-centre collaboration involving mathematicians; computer scientists; biologists and clinicians. State of the art technologies such as imaging; mass spectrometry; Raman spectroscopy; RNA sequencing and flow cytometry provide the capability to collect large volumes of data about individual cells; tissues or patients.; These technologies are being intensively pursued in the hope of developing new diagnostics; stratified medicine and novel targets for hard-to-treat and heterogeneous diseases; including male infertility; cancer; trauma and metabolic disease. A key challenge in exploiting this revolution in data availability is its high dimensionality (many variables measured simultaneously) relative to the typical number of patients and sample points in clinical research data sets. Data-driven machine learning techniques; especially deep learning; provide perhaps the most performant approaches to tasks such as patient classification based on high-dimensional data sets with relatively few individuals.; However; a limitation of these data-driven methods is their 'black box' approach prediction performance is optimised at the expense of explanatory power. The key challenge areas in which these ideas will be applied include: 1 - Male fertility diagnosis. 2 - Adrenal endocrine disorders and their management. 3 - Other areas in which integration of diverse data may help in the detection and management of disease; including lung and immunological pathologies.; ,The project 'Mechanistic learning for biomedical data science' will develop the concept of mechanistic learning; i.e. making sense of data from individual cells to people through models of the underlying biological processes. The concept; which is being developed as part of a multi-centre collaboration involving mathematicians; computer scientists; biologists and clinicians. State of the art technologies such as imaging; mass spectrometry; Raman spectroscopy; RNA sequencing and flow cytometry provide the capability to collect large volumes of data about individual cells; tissues or patients.; These technologies are being intensively pursued in the hope of developing new diagnostics; stratified medicine and novel targets for hard-to-treat and heterogeneous diseases; including male infertility; cancer; trauma and metabolic disease. A key challenge in exploiting this revolution in data availability is its high dimensionality (many variables measured simultaneously) relative to the typical number of patients and sample points in clinical research data sets. Data-driven machine learning techniques; especially deep learning; provide perhaps the most performant approaches to tasks such as patient classification based on high-dimensional data sets with relatively few individuals.; However; a limitation of these data-driven methods is their 'black box' approach prediction performance is optimised at the expense of explanatory power. The key challenge areas in which these ideas will be applied include: 1 - Male fertility diagnosis. 2 - Adrenal endocrine disorders and their management. 3 - Other areas in which integration of diverse data may help in the detection and management of disease; including lung and immunological pathologies.; ,['N/A'],Professor Dave Smith 
david-aspinall,['N/A'],David Aspinall holds a personal chair in Software Safety and Security at University of Edinburgh. He leads the Security and Privacy research group in the School of Informatics and a cross-discipline Cyber Security & Privacy Research Network in the University.  His research interests cover foundational and applied areas of computer security; as well as interactive theorem proving and proof engineering.  ; Professor Aspinall wants to pursue several ideas in The Alan Turing Institute.  First; he plans to work on applying interactive theorem proving to cryptography; by formalising game and simulation based cryptographic soundness proofs on machine.  Second; he wants to build on earlier work that combines machine learning and formal methods; which he used to construct robust malware classifiers for Android applications: he wants to extend this work to other sources of data; including data collected by the security industry; such as network traffic and log data; to develop better methods of anomaly detection and inference from event streams.  Finally; he is interested in privacy and security issues behind personal data; including data that is collected in eHealth and mHealth solutions; or IoT devices.  ; ,Professor Aspinall wants to pursue several ideas in The Alan Turing Institute.  First; he plans to work on applying interactive theorem proving to cryptography; by formalising game and simulation based cryptographic soundness proofs on machine.  Second; he wants to build on earlier work that combines machine learning and formal methods; which he used to construct robust malware classifiers for Android applications: he wants to extend this work to other sources of data; including data collected by the security industry; such as network traffic and log data; to develop better methods of anomaly detection and inference from event streams.  Finally; he is interested in privacy and security issues behind personal data; including data that is collected in eHealth and mHealth solutions; or IoT devices.  ; ,['N/A'],Professor David Aspinall 
david-atkinson,Mathematical physics; Neural networks; Computer vision; Deep learning; Reinforcement learning; Convex programming; Nonlinear programming; ,David Atkinson is a Reader in Magnetic Resonance Imaging (MRI) at University College London. He has a degree in Physics and a PhD in the field of semiconductors and optoelectronics. After a period as a scientific programmer in industry and a PostDoc at UCL; he moved to Guy's Hospital; London in 1996 to work on motion correction in MRI. In 2005 he returned to UCL as part of the newly formed Centre for Medical Image Computing within the UCL Departments of Medical Physics and Computer Science. Since 2011 he has been based in the UCL Division of Medicine within the Centre for Medical Imaging and also holds an Honorary NHS position at University College London Hospital. He facilitates clinical imaging; clinical research studies and has his own areas of active research.; Magnetic Resonance Imaging (MRI) can provide excellent visualisation of different types of soft tissue and uses no ionising radiation. Within MR scanners; pulse programming software controls the timing of RF pulses and magnetic field gradients used to generate MR signals. These signals are received over time and then reconstructed to form an image. Dr Atkinson's research has investigated new sequences and post-processing algorithms to infer clinically-relevant information about tissue properties. Research has included characterisation of tumours; placental and brain tissue; the removal of image artefacts; reduced distortions in prostate cancer imaging; and; the use of motion to characterise diseases of the gastro-intestinal tract. Motion at any time during an MR acquisition can result in blurring and ghosting across the reconstructed images. Convolutional Neural Networks can learn image features and the Turing Fellowship will be used to investigate the application of these networks for the removal of motion artefacts in MR images.; ,Magnetic Resonance Imaging (MRI) can provide excellent visualisation of different types of soft tissue and uses no ionising radiation. Within MR scanners; pulse programming software controls the timing of RF pulses and magnetic field gradients used to generate MR signals. These signals are received over time and then reconstructed to form an image. Dr Atkinson's research has investigated new sequences and post-processing algorithms to infer clinically-relevant information about tissue properties. Research has included characterisation of tumours; placental and brain tissue; the removal of image artefacts; reduced distortions in prostate cancer imaging; and; the use of motion to characterise diseases of the gastro-intestinal tract. Motion at any time during an MR acquisition can result in blurring and ghosting across the reconstructed images. Convolutional Neural Networks can learn image features and the Turing Fellowship will be used to investigate the application of these networks for the removal of motion artefacts in MR images.; ,['N/A'],Dr David Atkinson 
david-barber,['N/A'],David studied mathematics as an undergraduate (Cambridge) and the Statistical Mechanics of Neural Nets for my PhD (Edinburgh). He has worked in various institutions since then; joining the Department of Computer Science at UCL around 2006.  ; He is interested in developing methods to make progress in tackling fundamental challenges in Artificial Intelligence.  Particularly interesting areas for me are reasoning under uncertainty and how to represent knowledge. How can we best use large scale data and large scale computation to advance AI. What kinds of problems can we not currently solve and how could we try to solve them in the future?; ,He is interested in developing methods to make progress in tackling fundamental challenges in Artificial Intelligence.  Particularly interesting areas for me are reasoning under uncertainty and how to represent knowledge. How can we best use large scale data and large scale computation to advance AI. What kinds of problems can we not currently solve and how could we try to solve them in the future?; ,['N/A'],Dr David Barber 
david-beavan,Linguistics; Research methods; Social networks; Social media; Databases; Information retrieval; Visualisation (Computer systems & architectures); Natural language processing; ,David is Senior Research Software Engineer – Digital Humanities in the Research Engineering Group (affectionately known as Hut 23) in The Alan Turing Institute. He has been working in the Digital Humanities (DH) for over 15 years; working collaboratively; applying cutting edge computational methods to explore new humanities challenges. He is Co-Investigator for two Arts and Humanities Research Council (AHRC) funded projects: Living with Machines and Chronotopic Cartographies; is Co-organiser of the Humanities and Data Science Turing Interest Group; and is Research Engineering's challenge lead for Data Science for Science (and also humanities).; Prior to joining the Turing; David was Associate Director for Research at the UCL Centre for Digital Humanities (UCLDH) and Research Manager for the UCL Faculty of Arts & Humanities. He has worked on large-scale projects of international and national importance; such as the Scottish Corpus of Texts and Speech and its sibling projects; including the Corpus of Modern Scottish Writing; while at the university of Glasgow. David has served the Digital Humanities community as an elected member of the European Association for Digital Humanities (EADH) and has been a member of the Arts & Humanities Research Council (AHRC) Peer Review College.; ,['N/A'],['N/A'], David Beavan 
david-de-roure,Multi-agent systems; Artificial intelligence; Robotics; Multi-agent reasoning; Parallel computing; Human computer interface; Information retrieval; Real time computing; Applications (Machine learning); Social data science; Ethics; Research methods; Social media; Simulation; ,David De Roure is Professor of e-Research at University of Oxford; where he is jointly based in the Department of Engineering Science and the Humanities Division. He was previously Director of the Oxford e-Research Centre 2012-17.; David's research is distinctively interdisciplinary: his major recent grants are Fusing Audio and Semantic Technologies; The Theory and Practice of Social Machines; and Cyber Security of the Internet of Things. He is best known for his work on semantic web; scientific workflow systems; and digital musicology; and is a frequent speaker on the future of digital scholarship.; From 2009-13 David was the UK National Strategic Director for Digital Social Research for the Economic and Social Research Council; and subsequently a Strategic Advisor. He was closely involved in the UK e-Science programme and a founder of the UK Software Sustainability Institute.; Prior to moving to Oxford in 2010 David was Professor of Computer Science at University of Southampton; UK. He holds a PhD in Computer Science from Southampton. He is a Fellow of the British Computer Society and a Fellow of the Institute of Mathematics and its Applications. ;  ; David's interests are in Digital Humanities and collaborations with libraries; archives; and creative industries. He is part of the Data Science and Digital Humanities Interest Group.; His research activities are primarily focused on data science and music; including the use of AI in composition.; He is also interested in social data science; focusing on new and emerging forms of data and especially Internet of Things.; David has a longstanding interest in research infrastructure; including computational methods; automation; and software sustainability; and is interested in developing further work in computational archival science.; ,David's interests are in Digital Humanities and collaborations with libraries; archives; and creative industries. He is part of the Data Science and Digital Humanities Interest Group.; His research activities are primarily focused on data science and music; including the use of AI in composition.; He is also interested in social data science; focusing on new and emerging forms of data and especially Internet of Things.; David has a longstanding interest in research infrastructure; including computational methods; automation; and software sustainability; and is interested in developing further work in computational archival science.; ,['N/A'],Professor David De Roure 
david-firth,['N/A'],"Professor of Statistics at Warwick and Director of WDSI. Previously Professor of Social Statistics at Oxford. ESRC Professorial Fellow (2003-2006). Fellow of the British Academy (2008). RSS Guy Medals in Bronze (1998) and Silver (2012). John M Chambers Statistical Software Award; 2007. Editor of J Roy Stat Soc B (1998-2001). Publications include three RSS Read Papers and ten R packages.; He works on statistical theory; methods and computation; and applications in many disciplines; especially the social sciences. His research at the Turing will include:; General approaches to scalability of statistical models and associated software (with Ioannis Kosmidis); Scalable ""scoring"" in massive pair-comparison networks. Applications of this are in commerce; bibliometrics; sport; etc.; ""Sports analytics"" more generally (with Ioannis Kosmidis); ","He works on statistical theory; methods and computation; and applications in many disciplines; especially the social sciences. His research at the Turing will include:; General approaches to scalability of statistical models and associated software (with Ioannis Kosmidis); Scalable ""scoring"" in massive pair-comparison networks. Applications of this are in commerce; bibliometrics; sport; etc.; ""Sports analytics"" more generally (with Ioannis Kosmidis); ",['N/A'],Professor David Firth 
david-hogg,Neural networks; Pattern formation; Applications (Machine learning); Computer vision; Deep learning; Natural language processing; Pattern recognition; Semi-supervised learning; Unsupervised learning; Cognitive science; ,David Hogg is Professor of Artificial Intelligence at the University of Leeds. He has a BSc in Applied Mathematics from the University of Warwick; an MSc in Computer Science from the University of Western Ontario; and gained a PhD at the University of Sussex in 1984. His research is primarily on computer vision; particularly in the area of activity analysis. He works extensively across disciplinary boundaries; applying AI in engineering design; biology; and medicine.; He has been Pro-Vice-Chancellor for Research and Innovation at Leeds; visiting professor at the MIT Media Lab; chair of the EPSRC ICT Strategic Advisory Team; and chair of an international review panel on the EPSRC portfolio in Robotics and Autonomous Systems. Until 2018; he led iV&L Net; a major EU network on the integration of vision and language; and was chair of the Academic Advisory Group of the Worldwide Universities Network. He is a Turing Fellow; Fellow of the European Association for Artificial Intelligence (EurAI); a Distinguished Fellow of the British Machine Vision Association; and a Fellow of the International Association for Pattern Recognition.; David's research involves working closely with a number of domain specialists in applying machine learning and AI to a wide range of scientific and humanities problems with high potential for societal and economic impact. Existing collaborations include: (1) smart assistance in manufacturing design; simplifying and removing cost from product lifecycles; and (2) 3D motion analysis of worms; contributing to a holistic understanding of their biology; genetics; neural and physical mechanisms; with high potential impact on the design of soft robotics. There is high potential within these different problem areas for the application of recent methods using deep learning. The challenge is in selecting and applying methods in an appropriate fashion; contributing at the same time to our understanding of machine learning and AI.; ,David's research involves working closely with a number of domain specialists in applying machine learning and AI to a wide range of scientific and humanities problems with high potential for societal and economic impact. Existing collaborations include: (1) smart assistance in manufacturing design; simplifying and removing cost from product lifecycles; and (2) 3D motion analysis of worms; contributing to a holistic understanding of their biology; genetics; neural and physical mechanisms; with high potential impact on the design of soft robotics. There is high potential within these different problem areas for the application of recent methods using deep learning. The challenge is in selecting and applying methods in an appropriate fashion; contributing at the same time to our understanding of machine learning and AI.; ,['N/A'],Professor David Hogg 
david-leslie,Ethics; ,"David Leslie is the Ethics Theme Lead within the public policy programme. He was a 2017-2018 Mellon-Sawyer Fellow in Technology and the Humanities at Boston University; where he concentrated on the ethics and politics of emerging media and computationally based innovation as well as on issues of accountability; explainability; transparency; and stakeholder participation in the governance of machine learning research and innovation.; He has previously taught at Princeton’s University Center for Human Values (UCHV); where he also participated in the UCHV's 2017-2018 research collaboration with Princeton's Center for Information Technology Policy on ""Technology Ethics; Political Philosophy and Human Values: Ethical Dilemmas in AI Governance.""; Prior to teaching at Princeton; David held academic appointments at Yale’s programme in Ethics; Politics and Economics and at Harvard’s Committee on Degrees in Social Studies; where he received over a dozen teaching awards including the 2014 Stanley Hoffman Prize for Teaching Excellence. After receiving a Bachelor’s Degree from the University of Pennsylvania and an MPhil in Political Thought and Intellectual History from Cambridge University; he received his PhD from Yale.; David’s recent monograph; Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector; was published in June as part of an initiative led by the Office for Artificial Intelligence and the Government Digital Service. After receiving Ministerial approval; this work has since become the guiding ethical principles and protocols for the development and use of AI systems by all public sector agencies of the UK Government.; David’s recent and upcoming invited lectures; talks; and public appearances include:; David's current research focuses on digital ethics; algorithmic accountability; explainability; and the social and ethical impacts of machine learning and data-driven innovations. In his wider research; David studies the moral and ethical implications of emerging technologies. In particular; he is keen to question how the biospherically and geohistorically ramifying scope of contemporary scientific innovation (in areas ranging from AI and synthetic biology to nanotechnology and geoengineering) is putting pressure on the conventional action-orienting categories and norms by which humans; at present; regulate their behaviour.; ",David's current research focuses on digital ethics; algorithmic accountability; explainability; and the social and ethical impacts of machine learning and data-driven innovations. In his wider research; David studies the moral and ethical implications of emerging technologies. In particular; he is keen to question how the biospherically and geohistorically ramifying scope of contemporary scientific innovation (in areas ranging from AI and synthetic biology to nanotechnology and geoengineering) is putting pressure on the conventional action-orienting categories and norms by which humans; at present; regulate their behaviour.; ,['N/A'],Dr David Leslie 
david-llewellyn,Neural networks; Neuroscience; Deep learning; Pattern recognition; Cognitive science; Research methods; Uncertainty quantification; Causality; Monte Carlo methods; Simulation; Probability; ,Dr Llewellyn is a Senior Research Fellow in Clinical Epidemiology at the University of Exeter Medical School. He also holds an honorary contract with Devon Partnership NHS Trust. He moved to Exeter from the University of Cambridge in 2009 having received advanced training in epidemiology and data science. His his research aims to enhance the timely detection of dementia; with a focus on developing strategies for primary and secondary prevention using machine learning. He is an expert on the measurement of cognitive function; and he advises on cognitive measures as a member of the Scientific Advisory Board for the English Longitudinal Study of Ageing.; He also sits on Alzheimer's Research UK's Grant Review Board. In his research; he uses a combination of evidence synthesis; data science and machine learning to develop new translational insights that may lead to more effective interventions and an enhanced diagnostic pathway for dementia. He has developed a DEmentia identification COmputerized DEcision support system (DECODE) that is currently being trialed with patients in the NHS. Follow Dr Llewellyn on Twitter: @DrDJLlewellyn; Dr Llewellyn leads a programme of research on dementia prevention and diagnostics; and the development of new technology for personalised medicine. He will focus during his Turing Fellowship on how advanced machine learning methods can be used to develop new algorithms and clinical systems to enhance clinical decision making.; ,Dr Llewellyn leads a programme of research on dementia prevention and diagnostics; and the development of new technology for personalised medicine. He will focus during his Turing Fellowship on how advanced machine learning methods can be used to develop new algorithms and clinical systems to enhance clinical decision making.; ,['N/A'],Dr David Llewellyn 
david-lopez,Artificial intelligence; Machine learning; Applications (Machine learning); Deep learning; Natural language processing; Privacy & trust; Social data science; Data science of government & politics; Ethics; Management science; ,David Lopez is data scientist and scholar at the University of Exeter’s Business School and co-founder of the initiative in Digital Economy at Exeter (INDEX). He holds a PhD and MBA by UPM and IE Business School respectively. Prior to joining Exeter he led the data science unit at KPMG Madrid delivering solutions for large banks; fashion retailers and telecom. companies.; David has been involved in large scale data science related projects at Sky; Santander Bank; Banco Popular and Telefónica. He works with clients in the public and private sector including the Spanish Agency for organ donation and transplant; ZARA; CENTRICA and ZOETIS. He has published 1 book and numerous papers on open innovation; IPRights and business transformations induced by emerging technological paradigms.; David’s applied research is focused on the ethics of AI and its application to improve business processes and support fast strategic decision making. His current research projects involve the use of AI to improve learning outcomes in online education contexts. He also does research on the application of natural language processing techniques to better understand processes of entrepreneurship and innovation.;  ; ,David’s applied research is focused on the ethics of AI and its application to improve business processes and support fast strategic decision making. His current research projects involve the use of AI to improve learning outcomes in online education contexts. He also does research on the application of natural language processing techniques to better understand processes of entrepreneurship and innovation.;  ; ,['N/A'],Dr David Lopez 
david-plans,Multi-agent reasoning; Neuroscience; Deep learning; Social data science; Cognitive science; Social psychology; Uncertainty quantification; ,David Plans initially studied artificial intelligence and media; using evolutionary algorithms to investigate the nature of human improvisation. He helped build the first European merger for Open Source startups and worked within the UK’s National Health Service to deploy the first mobile application to let users self-report in chronic illness. He has given papers and talks at the European Conference on Artificial Life; IRCAM; the Darwin Symposium; and the Computer Arts Society in London.; His first PhD focused on genetic algorithms for classification of human musical behaviour in MPEG7 time series. He then led BioBeats as CEO; a startup that focuses on machine learning models of mental health and disorder. Through his work at BioBeats; he has retrained as a neuroscientist and experimental psychologist; and is now pursuing DPhil work at the University of Oxford's Social Cognition Lab. He is a member of the INDEX group and a Senior Lecturer in Organisational Neuroscience.; David's research interests include computational psychiatry models of stress and emotional resilience; and telemetric approaches to digital therapeutics for mental health. His work involves models through which we might understand the human body's response to stress from a psychobiology perspective; using data from body sensors and smartphone applications to build models that can predict mental disorder. He is particularly interested in models that involve interactions between one's sense of body signals (interoceptive awareness); capacity for empathy; cognitive vulnerabilities such as rumination; and how these interactions play out in individual and social frameworks such as workplaces.; ,David's research interests include computational psychiatry models of stress and emotional resilience; and telemetric approaches to digital therapeutics for mental health. His work involves models through which we might understand the human body's response to stress from a psychobiology perspective; using data from body sensors and smartphone applications to build models that can predict mental disorder. He is particularly interested in models that involve interactions between one's sense of body signals (interoceptive awareness); capacity for empathy; cognitive vulnerabilities such as rumination; and how these interactions play out in individual and social frameworks such as workplaces.; ,['N/A'],Dr David Plans MSc; PhD
david-pym,['N/A'],David Pym is Professor of Information; Logic; and Security at UCL and is The Alan Turing Institute’s University Liaison Director for UCL. He holds a PhD in logic and theoretical computer science from Edinburgh; and an MA and an ScD in mathematics from Cambridge. He is a Fellow of the IMA and the BCS. David spent many years with Hewlett-Packard’s Research Laboratories; where he developed interests in systems; security; and economics. He is Editor-in-Chief of OUP’s Journal of Cybersecurity. ; David works on a range of topics in logic; in security and privacy; and in distributed systems. At Turing; David has helped establish the Turing’s activity in ‘logical foundations of data science’; and is interested in the relationship been logical reasoning and machine learning. He is also interested in questions about access control in distributed systems; security and privacy policy; and the economics of security management. He is also interested in understanding basic questions in distributed systems architecture and behaviour; including the relationship between systems management policies and systems architecture. David addresses these issues using ideas and techniques from logic; theoretical computer science; and economics.; ,David works on a range of topics in logic; in security and privacy; and in distributed systems. At Turing; David has helped establish the Turing’s activity in ‘logical foundations of data science’; and is interested in the relationship been logical reasoning and machine learning. He is also interested in questions about access control in distributed systems; security and privacy policy; and the economics of security management. He is also interested in understanding basic questions in distributed systems architecture and behaviour; including the relationship between systems management policies and systems architecture. David addresses these issues using ideas and techniques from logic; theoretical computer science; and economics.; ,['N/A'],Professor David Pym 
david-topping,Complexity (Algorithms); Numerical (Algorithms); Dynamical systems & differential equations; Neural networks; Human computer interface; Visualisation (Computer systems & architectures); Applications (Machine learning); Deep learning; Pattern recognition; Supervised learning; Unsupervised learning; Deterministic (Mathematical modelling); Hardware optimisation (FPGA/GPU); ,David Topping is a senior lecturer in the Centre for Atmospheric Science at the University of Manchester. He completed his PhD in 2005 on ’Modelling the hygroscopic properties of atmospheric aerosol particles’ at the University of Manchester Institute of Science and Technology (UMIST) after finishing a degree in Physics at the same institute. Following this he became a fellow of the UK National Centre for Atmospheric Science (NCAS) before taking on the role of a senior research fellow part funded by the School of Earth and Environmental Science (SEES)/Centre for Atmospheric Science (CAS) at the University of Manchester.; Air pollution and climate change are two key socio-environmental drivers that represent some of the biggest multidisciplinary challenges in science; society and the economy today. The need to understand the chemical and physical processes in the atmosphere that dictate the impacts of both has created a wide range of experimental platforms over the past two decades. However; whilst these facilities persistently identify and hypothesise new processes and compounds deemed important to improve our understanding of change; the research community is now struggling to use the data and subsequent information in a truly meaningful way. David's work while a fellow of the Turing includes evaluating how machine learning might mitigate existing 'complexity' bottlenecks in atmospheric modelling; experimental data analysis and impact assessment. This includes; for example; replacing parameterised or iterative models of key atmospheric processes; traditionally solved using stiff ODE methods; using surrogate models. This also includes extracting new information from existing instrumentation through a wide range of both supervised and unsupervised methods. Alongside this; he will collaborate on methods that combine both air pollution data and human symptomatic responses.; ,Air pollution and climate change are two key socio-environmental drivers that represent some of the biggest multidisciplinary challenges in science; society and the economy today. The need to understand the chemical and physical processes in the atmosphere that dictate the impacts of both has created a wide range of experimental platforms over the past two decades. However; whilst these facilities persistently identify and hypothesise new processes and compounds deemed important to improve our understanding of change; the research community is now struggling to use the data and subsequent information in a truly meaningful way. David's work while a fellow of the Turing includes evaluating how machine learning might mitigate existing 'complexity' bottlenecks in atmospheric modelling; experimental data analysis and impact assessment. This includes; for example; replacing parameterised or iterative models of key atmospheric processes; traditionally solved using stiff ODE methods; using surrogate models. This also includes extracting new information from existing instrumentation through a wide range of both supervised and unsupervised methods. Alongside this; he will collaborate on methods that combine both air pollution data and human symptomatic responses.; ,['N/A'],Dr David Topping 
david-wall,Research methods; Social media; ,David S. Wall; PhD is Professor of Criminology in the Centre for Criminal Justice Studies; School of Law; University of Leeds; UK where he conducts interdisciplinary research into cybersecurity and cybercrimes in the cloud; ransomware; policing cybercrime; and organised cybercrime. He has published a wide range of articles and books on these subjects and has a sustained track record of interdisciplinary funded research in these areas from the EU FP6; FP7; H2020; ESRC; EPSRC; AHRC and other funders; such as the Home Office and Dstl.; David has been a member of various governmental working groups; such as the Ministerial Working Group on Horizon Planning 2020-25; the Home Office Cybercrime Working Group (2014-2016) which looked at issues of policy; costs and harms of crime and technology to society; also the HMIC Digital Crime and Policing working group in 2015; plus other non-givernmental groups such as chairing the Scientific Board of RISCS.; More recently he has worked with the UN on various initiatives related to cybercrime. He is an Academician of the Academy of Social Sciences (FAcSS) and a Fellow of the Royal Society of Arts (FRSA). He re-joined the University of Leeds in August 2015 from Durham University where he was Professor of Criminology (2010-2015) and Head of the School of Applied Social Sciences (2011-2014). Prior to moving to Durham he was Head of the School of Law (2005-2007) and Director of the Centre for Criminal Justice (2000-2005) at the University of Leeds.; David's Turing project seeks to conduct exploratory work into improving interdisciplinarity in cyber security research by exploring and mapping out a framework for developing commonly accepted conceptual metrics in cyber security. The project theme aligns with the Turing defence and security programme's applied area of research into cyber security and the project aims to improve the quality and extent of interdisciplinarity in the field of cyber security. The need for common cyber metrics guided by a commonly understood language has been highlighted as an important need by major policy making bodies ranging from the UNODC to DCMS. It also originates from a more practical need that has arisen in my own interdisciplinary research project work and also the scientific board of RISCS (Research Institute for the Science of Cyber Security).; This project will draw upon these experiences and conduct original research. The project aims are to: i) scope out the field of cyber security in terms of the different disciplinary understandings of it; ii) establish the strengths and weaknesses of different understandings of cyber security; iii) develop a 'conceptual language' or framework to enable the different disciplines to talk more meaningfully to each other; and then work together; iv) develop an agenda; and means to improve interdisciplinary communications to create added value in the formulation of ideas for proposals and improve the scientific outputs of projects; whilst increasing the overall value of the projects.; ,David's Turing project seeks to conduct exploratory work into improving interdisciplinarity in cyber security research by exploring and mapping out a framework for developing commonly accepted conceptual metrics in cyber security. The project theme aligns with the Turing defence and security programme's applied area of research into cyber security and the project aims to improve the quality and extent of interdisciplinarity in the field of cyber security. The need for common cyber metrics guided by a commonly understood language has been highlighted as an important need by major policy making bodies ranging from the UNODC to DCMS. It also originates from a more practical need that has arisen in my own interdisciplinary research project work and also the scientific board of RISCS (Research Institute for the Science of Cyber Security).; This project will draw upon these experiences and conduct original research. The project aims are to: i) scope out the field of cyber security in terms of the different disciplinary understandings of it; ii) establish the strengths and weaknesses of different understandings of cyber security; iii) develop a 'conceptual language' or framework to enable the different disciplines to talk more meaningfully to each other; and then work together; iv) develop an agenda; and means to improve interdisciplinary communications to create added value in the formulation of ideas for proposals and improve the scientific outputs of projects; whilst increasing the overall value of the projects.; ,['N/A'],Professor David Wall 
david-westhead,Numerical analysis; Neural networks; Systems theory; Databases; Information retrieval; Pattern recognition; Graph theory; Uncertainty quantification; High dimensional inference; Probability; ,David Westhead is Professor of Bioinformatics at the University of Leeds. He gained his PhD in theoretical physics from the University of Oxford in 1992; following undergraduate study in Natural Sciences (Physics and Theoretical Physics) at the University of Cambridge. Following PhD studies he worked in industry for a number of years as a mathematical modeller; statistician and computational biologist on problems as diverse as risk assessment following pesticide exposure and drug design.; He returned to academic work with a position at the European Bioinformatics Institute; working on protein structure with Professor Janet M. Thornton in 1996. From here he was recruited as Lecturer in Bioinformatics at the University of Leeds in 1998 and promoted to Senior Lecturer (2003) and Professor (2006). He has published over 100 scientific articles in peer reviewed journals and has an H index of 37 (Web of Science). He was Head of the School of Molecular and Cellular Biology in Leeds from 2011-2018.; David Westhead's research interests focus on genome scale data sets in cell biology and cancer research. In cell biology he is interested in predictive models of genetic regulation; and understanding how cell fate is controlled. Applied work concerns cancers of blood cells; leukaemias and lymphomas; where he is concerned with precision medicine. This involves using integrated genomic and gene expression information to understand underlying molecular mechanisms and suggest approaches to novel treatments. In this he collaborates extensively with clinicians and wet laboratory scientists; using large data sets from clinical trials and the general population.; He uses methods from statistics and machine learning. For example machine learning methods developed in his group have recently been used prospectively in randomisation for a major international clinical trial. More detail of this work is available at the links under his profile photo.; ,David Westhead's research interests focus on genome scale data sets in cell biology and cancer research. In cell biology he is interested in predictive models of genetic regulation; and understanding how cell fate is controlled. Applied work concerns cancers of blood cells; leukaemias and lymphomas; where he is concerned with precision medicine. This involves using integrated genomic and gene expression information to understand underlying molecular mechanisms and suggest approaches to novel treatments. In this he collaborates extensively with clinicians and wet laboratory scientists; using large data sets from clinical trials and the general population.; He uses methods from statistics and machine learning. For example machine learning methods developed in his group have recently been used prospectively in randomisation for a major international clinical trial. More detail of this work is available at the links under his profile photo.; ,['N/A'],Professor David Westhead 
david-wild,['N/A'],David Wild is a computational biologist with extensive academic and industrial experience of working at the interface of the mathematical and physical sciences and molecular biology. He received a B.A. in Physics from York University; a D.Phil. in Molecular Biophysics from Oxford University; and also holds masters degrees in Mathematics and Biostatistics. He has held staff positions at the European Molecular Biology Laboratory; the Salk Institute; and has worked in industry with Allelix Biopharmaceuticals; Oxford Molecular and GlaxoWellcome. He was one of the founding faculty of the Keck Graduate Institute of Applied Life Sciences in Claremont; California; and is currently a Professor in the Department of Statistics at the University of Warwick and is affiliated to the Warwick Systems Biology Centre. His research interests encompass bioinformatics; systems and structural biology.; David's research aims to build upon recent advances in algorithms for uncovering group structure in high-dimensional genomic data. A key component will be to consider techniques in high performance computing to ensure the methods developed can be feasibly applied to real data. The research is exploring potential projects in the area of parallel and distributed computing applications to sequential Monte Carlo. The ability to interact with Intel's dedicated architecture team; and utilise the Xeon Phi architecture is a unique opportunity; as is the Institute's access to NVidia GPU systems via the Microsoft Azure servers.; David is a Turing Fellow through his supervision of Nathan Cunningham; an enrichment student at the Turing.; ,David's research aims to build upon recent advances in algorithms for uncovering group structure in high-dimensional genomic data. A key component will be to consider techniques in high performance computing to ensure the methods developed can be feasibly applied to real data. The research is exploring potential projects in the area of parallel and distributed computing applications to sequential Monte Carlo. The ability to interact with Intel's dedicated architecture team; and utilise the Xeon Phi architecture is a unique opportunity; as is the Institute's access to NVidia GPU systems via the Microsoft Azure servers.; David is a Turing Fellow through his supervision of Nathan Cunningham; an enrichment student at the Turing.; ,['N/A'],Professor David Wild 
david-woods,Statistical methods & theory; Uncertainty quantification; Monte Carlo methods; Simulation; Modelling (Statistical methods & theory); ,David Woods is a Professor of Statistics in the Southampton Statistical Sciences Research Institute and School of Mathematical Sciences at the University of Southampton. He obtained his PhD in the statistical design of experiments from Southampton in 2003.; From 2012-2017; he held a 5-year Fellowship from the Engineering and Physical Sciences Research Council to conduct research into design of experiments for the complex nonparametric and mechanistic models required for modern scientific and industrial problems. He is an Associate Editor for Technometrics and the SIAM/ASA Journal of Uncertainty Quantification.; His research interests are in the statistical design and analysis of experiments for uncertainty quantification; particularly the development of new methods and criteria for design selection and assessment under linear and nonlinear models. A particular emphasis is on finding efficient designs when there is uncertainty in one or more aspects of the model for the response. Much of his work develops methodology that combines theory and computation to solve problems motivated by real experiments in science and industry. Application areas include engineering; chemistry and the pharmaceutical; automotive and aeronautics industries.; ,His research interests are in the statistical design and analysis of experiments for uncertainty quantification; particularly the development of new methods and criteria for design selection and assessment under linear and nonlinear models. A particular emphasis is on finding efficient designs when there is uncertainty in one or more aspects of the model for the response. Much of his work develops methodology that combines theory and computation to solve problems motivated by real experiments in science and industry. Application areas include engineering; chemistry and the pharmaceutical; automotive and aeronautics industries.; ,['N/A'],Professor David Woods 
deepak-parashar,Statistical methods & theory; High dimensional inference; Geometry & topology; Algebra; Mathematical physics; Machine learning; ,Dr Deepak Parashar is an Associate Professor in systems biology and oncology at the University of Warwick. He studied for BSc (Honours) and MSc Physics from University of Delhi; Master of Advanced Study in mathematics from Cambridge; and obtained a PhD in mathematics from Aberdeen. Previously; Deepak worked as postdoctoral research fellow in Leipzig (under fellowship of Max Planck Society); Swansea (under fellowship of the Royal Commission for the Exhibition of 1851); Rome (under a EU programme and a fellowship of the European Science Foundation); Assistant Professor in Mathematics Department at University of Warwick; visiting scientist at Max Planck Institute fur Mathematik in Bonn; and as Senior Statistician at the University of Cambridge.; Deepak has been invited by HRH Prince Philip and HRH Princess Anne to scientific receptions at Buckingham Palace; awarded Fellowship of Higher Education Authority; appointed editor and statistical reviewer for a number of journals; grant panels and book publishers.; Deepak's research interests in data science are at the interface of mathematics; statistics and cancer research. With a PhD in quantum groups and noncommutative geometry; Deepak has substantial experience of pure mathematical research at the postdoctoral level. On the statistical front; his research comprises of both methodological as well as applied themes. He has worked extensively on statistical data analyses of cancer clinical trials and is actively involved in developing novel biomarker-driven trial designs in oncology in the context of stratified medicine. Deepak is also interested in structural representations of multidimensional clinical trial data.; ,Deepak's research interests in data science are at the interface of mathematics; statistics and cancer research. With a PhD in quantum groups and noncommutative geometry; Deepak has substantial experience of pure mathematical research at the postdoctoral level. On the statistical front; his research comprises of both methodological as well as applied themes. He has worked extensively on statistical data analyses of cancer clinical trials and is actively involved in developing novel biomarker-driven trial designs in oncology in the context of stratified medicine. Deepak is also interested in structural representations of multidimensional clinical trial data.; ,['N/A'],Dr Deepak Parashar 
dino-sejdinovic,['N/A'],Dino Sejdinovic is an Associate Professor at the Department of Statistics; University of Oxford and a Fellow of Mansfield College. He previously held postdoctoral positions at the Gatsby Computational Neuroscience Unit; University College London (2011-2014) and at the Statistics Group; University of Bristol (2009-2011). He received a PhD in Electrical and Electronic Engineering (Bristol; 2009).  ; Dino is broadly interested in statistical foundations underpinning large-scale machine learning algorithms; with a particular emphasis on nonparametric and kernel methods. Recent research focused on discovery of higher-order interactions in datasets (when weak individual causes combine in a nonlinear way to form a strong effect); as well as on adaptive inference suited for complex models with nonlinear dependencies and intractable likelihoods. Further interests include tradeoffs between statistical; computational and communication properties of learning algorithms.; ,Dino is broadly interested in statistical foundations underpinning large-scale machine learning algorithms; with a particular emphasis on nonparametric and kernel methods. Recent research focused on discovery of higher-order interactions in datasets (when weak individual causes combine in a nonlinear way to form a strong effect); as well as on adaptive inference suited for complex models with nonlinear dependencies and intractable likelihoods. Further interests include tradeoffs between statistical; computational and communication properties of learning algorithms.; ,['N/A'],Professor Dino Sejdinovic 
ed-chalstrey,['N/A'],['N/A'],['N/A'],['N/A'], Ed Chalstrey 
ed-manley,Multi-agent systems; Neuroscience; Reinforcement learning; Cognitive science; Social media; Social psychology; Geometry & topology; ,Dr Ed Manley FRGS FRSA is an Associate Professor and Director of Research at the Centre for Advanced Spatial Analysis (CASA); University College London. Ed's research aims to deepen our quantitative understanding of human behaviour in cities; and how these behaviours shape urban dynamics. His research combines quantitative methods; such as exploratory data analysis; machine learning; agent-based modelling; and network modelling; with theory drawn from diverse disciplines such as urban geography; transportation; spatial cognition; judgment and decision-making; and sociology. He also has interests in the role of data visualisation in communicating complex datasets for decision-making. Ed is a Fellow of the Royal Geographical Society (RGS); Royal Society for the encouragement of Arts; Manufactures and Commerce (RSA); and Higher Education Academy (HEA). He is an Associate Editor of the Applied Spatial Analysis and Policy journal; and chairs the GIScience Research Group at the RGS. He received his Engineering Doctorate from University College London in 2013.; Ed's research at the Turing explores new ways to analyse; model and predict human movement in cities. In collaboration with psychologists; neuroscientists and geographers; his project seeks to develop detailed models of spatial cognition and decision-making in real-world spaces; combining large-scale observed movement data with advances in reinforcement learning; graphical modelling; and spatial representations. These models of behaviour will form the basis of agent-based simulations of urban dynamics; enabling the improved prediction and analysis of the impact caused by infrastructural changes and disruption to 'usual' city functions. These computational representations of human behaviour also present the opportunity for developing new pathways for human-machine interaction in relation to navigation and urban space.; ,Ed's research at the Turing explores new ways to analyse; model and predict human movement in cities. In collaboration with psychologists; neuroscientists and geographers; his project seeks to develop detailed models of spatial cognition and decision-making in real-world spaces; combining large-scale observed movement data with advances in reinforcement learning; graphical modelling; and spatial representations. These models of behaviour will form the basis of agent-based simulations of urban dynamics; enabling the improved prediction and analysis of the impact caused by infrastructural changes and disruption to 'usual' city functions. These computational representations of human behaviour also present the opportunity for developing new pathways for human-machine interaction in relation to navigation and urban space.; ,['N/A'],Dr Ed Manley 
edmund-burke,Algorithms; Operations research; Artificial intelligence; Neural & evolutionary computing; Optimisation; Management science; ,Professor Edmund Burke joined the University of Leicester as Deputy Vice-Chancellor in July 2018. Before that; he had been the Vice-Principal for Science and Engineering at Queen Mary University of London since 2015. He joined Queen Mary from the University of Stirling where he held the posts of 'Senior Deputy Principal & Deputy Vice-Chancellor' and 'Deputy Principal for Research'. Before joining Stirling in 2011; he was Dean of the Faculty of Science at the University of Nottingham. He had been a member of staff at Nottingham for 21 years. ; Prof Burke is Editor-in-chief of the Journal of Scheduling; Area Editor of the Journal of Heuristics and Associate Editor of the INFORMS Journal on Computing. Since 1995; he has led the organisation of the international series of conferences on the Practice and Theory of Automated Timetabling (PATAT). He has edited/authored 14 books and has published over 250 refereed papers. His Google Scholar publication profile can be seen at https://scholar.google.co.uk/citations?user=rUHfmpQAAAAJ; Prof Burke has been awarded 57 externally funded grants worth over £17M from a variety of sources including EPSRC; ESRC; BBSRC; EU; Research Council of Norway and industrial organisations. This funding portfolio includes a leading managerial role on several significant multi-site and multi-institutional EPSRC projects and initiatives. It also includes EPSRC Platform and Programme Grant awards.; Professor Burke carries out leading edge multi-disciplinary research into the investigation of computational search methodologies to underpin the engineering of powerful adaptive decision support systems.  His research lies at the interface of Operational Research and Artificial Intelligence and it cuts across a broad range of interdisciplinary application areas including Bioinformatics; Computational Chemistry and Management.  Moreover; it engages closely with industry and service sector organisations across a diverse range of businesses and activities.; Professor Burke’s strategic research vision aims to flexibly respond to emerging research opportunities in order to build upon his transformative research agenda and to extend his activity into a wide range of inter-disciplinary and industrially relevant application areas. Professor Burke’s long-term research mission is to address three key scientific challenges in Data Science and Operational Research. This will underpin the exploration and development of computational search methodologies and models that emerge from studying the complexity and uncertainty of real world scheduling; optimisation and decision support problems.  Key strategic goals include:; Automating the heuristic design process to build adaptive search engines: Over the last few years; Professor Burke’s research team has set the international agenda in the development of hyper-heuristics.  This work is motivated by the goal of underpinning the development of computational methodologies that can automatically build decision support systems; Closing the gap between industrial/real world practice and academic decision support research in complex and uncertain environments:  Professor Burke aims to establish new computational modelling approaches and automated search methodologies that push the boundaries of the capability of decision support systems and the levels of complexity that they are able to handle.;  ; 2012: FIMA: Fellow of the Institute of Mathematics and its Applications; 2008: FORS: Fellow of the Operational Research Society; 2005: FBCS: Fellow of the British Computer Society; 2005: CITP: Chartered IT Professional (British Computer Society); 2017: Companion of Operational Research; awarded by the Operational Research Society. This is one of the Society’s premier awards and is given “for sustained support and encouragement for the development of Operational Research or for those in influential positions who are in broad sympathy with the subject area”.; Honorary Appointments; 2018 – present Queen Mary University of London; Honorary Professor; 2015 – present University of Stirling Honorary Professor; 2013 – present: Lancaster University; Honorary Professor; ,Professor Burke carries out leading edge multi-disciplinary research into the investigation of computational search methodologies to underpin the engineering of powerful adaptive decision support systems.  His research lies at the interface of Operational Research and Artificial Intelligence and it cuts across a broad range of interdisciplinary application areas including Bioinformatics; Computational Chemistry and Management.  Moreover; it engages closely with industry and service sector organisations across a diverse range of businesses and activities.; Professor Burke’s strategic research vision aims to flexibly respond to emerging research opportunities in order to build upon his transformative research agenda and to extend his activity into a wide range of inter-disciplinary and industrially relevant application areas. Professor Burke’s long-term research mission is to address three key scientific challenges in Data Science and Operational Research. This will underpin the exploration and development of computational search methodologies and models that emerge from studying the complexity and uncertainty of real world scheduling; optimisation and decision support problems.  Key strategic goals include:; Automating the heuristic design process to build adaptive search engines: Over the last few years; Professor Burke’s research team has set the international agenda in the development of hyper-heuristics.  This work is motivated by the goal of underpinning the development of computational methodologies that can automatically build decision support systems; Closing the gap between industrial/real world practice and academic decision support research in complex and uncertain environments:  Professor Burke aims to establish new computational modelling approaches and automated search methodologies that push the boundaries of the capability of decision support systems and the levels of complexity that they are able to handle.;  ; ,2012: FIMA: Fellow of the Institute of Mathematics and its Applications; 2008: FORS: Fellow of the Operational Research Society; 2005: FBCS: Fellow of the British Computer Society; 2005: CITP: Chartered IT Professional (British Computer Society); 2017: Companion of Operational Research; awarded by the Operational Research Society. This is one of the Society’s premier awards and is given “for sustained support and encouragement for the development of Operational Research or for those in influential positions who are in broad sympathy with the subject area”.; Honorary Appointments; 2018 – present Queen Mary University of London; Honorary Professor; 2015 – present University of Stirling Honorary Professor; 2013 – present: Lancaster University; Honorary Professor; ,Professor Edmund Burke FBCS; FDRS; FIMA; CITP
eiko-yoneki,['N/A'],Eiko Yoneki is a Senior Researcher in the Systems Research Group of the University of Cambridge Computer Laboratory. She has received her PhD in Computer Science from the University of Cambridge on ‘Data Centric Asynchronous Communication’. During her postdoctoral work; she has worked on complex; time-dependent networks and multi-point communication inspired by social science and biology. Eiko has held an EPSRC Early Career Fellowship until 2016. Prior to academia; she has worked with IBM in USA; Japan; Italy and UK; where she received the highest Technical Award.; Eiko’s research spans distributed systems; networking and databases; including complex network analysis; and parallel computing for large-scale graph processing. Her current focus is on auto-tuning to deal with complex parameter space using machine-learning. She want to apply her group’s recent work; structured Bayesian Optimisation or Reinforcement Learning framework to existing problems; and build a solid auto-tuning platform in a complex parameter space. Optimisation of complex data processing is essential for data science; including the processing capability of computer systems. The multi-dimensional design space for optimising applications is huge. Today; techniques for load balancing; job scheduling and adaptive processors require run-time optimisations that depend on the dynamics of computation resources.; ,Eiko’s research spans distributed systems; networking and databases; including complex network analysis; and parallel computing for large-scale graph processing. Her current focus is on auto-tuning to deal with complex parameter space using machine-learning. She want to apply her group’s recent work; structured Bayesian Optimisation or Reinforcement Learning framework to existing problems; and build a solid auto-tuning platform in a complex parameter space. Optimisation of complex data processing is essential for data science; including the processing capability of computer systems. The multi-dimensional design space for optimising applications is huge. Today; techniques for load balancing; job scheduling and adaptive processors require run-time optimisations that depend on the dynamics of computation resources.; ,['N/A'],Dr Eiko Yoneki 
elena-simperl,['N/A'],Elena Simperl is Professor of computer science at the University of Southampton and director of the Southampton Data Science Academy. Before joining Southampton in 2012; she was assistant professor at the Karlsruhe Institute of Technology (KIT); Germany (2010-12) and vice-director of the Semantic Technologies Institute (STI) Innsbruck; Austria (2007-10). She has contributed to more than 20 research projects; often as principal investigator or project lead. Currently she is the PI of three grants: the EU-funded Data Pitch; which supports SMEs to innovate with data; the EU-funded QROWD; which uses crowd and artificial intelligence to improve smart transportation systems; and the EPSRC-funded Data Stories; which works on methods and tools to make data more engaging. She authored more than 100 papers in semantic technologies; linked data; social computing and crowdsourcing and was programme/general chair of the European and International Semantic Web Conference and of the European Data Forum.; Research interests:; - publishing and managing data; including open data sets; linked open data; and large knowledge graphs; - socio-technical methods to create and curate knowledge at scale; - social computing in its many manifestations; including crowdsourcing; citizen science and online communities and their applications to data ; - human factors in data science and human data interaction: frameworks; methods and guidelines to create accessible data representations and stories; - transparency and accountability of crowdsourced training data for AI applications; - hybrid human-machine workflows in AI; ,Research interests:; - publishing and managing data; including open data sets; linked open data; and large knowledge graphs; - socio-technical methods to create and curate knowledge at scale; - social computing in its many manifestations; including crowdsourcing; citizen science and online communities and their applications to data ; - human factors in data science and human data interaction: frameworks; methods and guidelines to create accessible data representations and stories; - transparency and accountability of crowdsourced training data for AI applications; - hybrid human-machine workflows in AI; ,['N/A'],Professor Elena Simperl 
elisabetta-versace,Neuroscience; Neural & evolutionary computing; Natural language processing; Pattern recognition; Cognitive science; Developmental psychology; Linguistics; Social psychology; ,Elisabetta is a lecturer at Queen Mary University of London; where she leads the Comparative Cognition Lab. After gaining her PhD at University of Trieste (Italy); she worked in animal behaviour and cognition; neuroscience and population genetics. Her research interests are focused on the building blocks of cognition; that she investigates using domestic chicks; tortoises and insects as models. The comparison between species and evolutionary perspective offer several advantages in understanding different strategies in early social behaviour; fast learning (e.g. filial imprinting); artificial grammar learning; lateralisation and other features that enable animals to make fast and effective decisions without intensive training.; Elisabetta is interested in using insights derived from behavioural mechanisms observed in animals to improve artificial intelligence and machine learning; and in developing robotic systems that interact with animals. Her work aims at clarifying what enhances and what hinders behavioural and cognitive performance.; Elisabetta is pursuing three main lines of research as a Turing Fellow: (a) how to use biological insights (e.g. learning mechanisms as filial imprinting and predisposed knowledge that does not depend on learning) to improve artificial intelligence; (b) developing robots capable of interacting with animals systems; (c) understand the minimal computational requirements for rule learning and artificial grammar learning.; ,Elisabetta is pursuing three main lines of research as a Turing Fellow: (a) how to use biological insights (e.g. learning mechanisms as filial imprinting and predisposed knowledge that does not depend on learning) to improve artificial intelligence; (b) developing robots capable of interacting with animals systems; (c) understand the minimal computational requirements for rule learning and artificial grammar learning.; ,['N/A'],Dr Elisabetta Versace 
emiliano-de-cristofaro,Applications (Machine learning); Deep learning; Semi-supervised learning; Supervised learning; Unsupervised learning; Cryptography (Privacy & trust); Differential privacy; Identity management; Social media; ,Emiliano De Cristofaro is an Associate Professor in Security and Privacy Enhancing Technologies at University College London (UCL). He is affiliated with the Computer Science Department and currently heads the Information Security Research Group. Before joining UCL in 2013; he was a research scientist at Xerox PARC. He received a summa-cum-laude Laurea degree in Computer Science from the University of Salerno; Italy (2005); then; in 2011; a PhD in Networked Systems from the University of California; Irvine; advised by Gene Tsudik. During his PhD; he also spent a few months on research internships at NEC in Heidelberg (2008); INRIA in Grenoble (2009); and Nokia in Lausanne (2010).; Emiliano's research interests include security; privacy; and applied cryptography. In particular; he works on understanding and countering security/privacy issues via measurement studies and data-driven analysis; as well as tackling problems at the intersection of machine learning and security/privacy.; Emiliano has chaired a few academic conferences; including the Privacy Enhancing Technologies Symposium (2013 and 2014); the Workshop on Genome Privacy and Security (2015 and 2018); and the security and privacy track at WWW (2018). He has received the distinguished paper award from the 25th Network and Distributed System Security Symposium (NDSS 2018) and the 5th Data Protection by Design Award; issued by the Catalan Data Protection authority and the Parliament of Catalonia.; ,Emiliano's research interests include security; privacy; and applied cryptography. In particular; he works on understanding and countering security/privacy issues via measurement studies and data-driven analysis; as well as tackling problems at the intersection of machine learning and security/privacy.; ,Emiliano has chaired a few academic conferences; including the Privacy Enhancing Technologies Symposium (2013 and 2014); the Workshop on Genome Privacy and Security (2015 and 2018); and the security and privacy track at WWW (2018). He has received the distinguished paper award from the 25th Network and Distributed System Security Symposium (NDSS 2018) and the 5th Data Protection by Design Award; issued by the Catalan Data Protection authority and the Parliament of Catalonia.; ,Dr Emiliano De Cristofaro 
emine-yilmaz,['N/A'],Emine is a senior lecturer (associate professor) at University College London; Department of Computer Science and a faculty fellow of the Alan Turing Institute. She also works as a research consultant for Microsoft Research; Cambridge and serve as one of the organizers of the Centre for Computational Statistics and Machine Learning (CSML) at UCL. She is the recipient of the Karen Sparck Jones Award in 2015 and is one of the recipients of the Google Faculty Research Award in 2014/2015.  ; Her research interests lie in the areas of information retrieval; web science; and applications of machine learning; probability and statistics. For more information about recent publications; please visit her publications page.; ,Her research interests lie in the areas of information retrieval; web science; and applications of machine learning; probability and statistics. For more information about recent publications; please visit her publications page.; ,['N/A'],Dr Emine Yilmaz 
emma-griffin,['N/A'],Emma Griffin is a Professor of Modern British History at the UEA. She is the author of several books; the editor of the Historical Journal; and a Fellow of the Royal Historical Society.Emma was educated at London and Cambridge and is now based at the University of East Anglia. She is the author of four books; most recently Liberty's Dawn: A People's History of the British Industrial Revolution; published by Yale UP in 2013; as well as many articles; essays and reviews in both academic and non-academic publications. Her current project is focused on women and work in Victorian Britain and explores the emotions of family life and the origins and consequences of low female pay.; Emma is a Co-I on the Living with Machines project. Her particular interest in the project is exploring how data science can help scholars to make sense of the vast amount of historical data available for Victorian Britain. The inaugural census of 1801 took a head count of Britain’s population; and subsequent censuses held at ten-yearly intervals; gathered increasingly intricate information about where individuals lived; where they had been born; the people they lived with; and the jobs they performed.; Along with the registration of births; marriages; and deaths; this provides us with billions of data points – an unparalleled trove of information about the lives of people during the world’s first industrial revolution. Most of the sources that the project will work with are well known to historians; but historians don’t by training have the skills to work with very large datasets. Much of Emma's contribution to the project involves collaborating with data-scientists to develop effective ways of probing this vast collection of records.; ,Emma is a Co-I on the Living with Machines project. Her particular interest in the project is exploring how data science can help scholars to make sense of the vast amount of historical data available for Victorian Britain. The inaugural census of 1801 took a head count of Britain’s population; and subsequent censuses held at ten-yearly intervals; gathered increasingly intricate information about where individuals lived; where they had been born; the people they lived with; and the jobs they performed.; Along with the registration of births; marriages; and deaths; this provides us with billions of data points – an unparalleled trove of information about the lives of people during the world’s first industrial revolution. Most of the sources that the project will work with are well known to historians; but historians don’t by training have the skills to work with very large datasets. Much of Emma's contribution to the project involves collaborating with data-scientists to develop effective ways of probing this vast collection of records.; ,['N/A'],Professor Emma Griffin 
emma-uprichard,['N/A'],Emma is Reader at the Centre for Interdisciplinary Methodologies (CIM); Warwick. She led Warwick’s Nuffield/ESRC/HEFCE Q-Step bid (£1.3mil) and set up the Warwick Q-Step Centre. She is recipient of an IBM Faculty Award for ‘Big Data and Real Time Analytics: Ethics and Data Linkage’ ; is a member of the National Statistician’s Data Ethics Advisory Committee (NSDEC); and co-investigator of CECAN; the 'Centre for the Evaluation of Complexity Across the Nexus’ (led by Prof. Nigel Gilbert); a £3m national research centre funded and supported by the ESRC; Defra; BEIS; NERC; EA; and FSA aimed at developing methods for complex evaluation.; Emma's work is driven by the methodological challenge of studying complex social systems across time and space and developing methods suitable for policy planning. This has involved the methodological challenge of researching cities as complex systems; food hates over the life-course; the complex interactions between knowledge; method and discipline; among other things. Her work on the fellowship focuses on the use of data science for complex government policy planning and evaluation.; ,Emma's work is driven by the methodological challenge of studying complex social systems across time and space and developing methods suitable for policy planning. This has involved the methodological challenge of researching cities as complex systems; food hates over the life-course; the complex interactions between knowledge; method and discipline; among other things. Her work on the fellowship focuses on the use of data science for complex government policy planning and evaluation.; ,['N/A'],Dr Emma Uprichard 
emmanouil-benetos,Neural networks; Information retrieval; Neural & evolutionary computing; Real time computing; Deep learning; Natural language processing; Pattern recognition; Speech recognition; Verification; Ethics; Probability; ,Dr Emmanouil Benetos is Senior Lecturer and Royal Academy of Engineering Research Fellow at the School of Electronic Engineering and Computer Science of Queen Mary University of London (QMUL). At QMUL; he is member of the Centre for Digital Music; Centre for Intelligent Sensing; Institute of Applied Data Sciences and he is co-leading the School's Machine Listening Lab. In 2013-15; he was University Research Fellow at the Department of Computer Science of City; University of London. He obtained BSc (2005) and MSc (2007) degrees in informatics from Aristotle University of Thessaloniki and received a PhD (2012) in electronic engineering from Queen Mary University of London.; Dr Benetos' research at The Alan Turing Institute focuses on audio data science; specifically towards advancing research on models; algorithms and practices for making sense of complex audio data in real-world multi-source environments. His research covers two application domains: sound monitoring in urban & domestic environments and automatic analysis of music audio recordings in noisy environments. A major part of his Turing Fellowship will focus on shaping standards and policy for privacy-preserving audio analysis methods and on identifying ethical issues and implications related to access and (re-)use of audio data.; ,Dr Benetos' research at The Alan Turing Institute focuses on audio data science; specifically towards advancing research on models; algorithms and practices for making sense of complex audio data in real-world multi-source environments. His research covers two application domains: sound monitoring in urban & domestic environments and automatic analysis of music audio recordings in noisy environments. A major part of his Turing Fellowship will focus on shaping standards and policy for privacy-preserving audio analysis methods and on identifying ethical issues and implications related to access and (re-)use of audio data.; ,['N/A'],Dr Emmanouil Benetos 
emmanouil-tranos,Research methods; Social media; Time series; Causality; Graph theory; Natural language processing; ,Emmanouil is an economic geographer focusing primarily on the spatiality of the digital economy. He is currently a Reader in Quantitative Human Geography at the University of Bristol and has published on issues related with the geography of the internet infrastructure; the economic impacts that this digital infrastructure can generate on cities and regions and the position of cities within spatial; complex networks. His research in this area led to a monograph on 'The Geography of the Internet: Cities; Regions and Internet Infrastructure'. Emmanouil has also a strong interest and expertise on the use of new sources of big data; such as data from mobile phone operators; as a means to improve our understanding of the complexities of cities and urban systems.; Emmanouil's research at the Turing will explore the potential of web data and digital archives as a means to understand cities and the spatial economy. His research aims to generate new knowledge about business activities and the digital economy as well as their evolution over space and time at a much higher level of granularity; in terms of space; time and content; than previous conventional data sources and methods were able to do. In addition; his Turing research aims to create broader transformative effects to social sciences as it will illustrate the value of web data and digital archives for social science research at scale and also provide the necessary tools to take advantage of these data; which have been underutilised.; ,Emmanouil's research at the Turing will explore the potential of web data and digital archives as a means to understand cities and the spatial economy. His research aims to generate new knowledge about business activities and the digital economy as well as their evolution over space and time at a much higher level of granularity; in terms of space; time and content; than previous conventional data sources and methods were able to do. In addition; his Turing research aims to create broader transformative effects to social sciences as it will illustrate the value of web data and digital archives for social science research at scale and also provide the necessary tools to take advantage of these data; which have been underutilised.; ,['N/A'],Dr Emmanouil Tranos 
eram-rizvi,Neural networks; Pattern formation; Operating systems; Real time computing; Computer vision; Deep learning; Pattern recognition; Hardware optimisation (FPGA/GPU); High dimensional inference; Monte Carlo methods; Time series; ,Eram is a Reader in Particle Physics at Queen Mary; University of London where he leads the ATLAS research group working on the Large Hadron Collider at CERN. After gaining his PhD Eram was awarded a Research Fellowship at the DESY particle physics laboratory in Hamburg; and then moved to the University of Birmingham as a postdoctoral researcher before taking up his current position. As the Director of Training in the DISCnet CDT; he is responsible for delivering data science training to PhD students between the universities of Queen Mary; London; Sussex; Southampton; Portsmouth and the Open University. He is currently the Deputy Dean for Research (postgraduate) at Queen Mary; and holds an Associateship with the University of Durham in particle physics phenomenology.; Analysis of data from the Large Hadron Collider will benefit greatly from applications of data science and artificial intelligence algorithms. Eram will continue his research in three main areas in collaboration with Turing. The first is the application of advanced AI techniques to observe the very rare decay of the Higgs boson to muon and anti-muon pairs which is overwhelmed by large backgrounds. This decay not yet been seen and is the only possible test of Higgs couplings to second generation fermions. Secondly the upgrade of the LHC in 2021 will require rapid online selections to be made within microseconds before recording collision data to disk. The selections will be deployed in fast electronics as part of the trigger system for the ATLAS detector. Eram is interested in the implementation of AI methods to improve data efficiency. Finally he aims to perform precision measurements of the electroweak sector of the Standard Model in which novel AI methods can be used to unfold complex detector distortions from high precision data.; ,Analysis of data from the Large Hadron Collider will benefit greatly from applications of data science and artificial intelligence algorithms. Eram will continue his research in three main areas in collaboration with Turing. The first is the application of advanced AI techniques to observe the very rare decay of the Higgs boson to muon and anti-muon pairs which is overwhelmed by large backgrounds. This decay not yet been seen and is the only possible test of Higgs couplings to second generation fermions. Secondly the upgrade of the LHC in 2021 will require rapid online selections to be made within microseconds before recording collision data to disk. The selections will be deployed in fast electronics as part of the trigger system for the ATLAS detector. Eram is interested in the implementation of AI methods to improve data efficiency. Finally he aims to perform precision measurements of the electroweak sector of the Standard Model in which novel AI methods can be used to unfold complex detector distortions from high precision data.; ,['N/A'],Dr Eram Rizvi 
eric-daub,Numerical (Algorithms); Dynamical systems & differential equations; Mathematical physics; Nonlinear dynamics; Parallel computing; Visualisation (Computer systems & architectures); Applications (Machine learning); Uncertainty quantification; Time series; ,Eric Daub received his PhD from the University of California; Santa Barbara; in computational physics; where he studied numerical models of earthquake rupture and failure of amorphous materials. He has many years of research experience as a postdoctoral researcher and professor using computer simulations to understand the complex dynamics of earthquakes and material failure.; At the Turing; Eric is interested in using large scale data and high performance computing to understand complex systems in a wide range of contexts and applications. He is also committed to ensuring that scientific researchers have access to software that can be easily used and produces results that are accurate; reproducible; and computationally efficient.; ,At the Turing; Eric is interested in using large scale data and high performance computing to understand complex systems in a wide range of contexts and applications. He is also committed to ensuring that scientific researchers have access to software that can be easily used and produces results that are accurate; reproducible; and computationally efficient.; ,['N/A'],Dr Eric Daub 
eric-meyer,['N/A'],Eric T. Meyer’s work focuses on shifts in work; knowledge creation; and interactions when digital technologies replace their previously non-digital counterparts. His research in this area has included studies of data sharing in dementia research; the use of digital images in biology; digital information practices in the sciences; social sciences; arts and humanities; and uses of web archive data. He is currently leading research on automation and computerisation in the health sector.; He has three main interests that he plans to develop further as part of the Turing. First; the ability to detect emerging research areas and disciplines using novel techniques related to mining published material; such as journals and scanned books; but also web archives such as those held at the British Library. Second; he is leading research on automation in the health sector to better understand tasks in primary care; and then build models that will help understand the potential for automation. The third area is blockchain as it relates to art; working with art organisations around protecting artists’ intellectual property rights; enhancing their financial security; and dealing with complex artists’ estates.; ,He has three main interests that he plans to develop further as part of the Turing. First; the ability to detect emerging research areas and disciplines using novel techniques related to mining published material; such as journals and scanned books; but also web archives such as those held at the British Library. Second; he is leading research on automation in the health sector to better understand tasks in primary care; and then build models that will help understand the potential for automation. The third area is blockchain as it relates to art; working with art organisations around protecting artists’ intellectual property rights; enhancing their financial security; and dealing with complex artists’ estates.; ,['N/A'],Professor Eric Meyer 
erin-young,Ethics; Data science of government & politics; Research methods; Artificial intelligence; ,Erin Lorelie Young is a postdoctoral research fellow on the Women in Data Science and AI project; within the Public Policy programme. She is currently finishing her DPhil (PhD) at the University of Oxford; researching the socio-technical practices of interdisciplinary research and development projects. Prior to this; Erin was an H-STAR Visiting Researcher at Stanford University; and a Research Assistant at the Oxford Internet Institute; working on various projects including investigating the potential of artificial intelligence (AI) for lifelong learning.; Erin has also held positions as a consultant at the International Institute for Educational Planning (IIEP) at UNESCO in Paris; and as an analyst for Kantar Consulting (WPP) in London; and for Thomson Reuters in New York City. She has a PGC in International Business Administration and Practice and Organisational Behaviour; and earned scholarships to study at the British Schools of Athens and Rome. She holds an MSc (Distinction) from the University of Oxford in Education (Learning and Technology); and an MA in Classics from the University of Cambridge.; Erin is interested in the social and ethical implications of technologies; particularly artificial intelligence (AI); machine learning and other data-driven innovations. Her work sits at the intersection of technology and society and draws from Science and Technology Studies (STS); in particular Actor-Network Theory (ANT); organisational sociology; ethnography; intersectional techno-feminism and the social shaping of technology. Her research at the Turing Institute examines the gendered practices mediating the data science and AI fields; considering the political and socio-economic roots of the networks that shape; deploy and govern AI systems and their applications. She investigates the factors which impact the position and role of women in data science and AI professions; and the systemic conditions and structural inequality of opportunity that perpetuate patterns of discrimination against women in these areas.; ,Erin is interested in the social and ethical implications of technologies; particularly artificial intelligence (AI); machine learning and other data-driven innovations. Her work sits at the intersection of technology and society and draws from Science and Technology Studies (STS); in particular Actor-Network Theory (ANT); organisational sociology; ethnography; intersectional techno-feminism and the social shaping of technology. Her research at the Turing Institute examines the gendered practices mediating the data science and AI fields; considering the political and socio-economic roots of the networks that shape; deploy and govern AI systems and their applications. She investigates the factors which impact the position and role of women in data science and AI professions; and the systemic conditions and structural inequality of opportunity that perpetuate patterns of discrimination against women in these areas.; ,['N/A'], Erin Young 
eva-morris,Data structures; Operations research; Databases; Research methods; ,Eva Morris is the Professor of Cancer Epidemiology at the University of Leeds where she leads the Cancer Epidemiology Group. Her research is largely centred on the epidemiology of colorectal cancer and she undertakes large-scale population-based studies that seek to quantify variation in the processes of management and outcome of this disease. These studies are generating the evidence needed to inform NHS cancer services. In particular; she leads the UK Colorectal Cancer Intelligence Hub. This is a £3.4m Cancer Research UK programme that involves establishing a UK wide colorectal cancer data repository (known as CORECT-R) that will contain information about the diagnosis; management and outcome of every colorectal cancer diagnosed in the country. CORECT-R will bring together all of the datasets that are relevant to colorectal cancer and securely link them to produce the high-quality cancer intelligence needed to improve outcomes.; Eva Morris's Turing research will involve exploiting the data within CORECT-R to improve colorectal cancer outcomes. The data within the resource are being amassed from across the cancer pathway ranging from genetic and molecular data to detailed clinical information and outcome data. The research potential of these data are huge. To date her research has centred on the more standard uses of the data; for example; monitoring inequalities in diagnosis; management and outcome within the NHS but the scope of the analyses that are feasible with the newly available data range significantly beyond this. The Turing Fellowship will allow her to explore these more novel research areas further. New areas for investigation include using routine data to inform both precision medicine and clinical trials.; ,Eva Morris's Turing research will involve exploiting the data within CORECT-R to improve colorectal cancer outcomes. The data within the resource are being amassed from across the cancer pathway ranging from genetic and molecular data to detailed clinical information and outcome data. The research potential of these data are huge. To date her research has centred on the more standard uses of the data; for example; monitoring inequalities in diagnosis; management and outcome within the NHS but the scope of the analyses that are feasible with the newly available data range significantly beyond this. The Turing Fellowship will allow her to explore these more novel research areas further. New areas for investigation include using routine data to inform both precision medicine and clinical trials.; ,['N/A'],Professor Eva Morris 
evangelia-kalyvianaki,Operations research; Control theory; Systems theory; Communications; Databases; Parallel computing; Operating systems; Real time computing; Reinforcement learning; Convex programming; Hardware optimisation (FPGA/GPU); Probabilistic programming; Causality; Time series; ,Eva Kalyvianaki is a Senior Lecturer (Assistant/Associate Professor) in the Department of Computer Science and Technology at the University of Cambridge and a member of the (SRG/netos group). Before; she was a Lecturer at the Department of Computer Science at City University London and a post-doctoral researcher in the Department of Computing; Imperial College London. She obtained her Ph.D. from the Computer Laboratory in Cambridge University. She holds a M.Sc.and a B.Sc. degrees from the Computer Science Department of the University of Crete; Greece.; Eva Kalyvianaki's research interests span the areas of Cloud Computing; Big Data Processing; Autonomic Computing; Distributed Systems and Systems Research in general. She is interested in the design and management of next generation; large-scale applications in the Cloud.; ,Eva Kalyvianaki's research interests span the areas of Cloud Computing; Big Data Processing; Autonomic Computing; Distributed Systems and Systems Research in general. She is interested in the design and management of next generation; large-scale applications in the Cloud.; ,['N/A'],Dr Evangelia Kalyvianaki 
evelina-gabasova,Algorithms; Artificial intelligence; Machine learning; Applications (Machine learning); Ethics; Research methods; Uncertainty quantification; ,Evelina is Principal Research Data Scientist at The Alan Turing Institute; focusing on open source activities in the research engineering group and the wider institute. She is passionate about making data science understandable and accessible to everyone.; She originally started as a programmer but got interested in machine learning early on and did a mathematics PhD at the University of Cambridge. During her PhD; she worked on Bayesian models for unsupervised learning that integrate heterogeneous biomedical datasets. After that she worked in cancer research at the MRC Cancer unit in Cambridge; where she focused on helping biologists analyse genomic data.; Outside of academia; she is also an avid speaker at developer conferences and she was awarded the Microsoft MVP award for her work in the F# community.; ,['N/A'],['N/A'],Dr Evelina Gabasova 
ewa-luger,Differential privacy; Ethics; Research methods; ,Ewa Luger is a Chancellor's Fellow in Digital Arts and Humanities; University of Edinburgh; a consulting researcher at Microsoft Research UK (AI and Ethics); and Research Excellence Framework (REF2021) co-ordinator for Design at Edinburgh College of Art. Her work explores applied ethical issues within the sphere of machine intelligence and data-driven systems. This encompasses practical considerations such as data governance; consent; privacy; explainable AI; and how intelligent networked systems might be made intelligible to the user; with a particular interest in distribution of power and spheres of digital exclusion.; Ewa holds an EPSRC-funded PhD (Computer Science); BA Hons (Politics & International Relations); and an MA (International Relations ESRC Research Track) from the University of Nottingham. Ewa previously undertook a Fellowship at Corpus Christi College (University of Cambridge) and Microsoft Research (UK) and builds upon 15 years as a practitioner in the third sector conducting research and evaluation studies related to digital/financial inclusion amongst marginalised groups.; Ethical AI by Design: Formalising a Human-Computer Interaction (HCI) Agenda Intelligent devices and services have become an embedded feature of our lives. Such systems act to distribute cognition and control between humans and computational agents and are increasingly used to support decision-making processes in sensitive/high-risk contexts. This has led to a broad consensus that algorithms should (a) be predictable to those that govern; (b) robust against manipulation; and (c) transparent to inspection. To be transparent is to make visible or expose all aspects of an entity. However; the algorithms that will underpin emerging intelligent systems rely on more complex models that endeavor to reflect the processes of the human brain.; The complexities of such models make it very difficult to predict how they will perform on some given input; even for subject matter experts. Whilst the dominant work emerging from this field is technical; an equally pressing problem is how one might consider social; conceptual and experiential understandings of algorithmic systems. How can we design systems that support human trust and understanding of AI? In light of this; the proposed research seeks to investigate how we might design intelligible; inspectable and accountable systems from the perspective of human-computer interaction.; ,Ethical AI by Design: Formalising a Human-Computer Interaction (HCI) Agenda Intelligent devices and services have become an embedded feature of our lives. Such systems act to distribute cognition and control between humans and computational agents and are increasingly used to support decision-making processes in sensitive/high-risk contexts. This has led to a broad consensus that algorithms should (a) be predictable to those that govern; (b) robust against manipulation; and (c) transparent to inspection. To be transparent is to make visible or expose all aspects of an entity. However; the algorithms that will underpin emerging intelligent systems rely on more complex models that endeavor to reflect the processes of the human brain.; The complexities of such models make it very difficult to predict how they will perform on some given input; even for subject matter experts. Whilst the dominant work emerging from this field is technical; an equally pressing problem is how one might consider social; conceptual and experiential understandings of algorithmic systems. How can we design systems that support human trust and understanding of AI? In light of this; the proposed research seeks to investigate how we might design intelligible; inspectable and accountable systems from the perspective of human-computer interaction.; ,['N/A'],Dr Ewa Luger 
ewan-klein,['N/A'],Ewan Klein has been Professor of Language Technology since 2004 in the School of Informatics; University of Edinburgh. He received an MSc from Reading University in 1973 and a PhD from Cambridge in 1978. He is a project leader of the Natural Language Toolkit (NLTK) open source library; and a co-author of Natural Language Processing with Python. He has moved across disciplinary boundaries as a founder and co-Director of Edinburgh Living Lab; which explores the role of data and technology in meeting the challenges of sustainable future cities. He is the academic coordinator of the University of Edinburgh's Internet of Things Programme.; His research interests include civic applications of IoT technology; standards; tools and governance for data interoperability and data sharing across organisations and sectors; and human-data interaction in the context of smart cities.; ,His research interests include civic applications of IoT technology; standards; tools and governance for data interoperability and data sharing across organisations and sectors; and human-data interaction in the context of smart cities.; ,['N/A'],Professor Ewan Klein 
fabrizio-smeraldi,Robotics; Game theory; Information retrieval; Computer vision; Natural language processing; Pattern recognition; Graph theory; Cognitive science; Social media; Simulation; Probability; ,Fabrizio Smeraldi received an MSc in Physics from the University of Genoa; Italy; and a PhD in Science from EPFL; Switzerland. He is currently a Lecturer in the School of Electronic Engineering and Computer Science; Queen Mary; University of London. Before joining Queen Mary he was Universitetslektor in the School of Information Science; Computer and Electrical Engineering (IDE); University of Halmstad; Sweden. Fabrizio held visiting positions at the University of Genoa; the University of Perugia and the Italian Institute of Technology in Italy; Tokyo Institute of Technology in Japan; Macau University of Science and Technology in China and Rothamsted Research; UK.; Fabrizio is statistical learning expert with a solid grounding in the natural sciences and a zest for bridging the gap between theory and application in a broad range of fields. His research interests include machine learning and pattern recognition; with applications to computer vision and bioinformatics. He is particularly interested in nonparametric and rank-based approaches. Other interests include cyber security and the physical foundations of computing. He is currently working towards the creation of a spinout company in social media analytics for digital health.; ,Fabrizio is statistical learning expert with a solid grounding in the natural sciences and a zest for bridging the gap between theory and application in a broad range of fields. His research interests include machine learning and pattern recognition; with applications to computer vision and bioinformatics. He is particularly interested in nonparametric and rank-based approaches. Other interests include cyber security and the physical foundations of computing. He is currently working towards the creation of a spinout company in social media analytics for digital health.; ,['N/A'],Dr Fabrizio Smeraldi 
faisal-mushtaq,Neuroscience; Reinforcement learning; Cognitive science; ,Dr Faisal Mushtaq is an experimental psychologist and Academic Fellow in Health Engineering at the University of Leeds. He is the lead investigator of the Immersive Cognition (ICON) Research Group and Associate Director of the Centre for Immersive Technologies at the University of Leeds.; Mushtaq's research examines interactions between action and cognition in human reinforcement learning and decision-making. His research sits at the interface between psychology; engineering and computer science and takes a multi-modal approach to understanding the relationship between action and cognition; from capturing neural activity (primarily EEG) and behavioural markers of performance using virtual and augmented reality technology in laboratory-controlled environments; through to extracting measures of performance in the real world from large-scale datasets. Mushtaq has also been driving forward the application of principles from cognitive science to real world contexts to accelerate the acquisition of skilled behaviour - from surgeons performing minimally invasive surgery through to facilitating human-like decision-making in robots.; ,Mushtaq's research examines interactions between action and cognition in human reinforcement learning and decision-making. His research sits at the interface between psychology; engineering and computer science and takes a multi-modal approach to understanding the relationship between action and cognition; from capturing neural activity (primarily EEG) and behavioural markers of performance using virtual and augmented reality technology in laboratory-controlled environments; through to extracting measures of performance in the real world from large-scale datasets. Mushtaq has also been driving forward the application of principles from cognitive science to real world contexts to accelerate the acquisition of skilled behaviour - from surgeons performing minimally invasive surgery through to facilitating human-like decision-making in robots.; ,['N/A'],Dr Faisal Mushtaq 
federico-caprotti,Multi-agent reasoning; Human computer interface; Operating systems; Real time computing; Identity management; Data science of government & politics; Ethics; Management science; Research methods; Social media; Social psychology; ,Federico Caprotti is an urban geographer interested in smart and sustainable cities. Over the past decade he has worked closely on eco-city and smart city projects in China and the EU. At Exeter; he leads the 'Smart Eco-Cities for a Green Economy (SMART-ECO)' research consortium: this involves researchers in the UK; the Netherlands; France; Germany and China. SMART-ECO is funded by the ESRC; China's NSFC; and the national research funding agencies of France; Germany; and the Netherlands. Federico also leads an ESRC Urban Transformations project on energy transitions in South African municipalities.; He has previously lectured at the universities of Leicester; Oxford; UCL; and Plymouth. Prior to joining Exeter as associate professor in human geography; Federico was senior lecturer and then reader in cities and sustainability at King's College London; where he coordinated the Masters in Sustainable Cities. Federico holds a bachelor's and doctoral degree from Oxford University; and is a Fellow of the Higher Education Academy.. In 2017; a paper he was lead author on was named as one of the 25 most significant papers published in the past 40 years in the International Journal of Urban and Regional Research.; During his tenure as a Turning Fellow; Federico will be working on smart and digital urban policy and technology-related projects. He is interested; in particular; in two key themes: firstly; the role of emergent platform urbanism in changing the ways in which cities are governed; function and are experienced. As part of this work; he has recently been engaged in carrying out work on urban social credit systems and on the export of smart city packages. Secondly; and more specifically; he will develop work that will bring together academics; policymakers and technologists in the field of smart urban health. The key underlying theme to these interests is a focus on the transnational links between the UK and other countries focused on smart urbanism; especially in East Asia.; ,During his tenure as a Turning Fellow; Federico will be working on smart and digital urban policy and technology-related projects. He is interested; in particular; in two key themes: firstly; the role of emergent platform urbanism in changing the ways in which cities are governed; function and are experienced. As part of this work; he has recently been engaged in carrying out work on urban social credit systems and on the export of smart city packages. Secondly; and more specifically; he will develop work that will bring together academics; policymakers and technologists in the field of smart urban health. The key underlying theme to these interests is a focus on the transnational links between the UK and other countries focused on smart urbanism; especially in East Asia.; ,['N/A'],Dr Federico Caprotti 
federico-nanni,Linguistics; Ethics; Data science of government & politics; Research methods; Social media; Information retrieval; Knowledge representation; Natural language processing; Deep learning; ,Federico is a Research Data Scientist at The Alan Turing Institute; working as part of the Research Engineering Group; and a visiting fellow at the School of Advanced Study; University of London.; Prior to joining the Institute; he completed a PhD in History of Technology and Digital Humanities at the University of Bologna focused on the use of web archives in historical research and has been a post-doc in Computational Social Science at the Data and Web Science Group of the University of Mannheim.; He also spent time as a visiting researcher at the Foundation Bruno Kessler and the University of New Hampshire; working on Natural Language Processing and Information Retrieval.; Federico is a historian by training and works exploring the intersections between digital humanities; computational social science; internet studies and natural language processing.; ,Federico is a historian by training and works exploring the intersections between digital humanities; computational social science; internet studies and natural language processing.; ,['N/A'],Dr Federico Nanni 
felix-cuadrado,Databases; Parallel computing; Real time computing; Graph theory; Software framework development; ,Dr Felix Cuadrado completed his PhD from Technical University of Madrid in 2010; where he studied autonomic mechanisms to manage banking services in data-centers. His PhD work received a national award by the Spanish association of Telecommunication Engineers. Currently he is senior lecturer at the School of Electronic Engineering and Computer Science of Queen Mary University of London; where he coordinates the MSc in Big Data Science.; Felix Cuadrado's research aims to enable data science algorithms to operate at a massive scale; by developing novel distributed systems techniques that help with scalability and real-time requirements. He is currently working on systems that that create and analyse large-scale temporal graphs; in order to observe the dynamics of relationships from massive amounts of data.; ,Felix Cuadrado's research aims to enable data science algorithms to operate at a massive scale; by developing novel distributed systems techniques that help with scalability and real-time requirements. He is currently working on systems that that create and analyse large-scale temporal graphs; in order to observe the dynamics of relationships from massive amounts of data.; ,['N/A'],Dr Felix Cuadrado 
filip-rindler,['N/A'],Filip Rindler is currently a Reader in Mathematics at the University of Warwick. A computer scientist turned mathematician; he contributes to the Turing's activities at the interface between theory and applications. After completing his doctorate in PDE theory at the OxPDE centre within the University of Oxford in 2011; he moved to the University of Cambridge to take up the Gonville & Caius College Drosier Research Fellowship (JRF); holding the position before joining Warwick in 2013. For 2018-2023 he is funded by an ERC Starting Grant and a DCE/LRF grant within the Turing. In 2018 he was awarded an LMS Whitehead Prize.; He is interested in variational/optimisation problems; interpreted broadly. He is particularly interested in theoretical aspects; but also in applications to material science; physics; and economics. One particular aspect of his research to date has centred around generalised solutions to minimisation problems that do not have a minimiser; but whose approximate solutions develop finer and finer oscillations or concentrations as one approaches the minimum. Describing and analysing such situations often relies on advanced tools; such as Young measures; and he has contributed several results and techniques in this area. Recently; he has also been interested in anomaly detection problems.; ,He is interested in variational/optimisation problems; interpreted broadly. He is particularly interested in theoretical aspects; but also in applications to material science; physics; and economics. One particular aspect of his research to date has centred around generalised solutions to minimisation problems that do not have a minimiser; but whose approximate solutions develop finer and finer oscillations or concentrations as one approaches the minimum. Describing and analysing such situations often relies on advanced tools; such as Young measures; and he has contributed several results and techniques in this area. Recently; he has also been interested in anomaly detection problems.; ,['N/A'],Dr Filip Rindler 
filippo-simini,Multi-agent reasoning; Neural networks; Applications (Machine learning); Stochastic (Mathematical modelling); Stochastic optimisation; Social networks; Social media; Monte Carlo methods; Simulation; Modelling (Statistical methods & theory); ,Filippo Simini is a Lecturer in the department of Engineering Mathematics at the University of Bristol. He combines methods from statistical physics; machine learning and data science to discover and characterise the distinctive statistical patterns of a system and to develop and test mathematical models to describe the system’s dynamics and emergent properties. He is particularly interested in interdisciplinary problems and applications; including collective and individual human mobility; transportation; ecological networks; computational social science and population dynamics.; Mathematical models to estimate individual and collective human mobility have important applications; including transportation; urban planning and epidemic modelling. Traffic congestion; domestic migration; and the spread of infectious diseases are processes in which the presence of mobility flows induces a net change of the spatial distribution of some quantity of interest (vehicles; population; pathogens). The ability to accurately describe the dynamics of these processes depends on our understanding of the characteristics of the underlying spatial flows. ; Filippo's research focuses on two main goals: 1) improve the predictive power of models to estimate spatial flows (origin-destination matrices) globally and 2) make mobility models more accessible to practitioners and end-users; developing online resources and services to provide a convenient and simple framework to request ad-hoc simulations of human mobility scenarios for a wide range of applications.; ,Mathematical models to estimate individual and collective human mobility have important applications; including transportation; urban planning and epidemic modelling. Traffic congestion; domestic migration; and the spread of infectious diseases are processes in which the presence of mobility flows induces a net change of the spatial distribution of some quantity of interest (vehicles; population; pathogens). The ability to accurately describe the dynamics of these processes depends on our understanding of the characteristics of the underlying spatial flows. ; Filippo's research focuses on two main goals: 1) improve the predictive power of models to estimate spatial flows (origin-destination matrices) globally and 2) make mobility models more accessible to practitioners and end-users; developing online resources and services to provide a convenient and simple framework to request ad-hoc simulations of human mobility scenarios for a wide range of applications.; ,['N/A'],Dr Filippo Simini 
florian-ostmann,Data science of government & politics; ,Florian Ostmann is the Policy Theme Lead within the public policy programme. His research interests are centred around applications of data science and AI in the public sector; ethical and regulatory questions in relation to emerging technologies across all sectors of the economy; and the future of work and social welfare systems.; Prior to joining the Turing; Florian was a Research Associate at Harvard Kennedy School's Shorenstein Center on Media; Politics and Public Policy where he conducted research on questions of fairness and transparency in the context of algorithmic decision-making. In relation to these topics; his work has been focused on translating between conceptual frameworks from statistics; computer science; philosophy; and the law; and on thinking about the demands of transparency from an application-specific perspective.; Florian is a member of the Law Committee for the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. His previous experience includes working for the Pan America Health Organization and working as a consultant on responsible investing (with a focus on modern slavery risks); autonomous vehicle policy; and social impact measurement. Florian holds a Master in Public Policy from the Harvard Kennedy School and a PhD in Political Philosophy and MA in Legal and Political Theory from University College London. Going back to earlier roles and academic work; he maintains active interests in corporate responsibility and business ethics (with a focus on business & human rights and the ethics of market transactions); bioethics (with a focus on priority-setting and research ethics); and health policy.; ,['N/A'],['N/A'],Dr Florian Ostmann 
frances-griffiths,Data science of government & politics; Ethics; Management science; Research methods; Social media; ,Frances Griffiths trained in medicine at the University of Cambridge and King's College Hospital; London; and went on to become a general practitioner in Stockton-on-Tees. While working as a GP she undertook her PhD at the University of Durham; Department of Sociology and Social Policy. Frances joined the University of Warwick in 1998 where she developed her research interest in the impact of technology on the experience of health and healthcare. In 2003 she was awarded a Department of Health National Career Scientist Award to develop a programme of research on complexity and health. This led to collaborations with academics from mathematics; statistics; economics and organisational sciences. Frances provides social science input to clinical research on complex health issues such as back pain; headache and depression. She continues to lead research on use of technology in healthcare with particular focus on digital communication technology.; The scene is almost set for the widespread use of artificial intelligence in healthcare including diagnostics; prognostics; health management and information sharing. Social and ethical implications require more debate to ensure the public; patients and healthcare providers are prepared to accompany the innovators into this scene. Frances Griffiths is undertaking case studies within healthcare to inform this debate.; ,The scene is almost set for the widespread use of artificial intelligence in healthcare including diagnostics; prognostics; health management and information sharing. Social and ethical implications require more debate to ensure the public; patients and healthcare providers are prepared to accompany the innovators into this scene. Frances Griffiths is undertaking case studies within healthcare to inform this debate.; ,['N/A'], Frances Griffiths 
francesco-shankar,['N/A'],Dr Shankar is Director of Year 2 and Admission Coordinator for the Astronomy group for both postgraduate and undegraduate students at the University of Southampton.; He is Associate Professor in the Astronomy group. After obtaining his PhD at SISSA; he moved for a postdoc to the Ohio State University; to the Max Planck Institute for Astrophysics as an Alexander von Hulmboldt Fellow; and then to the Observatoire de Paris as a Marie Curie Fellow. He then joined the Department of Physics and Astronomy in 2013. Dr Shankar is now PI of a H2020 Marie Skłodowska-Curie Innovative Training Network which will kick off in 2020.; ,['N/A'],['N/A'],Dr Francesco Shankar 
francois-caron,['N/A'],Since September 2015; Professor Francois Caron has been an Associate Professor in the department of Statistics and a tutorial Fellow at Keble College; University of Oxford. From September 2013 to August 2015; he was a Marie Curie Research Fellow in the department of Statistics at the University of Oxford. From 2008 to 2013; he was a Research Scientist (Chargé de Recherche) at INRIA Bordeaux. He completed his Ph.D. in Information Engineering at the University of Lille I in 2006; and spent two years as a post-doctoral researcher in the departments of Computer Science and Statistics at the University of British Columbia (2006-2008).  ; Prof. François Caron’s research interests lie in the development of statistical models and computational procedures for the analysis of structured data; with particular interest in Bayesian nonparametrics and Monte Carlo methods. His recent research focuses on building probabilistic models of network data which can capture the salient properties of real-world networks (sparsity; modularity); as well as efficient algorithms for learning the hidden structure of those networks.; ,Prof. François Caron’s research interests lie in the development of statistical models and computational procedures for the analysis of structured data; with particular interest in Bayesian nonparametrics and Monte Carlo methods. His recent research focuses on building probabilistic models of network data which can capture the salient properties of real-world networks (sparsity; modularity); as well as efficient algorithms for learning the hidden structure of those networks.; ,['N/A'],Professor Francois Caron 
frank-wood,['N/A'],Professor Wood is an associate professor at the Department of Engineering Science; University of Oxford. Before that Dr Wood was an assistant professor of Statistics at Columbia University and a research scientist at the Columbia Center for Computational Learning Systems. He formerly was a postdoctoral fellow of the Gatsby Computational Neuroscience Unit of the University College London under Dr Yee Whye Teh. He received his PhD from Brown University in computer science under the supervision of Dr Michael Black and Dr Tom Griffiths. Prof. Wood is a product of the Illinois Mathematics and Science Academy (1992).; He began college at the University of Illinois at Chicago (UIC) but transferred and received a B.S. in Computer Science from Cornell University (1996). Prior to his academic career he was a successful entrepreneur having run and sold the content-based image retrieval company ToFish! to Time Warner and serving as CEO of Interfolio. He started his career working at both the Cornell Theory Center and subsequently the Lawrence Berkeley National Laboratory.; ,['N/A'],['N/A'],Professor Frank Wood 
franz-kiraly,['N/A'],As a practical statistician and machine learner; Franz is interested in creating a data analytics workflow which is empirically solid; quantitative; and useful in the real world; with a focus on predictive modelling.; He is working on what he considers to be two of the most pressing challenges in a practical and data-centric context: namely; how to deal with structured data; such as learning with data samples of series; sequences; matrices; or graphs; and how to quantitatively assess and compare methods against each other; for example whether complicated algorithm X is really better than a random guess.; These are especially relevant in applications where usually the data and the associated scientific questions; and not a single method class is in the focus of interest; current project and collaboration domains include the medical sciences; sports and prevention; geoscience; physics and finance.; Recently; Franz has been doing research on these applications:; Prediction and Prevention of Falls in a Neurological In-Patient Population. Falling; and associated injuries such as hip fracture; are a major strain on health and health resources; especially in the elderly or hospitalized. We are able to predict; with high accuracy in a neurological population; whether a patient is likely to fall during their stay; using only a number connecting test (the Trail making test). (Read more); Quantification and Prediction in Running Sports. Characterizing the training state of running athletes; and making predictions for race planning and training. We can predict Marathon times with an error in the order of a few minutes; and we are able to accurately summarize an athlete by three characteristic numbers. (Read more); His current work on data analysis methodology includes:; Non-linear prediction and dimension reduction with series-valued samples. We propose a new learning framework for the situation where the data samples are time series or otherwise sequentially ordered; based on kernels whose features are ordered variants of sample moments. (Read more) ; Single-Entry Matrix Completion and Local Matrix Completion. Our new methods can (i) reliably impute or predict single missing entries in a numerical data table; with error bars; and (ii) do so without necessarily reading in all entries in a big data table. They are the first of their kind under the common low-rank assumption. (Read more); Kernel Learning with Invariances. Encoding known invariances of the data; say sign/mirror symmetry; scaling or phase invariance; efficiently with a kernel (work in progress). (Read more); ,Recently; Franz has been doing research on these applications:; Prediction and Prevention of Falls in a Neurological In-Patient Population. Falling; and associated injuries such as hip fracture; are a major strain on health and health resources; especially in the elderly or hospitalized. We are able to predict; with high accuracy in a neurological population; whether a patient is likely to fall during their stay; using only a number connecting test (the Trail making test). (Read more); Quantification and Prediction in Running Sports. Characterizing the training state of running athletes; and making predictions for race planning and training. We can predict Marathon times with an error in the order of a few minutes; and we are able to accurately summarize an athlete by three characteristic numbers. (Read more); His current work on data analysis methodology includes:; Non-linear prediction and dimension reduction with series-valued samples. We propose a new learning framework for the situation where the data samples are time series or otherwise sequentially ordered; based on kernels whose features are ordered variants of sample moments. (Read more) ; Single-Entry Matrix Completion and Local Matrix Completion. Our new methods can (i) reliably impute or predict single missing entries in a numerical data table; with error bars; and (ii) do so without necessarily reading in all entries in a big data table. They are the first of their kind under the common low-rank assumption. (Read more); Kernel Learning with Invariances. Encoding known invariances of the data; say sign/mirror symmetry; scaling or phase invariance; efficiently with a kernel (work in progress). (Read more); ,['N/A'],Dr Franz Kiraly 
ganna-pogrebna,['N/A'],Ganna is a decision theorist and a behavioral scientist. Before joining The Alan Turing Institute; she worked at the University of Innsbruck (Austria); the University of Bonn (Germany); Humboldt- Universität zu Berlin (Germany); University of Sheffield (UK); and Columbia University in New York (USA). She is currently a Professor of Behavioral Science at the University of Birmingham and a Research Fellow at Warwick Manufacturing Group (WMG) at the University of Warwick.;  ; Blending behavioral science; computer science; data analytics; engineering; and business model innovation; Ganna helps cities; businesses; charities; and individuals to better understand why they make decisions they make and how they can optimize their behavior to achieve higher profit; better social outcomes; as well as flourish and bolster their well-being. Her recent projects focus on smart technological and social systems; cybersecurity; human-computer and human-data interactions and business models.; ,Blending behavioral science; computer science; data analytics; engineering; and business model innovation; Ganna helps cities; businesses; charities; and individuals to better understand why they make decisions they make and how they can optimize their behavior to achieve higher profit; better social outcomes; as well as flourish and bolster their well-being. Her recent projects focus on smart technological and social systems; cybersecurity; human-computer and human-data interactions and business models.; ,['N/A'],Professor Ganna Pogrebna 
gareth-roberts,Stochastic (Mathematical modelling); Statistical methods & theory; Uncertainty quantification; Monte Carlo methods; Simulation; ,['N/A'],Gareth's research interests include:; ,['N/A'],Professor Gareth Roberts 
gareth-tyson,Communications; Parallel computing; Human computer interface; Differential privacy; Cognitive science; Data science of government & politics; Social media; Social psychology; ,Dr Gareth Tyson is a lecturer and Internet Data Scientist at Queen Mary University of London. Prior to this he worked at King's College London and Lancaster University; as well as holding visiting positions at University College London and Cambridge Computer Lab. His work takes a data-driven approach to understanding and solving emerging challenges in Internet systems. He tends to sit at the intersection between traditional systems design (i.e. understanding technology) and social computing (i.e. understanding humans). By collecting; compiling and combining empirical insights on these two things; he strives to improve the online security and performance for both humans and technology alike.; His work has received coverage from news outlets such as MIT Tech Review; Washington Post; Slashdot; BBC; The Times; Daily Mail; Wired; Science Daily; Ars Technica; The Independent; Business Insider and The Register. Dr Tyson serves as a reviewer and program committee member for a number of prominent conferences/journals; and has received several awards including the Outstanding Reviewer Award twice at ICWSM; the Honourable Mention Award at WWW'18; a Teaching Excellence Award at QMUL; and the Brendan Murphy Young Research Award.; The Internet is an increasingly prominent element of our daily lives. Rather than being a simple communications vehicle; it has become a complex ecosystem driven by a mix of social; economic and technological aspects. Dr Tyson's research focuses on understanding this mix to better streamline and secure how the Internet operates. His work as a Turing Fellow particularly focuses on the Decentralised Web. This theme is driven by a growing concern that has gained attention in recent years: the increasing centralisation of power around a small number of online 'hypergiants' (e.g. Amazon; Facebook).; This centralisation refers not only to the vast quantity of infrastructure they own; but also to the amount of user data they hold. In response to this trend; a number of decentralised alternatives have emerged. These are web platforms that offer similar services (e.g. microblogging; social networking) but without centralising infrastructure and data under the ownership of a single organisation. This new Decentralised Web offers countless opportunities; but also comes with significant security; privacy and performance challenges. Dr Tyson's work at the Turing focuses on understanding how these emerging platforms are used; and devising decentralised techniques by which their security and performance can compete with (and even outstrip) that of the existing hypergiants.; ,The Internet is an increasingly prominent element of our daily lives. Rather than being a simple communications vehicle; it has become a complex ecosystem driven by a mix of social; economic and technological aspects. Dr Tyson's research focuses on understanding this mix to better streamline and secure how the Internet operates. His work as a Turing Fellow particularly focuses on the Decentralised Web. This theme is driven by a growing concern that has gained attention in recent years: the increasing centralisation of power around a small number of online 'hypergiants' (e.g. Amazon; Facebook).; This centralisation refers not only to the vast quantity of infrastructure they own; but also to the amount of user data they hold. In response to this trend; a number of decentralised alternatives have emerged. These are web platforms that offer similar services (e.g. microblogging; social networking) but without centralising infrastructure and data under the ownership of a single organisation. This new Decentralised Web offers countless opportunities; but also comes with significant security; privacy and performance challenges. Dr Tyson's work at the Turing focuses on understanding how these emerging platforms are used; and devising decentralised techniques by which their security and performance can compete with (and even outstrip) that of the existing hypergiants.; ,['N/A'],Dr Gareth Tyson 
gavin-shaddick,Uncertainty quantification; Modelling (Statistical methods & theory); ,['N/A'],Professor Shaddick's research interests include the theory and application of Bayesian hierarchical models and spatio-temporal modelling in a number of fields including epidemiology; environmental modelling; and disease progression in rheumatology. A major focus is modelling global air quality by integrating information from multiple sources; including ground monitoring; remote sensing satellites and chemical transport models.; He is also actively engaged in research with the power industry; using big data to model demand profiles; forecasting demands and identifying customer profiles. Of particular interest are computational techniques that allow the implementation of complex statistical models to real-life applications where the scope over both space and time may be very large.; ,['N/A'],Professor Gavin Shaddick 
genevieve-liveley,Robotics; Evolution & adaptation; Human computer interface; Research methods; ,Genevieve Liveley is Reader in Classics at the University of Bristol; where her research and teaching centres upon narratologically inflected studies of the ancient world. Her most recent book; Narratology (Oxford University Press) exposes the dynamic (mis)appropriation of ancient scripts that gives modern narratology its shape. Her new research; on the ancient and future (hi)stories of AI and robots; builds on this work; and seeks a better understanding of the frames; schemata; and scripts that programme cultural narratives about human interaction with artificial humans; automata; and AI. She has published a significant body of original work: three single-authored monographs; a co-edited collection of essays; more than 20 single-authored academic papers in journals and collections; including (among others) articles on the classical tradition; chaos theory; and cyborgs.; The exciting vision of this research is to help inform and transform the design and deployment of human-facing AI by bringing the humanities into conversations about human/machine interactions and relationships. By analysing 3000 years of enduring and changing preferences and antipathies in robot and AI stories; it aims to produce new knowledge about the narrative scripts and frames that are deployed when humans and autonomous/intelligent machines interact. In so doing; it will help to inform and shape the design of AI services that are tailored to people's individual needs and situations and inform the technological narrative of the future. This new research will also help to provide proof of concept for the feasibility (and utility) of large-scale diachronic digital data analysis of story-form and build the first open access; searchable/expandable database of AI representation in the Western canon. In so doing; the project will highlight what we can learn about the future of AI in society from the history of AI in stories – from ancient myths to modern movies and media representations. Preliminary research indicates that public attitudes to AI in society are coded by their experience of AI in fiction. So; by allowing a better understanding of the narrative dynamics shaping such coding – that is; the narrative scripts and frames that programme human responses to AI – could help create a step-change in the design and deployment of AI in a range of personalised social contexts.; ,The exciting vision of this research is to help inform and transform the design and deployment of human-facing AI by bringing the humanities into conversations about human/machine interactions and relationships. By analysing 3000 years of enduring and changing preferences and antipathies in robot and AI stories; it aims to produce new knowledge about the narrative scripts and frames that are deployed when humans and autonomous/intelligent machines interact. In so doing; it will help to inform and shape the design of AI services that are tailored to people's individual needs and situations and inform the technological narrative of the future. This new research will also help to provide proof of concept for the feasibility (and utility) of large-scale diachronic digital data analysis of story-form and build the first open access; searchable/expandable database of AI representation in the Western canon. In so doing; the project will highlight what we can learn about the future of AI in society from the history of AI in stories – from ancient myths to modern movies and media representations. Preliminary research indicates that public attitudes to AI in society are coded by their experience of AI in fiction. So; by allowing a better understanding of the narrative dynamics shaping such coding – that is; the narrative scripts and frames that programme human responses to AI – could help create a step-change in the design and deployment of AI in a range of personalised social contexts.; ,['N/A'],Dr Genevieve Liveley 
george-bassel,Complexity (Algorithms); Computing networks; Pattern formation; Systems theory; Graph theory; ,Professor Bassel received a B.Sc. (2001) and Ph.D. (2006) at The University of Guelph; Canada; and was group leader at The University of Birmingham from 2012 to 2019; where he held a Chair in Plant Computational Biology. His research lies at the interface between biology; physics and computer science; and seeks to understand how plants process information from the environment to control their development.; Research being undertaken in conjunction with the Turing is focused on information processing in plants.; By viewing multicellular plant tissues as distributed information processing systems; the mechanisms these collections of cells are using to compute are being investigated. Both experimental and theoretical approaches are being taken; including mapping the multicellular circuitry of plant organs and developing models of collective decision-making which run within these templates.; This research seeks to establish a framework to rationally reprogram crop plants; and increase their performance in response to complex environments. Achieving this is of increasing significance in light of the growing demand for food production and challenges posed by climate change.; ,Research being undertaken in conjunction with the Turing is focused on information processing in plants.; By viewing multicellular plant tissues as distributed information processing systems; the mechanisms these collections of cells are using to compute are being investigated. Both experimental and theoretical approaches are being taken; including mapping the multicellular circuitry of plant organs and developing models of collective decision-making which run within these templates.; This research seeks to establish a framework to rationally reprogram crop plants; and increase their performance in response to complex environments. Achieving this is of increasing significance in light of the growing demand for food production and challenges posed by climate change.; ,['N/A'],Professor George Bassel 
george-danezis,['N/A'],George Danezis is a Professor of Security and Privacy Engineering at the Department of Computer Science of University College London; and Head of the Information Security Research Group. He has been working on anonymous communications; privacy enhancing technologies (PET); and traffic analysis since 2000. He has previously been a researcher for Microsoft Research; Cambridge; a visiting fellow at K.U.Leuven (Belgium); and a research associate at the University of Cambridge (UK); where he also completed his doctoral dissertation under the supervision of Prof. R.J. Anderson.; His theoretical contributions to the Privacy Technologies field include the established information theoretic and other probabilistic metrics for anonymity and pioneering the study of statistical attacks against anonymity systems. On the practical side he is one of the lead designers of the anonymous mail system Mixminion; as well as Minx; Sphinx; Drac and Hornet; he has worked on the traffic analysis of deployed protocols such as Tor.; His current research interests focus around secure communications; high-integirty systems to support privacy; smart grid privacy; peer-to-peer and social network security; as well as the application of machine learning techniques to security problems. He has published over 70 peer-reviewed scientific papers on these topics in international conferences and journals. He was the co-program chair of ACM Computer and Communications Security Conference in 2011 and 2012; IFCA Financial Cryptography and Data Security in 2011; the Privacy Enhancing Technologies Workshop in 2005 and 2006. He sits on the PET Symposium board and ACM CCS Steering committee and he regularly serves in program committees of leading conferences in the field of privacy and security. He is a fellow of the British Computing Society since 2014.; George's research at the Turing revolves around two key themes; or privacy and distributed ledgers. First; he research how the degree of privacy protection may be quantified and experimentally calculated for different proposed Privacy Enhancing Technologies. His approach is influenced by Differential Privacy definitions; however it is adapted to the settings and driven by experimental; rather than purely analytical evaluations. Second; he researches distributed ledgers; which are transparent and accountable distributed computational platforms. Those form the core of `blockchain’ technologies; and are challenging to scale while retaining beneficial security and governance properties; ,George's research at the Turing revolves around two key themes; or privacy and distributed ledgers. First; he research how the degree of privacy protection may be quantified and experimentally calculated for different proposed Privacy Enhancing Technologies. His approach is influenced by Differential Privacy definitions; however it is adapted to the settings and driven by experimental; rather than purely analytical evaluations. Second; he researches distributed ledgers; which are transparent and accountable distributed computational platforms. Those form the core of `blockchain’ technologies; and are challenging to scale while retaining beneficial security and governance properties; ,['N/A'],Professor George Danezis 
george-deligiannidis,Monte Carlo methods; Probability; ,Professor Deligiannidis studied Mathematics (MMath) at the University of Warwick and Financial Mathematics (MSc) at Heriot-Watt University and the University of Edinburgh. After obtaining his PhD from the School of Mathematical Sciences of the University of Nottingham; he moved to the Department of Mathematics of the University of Leicester as a Teaching Assistant/Fellow. In 2012 he moved to the Department of Statistics of the University of Oxford as Departmental Lecturer. He stayed in Oxford until September 2016 when he moved to the Department of Mathematics of King’s College London as Lecturer in Statistics. He moved back to the University of Oxford in December 2017 as Associate Professor of Statistics.; Professor Deligiannidis is interested in understanding the asymptotic behaviour of random processes; for example Markov chains; used in computational statistics; and random walks.; ,Professor Deligiannidis is interested in understanding the asymptotic behaviour of random processes; for example Markov chains; used in computational statistics; and random walks.; ,['N/A'],Professor George Deligiannidis 
george-konstantinidis,Algorithms; Artificial intelligence; Databases; Parallel computing; Privacy & trust; Logic (Theoretical mathematics); ,Dr. George Konstantinidis is a Turing Fellow and an Assistant Professor (Lecturer) of Artificial Intelligence at the Web and Internet Science Research Group; School of Electronics and Computer Science; University of Southampton.; Before joining Southampton; George was a Senior Researcher at the Information Systems group; Department of Computer Science; University of Oxford. He received his PhD from the Department of Computer Science; Viterbi School of Engineering; University of Southern California; and was also a member of the Information Integration Group; Intelligent Systems Division-Information Sciences Institute (ISI) at Marina Del Rey; CA. ; Dr. Konstantinidis studies the integration and exchange of data and knowledge within organisations and on the web. That is; ways to bring together; understand and process vast amounts of heterogeneous and distributed data; via building unifying access and data warehouses. He also works on developing frameworks for supporting data usage contracts; these are technical specifications that describe intentions and agreements of data usage; privacy policies and user consent. He also studies the sharing of private data across Databases; Knowledge Graphs and Blockchains.; Dr. Konstantinidis is developing algorithms and systems to address these questions. Tools that help in this direction are database theory and technologies; the semantic web; ontologies; and distributed systems.; ,Dr. Konstantinidis studies the integration and exchange of data and knowledge within organisations and on the web. That is; ways to bring together; understand and process vast amounts of heterogeneous and distributed data; via building unifying access and data warehouses. He also works on developing frameworks for supporting data usage contracts; these are technical specifications that describe intentions and agreements of data usage; privacy policies and user consent. He also studies the sharing of private data across Databases; Knowledge Graphs and Blockchains.; Dr. Konstantinidis is developing algorithms and systems to address these questions. Tools that help in this direction are database theory and technologies; the semantic web; ontologies; and distributed systems.; ,['N/A'],Dr George Konstantinidis 
georgios-aivaliotis,Operations research; Parallel computing; Visualisation (Computer systems & architectures); Applications (Machine learning); Pattern recognition; Reinforcement learning; Supervised learning; Unsupervised learning; Dynamic/static (Mathematical modelling); Stochastic (Mathematical modelling); Data science of government & politics; Ethics; Management science; Non-parametric & semi-parametric methods; Simulation; Time series; Estimation theory; Modelling (Statistical methods & theory); Probability; ,Georgios (George) Aivaliotis is a lecturer in financial mathematics at the University of Leeds. He obtained his PhD in statistics from the University of Leeds. Before that he graduated with an MSc in financial mathematics from Herriot-Watt University and a BSc in statistics from Athens University of Economics and Business.; George's research interests evolve around probability; statistics; stochastic processes and machine learning. Applications of his research can be found in the broad area of financial and actuarial mathematics; data analytics; survival analysis as well as financial technologies.He has worked in the area of stochastic control for mean-variance type problems and applications in portfolio selection and agent remuneration. Currently; he is applying ideas from probability and statistics into the field of data analytics; in particular he is working on robust temporal pattern mining from data that are time stamped as part of the EPSRC funded project QuantiCode.; ,George's research interests evolve around probability; statistics; stochastic processes and machine learning. Applications of his research can be found in the broad area of financial and actuarial mathematics; data analytics; survival analysis as well as financial technologies.He has worked in the area of stochastic control for mean-variance type problems and applications in portfolio selection and agent remuneration. Currently; he is applying ideas from probability and statistics into the field of data analytics; in particular he is working on robust temporal pattern mining from data that are time stamped as part of the EPSRC funded project QuantiCode.; ,['N/A'],Dr Georgios Aivaliotis 
georgios-gkoutos,Neural networks; Neuroscience; Databases; Information retrieval; Computing networks; Applications (Machine learning); Deep learning; Natural language processing; ,A biochemist by training; George was initially involved in the field of Computational Biology following a MSc degree by research on correlated mutations analysis on G-Protein coupled receptors which involved modelling class A G-Protein Coupled Receptors (GPCRs) and drug design. He then proceeded to obtain a PhD in the areas of Chemoinformatics/Bioinformatics at Imperial College of London.After the completion of his PhD; he was awarded a MRC Career Development Fellowship at MRC Harwell; Oxford. In 2005; he joined the Department of Genetics at Cambridge University and was part of various international consortia aimed at facilitating the translation of basic research findings to applications that aimed at the identification of the genetic underpinnings of disease mechanisms. In 2012; he became the head of the Bioinformatics and Computational Biology Group at Aberystwyth University; an interdisciplinary group of bioinformatics researchers; working at the interface between computing; biology and medical applications and crossing the Department of Computer Science and the Institute of Biological; Environmental and Rural Sciences (IBERS). George maintains an Aberystwyth University honorary Professorship in Bioinformatics.In September 2015; George joined the University of Birmingham as the Chair of Clinical Bioinformatics; a joint appointment between the UoB Medical School and the University Hospitals Birmingham NHS Foundation Trust. Professor Gkoutos splits his time between the Institute of Translational Medicine; the Centre of Computational Biology; the College of Medical and Dental Sciences and the Queen Elizabeth Hospital.; Professor Gkoutos main interests lie in the fields of clinical and biomedical informatics; computational biology; and translational research aiming at the discovery of molecular origins of human disease and the development of novel disease diagnostic and intervention strategies. His expertise lies in the fields of integrative systems biology; biomedical knowledge formalisation; standardisation and representation; multimodal large data harmonisation; interoperability; integration and analysis; comparative phenomics; chemical and clinical informatics. The primary areas of applications include development of data standards; analytical methods and tools; as well as the semantic representation; integration and analysis of biomedical and health data; enabling their translation both within and across species; domains and levels of granularity with application on the investigation of the pathophysiology and pathobiology of human disease; and pharmacogenomics; ,Professor Gkoutos main interests lie in the fields of clinical and biomedical informatics; computational biology; and translational research aiming at the discovery of molecular origins of human disease and the development of novel disease diagnostic and intervention strategies. His expertise lies in the fields of integrative systems biology; biomedical knowledge formalisation; standardisation and representation; multimodal large data harmonisation; interoperability; integration and analysis; comparative phenomics; chemical and clinical informatics. The primary areas of applications include development of data standards; analytical methods and tools; as well as the semantic representation; integration and analysis of biomedical and health data; enabling their translation both within and across species; domains and levels of granularity with application on the investigation of the pathophysiology and pathobiology of human disease; and pharmacogenomics; ,['N/A'],Professor Georgios Gkoutos 
gesine-reinert,['N/A'],Prof. Gesine Reinert is a University Lecturer at the Department of Statistics; Oxford; and Fellow at Keble College; Oxford (2000 – present).  ; Her current and main research interests are in network statistics and to investigate such networks in a statistically rigorous fashion. Often this will require some approximation; and approximations in statistics are another of her research interests. Stein’s method is an excellent method to derive distances between the distributions of random quantities; and is one that Prof Reinert has required some expertise in over the years. The general area of Prof Reinert’s research falls under the category of Applied Probability while many of the problems and examples she studies are from the area of Computational Biology (or bioinformatics).; ,Her current and main research interests are in network statistics and to investigate such networks in a statistically rigorous fashion. Often this will require some approximation; and approximations in statistics are another of her research interests. Stein’s method is an excellent method to derive distances between the distributions of random quantities; and is one that Prof Reinert has required some expertise in over the years. The general area of Prof Reinert’s research falls under the category of Applied Probability while many of the problems and examples she studies are from the area of Computational Biology (or bioinformatics).; ,['N/A'],Professor Gesine Reinert 
gian-marco-campagnolo,['N/A'],Gian Marco Campagnolo is Lecturer in Science; Technology & Innovation Studies at the University of Edinburgh. His research highlights aspects of business knowledge as apparent in client-consultant relationships as well as vendor-user interaction or in special conditions such as IT symposia and software demonstrations. When business turned heavily to analytics; he became interested in data science applications with a view on how 'big data' is made visible and turned into action.; In his latest projects; Gian's research addressed how empirical sociology can contribute to data-intensive research in understanding the evolution of careers in the IT sector. Taken up by the media and presented at The Alan Turing Institute inaugural workshop on Social Data Science; this research led to knowledge exchange with professional networking company LinkedIn.; ,In his latest projects; Gian's research addressed how empirical sociology can contribute to data-intensive research in understanding the evolution of careers in the IT sector. Taken up by the media and presented at The Alan Turing Institute inaugural workshop on Social Data Science; this research led to knowledge exchange with professional networking company LinkedIn.; ,['N/A'],Dr Gian Marco Campagnolo 
gianni-antichi,Data structures; Operations research; Robotics; Neural networks; Systems theory; Communications; Information retrieval; Reinforcement learning; Hardware optimisation (FPGA/GPU); Time series; ,Dr Gianni Antichi is a lecturer (assistant professor) at the School of Electronic Engineering and Computer Science of Queen Mary; University of London. Dr Antichi holds an MEng and a PhD in Information Engineering from the University of Pisa defended in 2007 and 2011; respectively. After being appointed as Research Associate for two years at University of Pisa; he moved to the University of Cambridge; first as Research Associate in 2013; and then promoted to Senior Research Associate in 2015.; Dr Antichi's research interests cover a broad spectrum of topics in both networks and systems. Specifically; they span network monitoring; networking systems performance characterisation and Software Defined Networking. Lately; he has been also focusing on understanding how better dataplane programmability can impact the design of networks as well as their services. One of his works got awarded with the best paper at ACM SIGCOMM 2017; considered the most prestigious conference in data communications and networking. He also acted as joint investigator on multiple EU H2020 projects (ENDEAVOUR and SSICLOPS) and recently I am co-investigator on the UK EPSRC fundend EARL project. The project seeks to pioneer SDN enabled measurement-based network management to enhance the Internet infrastructure.; The LIME (Learning-based reactive Internet Engineering) project seeks to help the monitoring of large networks by introducing AI/learning mechanisms and design novel ways to execute them at scale. Specifically; during the project; the team will design novel reactive network engineering techniques; by relying on distributed Internet monitoring. The key idea is to couple network monitoring and management; by introducing a feedback loop; so as to automatically adapt network management practices; towards better usage of network resources and more secure network infrastructures.; ,The LIME (Learning-based reactive Internet Engineering) project seeks to help the monitoring of large networks by introducing AI/learning mechanisms and design novel ways to execute them at scale. Specifically; during the project; the team will design novel reactive network engineering techniques; by relying on distributed Internet monitoring. The key idea is to couple network monitoring and management; by introducing a feedback loop; so as to automatically adapt network management practices; towards better usage of network resources and more secure network infrastructures.; ,['N/A'],Dr Gianni Antichi 
ginestra-bianconi,Data structures; Mathematical physics; Multi-agent reasoning; Control theory; Neural networks; Neuroscience; Communications; Neural & evolutionary computing; Deep learning; Reinforcement learning; Graph theory; Social media; Uncertainty quantification; Causality; Simulation; Probability; Combinatorics; Geometry & topology; ,Ginestra Bianconi is Reader of Applied Mathematics at the School of Mathematical Sciences of Queen Mary University of London. Her research activity on network science includes network theory and data science interdisciplinary applications. She has formulated the Bianconi-Barabasi model that displays the Bose-Einstein condensation in complex networks. She has worked in network entropy and network ensembles and on dynamical processes on networks. In the last years she has been focusing on multilayer networks; network geometry and topology; percolation and network control. Ginestra Bianconi has published more than 130 papers and her work has appeared in major scientific journals such as Science; Nature; PNAS; PRX and Physical Review Letters. She is the author of the book 'Multilayer Networks: Structure and Function' by Oxford University Press.; Ginestra Bianconi works on Applied Mathematics and Network Science. Her research covers different aspects of the field including the statistical modeling of networks and the formulation of advanced information theory methods to extract relevant information from network data. Her major contributions include works that mathematically model complex systems and  predict their functional behaviour. Recently her research focus is on on generalised network structures including multilayer networks and  simplicial complexes. ; Ginestra's research is having impact in different disciplines ranging from neuroscience to communication networks. Her main achievements and the impact on different disciplines are:;  ; ,Ginestra Bianconi works on Applied Mathematics and Network Science. Her research covers different aspects of the field including the statistical modeling of networks and the formulation of advanced information theory methods to extract relevant information from network data. Her major contributions include works that mathematically model complex systems and  predict their functional behaviour. Recently her research focus is on on generalised network structures including multilayer networks and  simplicial complexes. ; Ginestra's research is having impact in different disciplines ranging from neuroscience to communication networks. Her main achievements and the impact on different disciplines are:;  ; ,['N/A'],Dr Ginestra Bianconi 
giovanni-montana,Machine learning; Deep learning; Reinforcement learning; Modelling (Statistical methods & theory); ,Giovanni Montana received a Ph.D. in Computational Statistics from the Department of Statistics of Warwick University in 2003 and then held a post-doctoral position at the University of Chicago. In 2005; after a short stint in industry working as a research statistician for Bristol-Myers Squibb; in Princeton; he returned to the UK to join the faculty of Imperial College London as GlaxoSmithKline Lecturer of Statistics in the Mathematics Department. There; he led a statistical learning group with a focus on high-dimensional data analysis and applications in bioinformatics and medical imaging.; In 2013; shortly after being promoted to Reader; Giovanni joined the Department of Biomedical Engineering at King's College London as Chair in Biostatistics & Bioinformatics and continued to develop statistical and machine/deep learning algorithms for a number of large-scale applied problems in genomics; computer vision; and text analytics. He is a Fellow of the Royal Statistical Society; Chair of the Statistical Computing Section of the Royal Statistical Society and a Chartered Statistician.; Giovanni returned to Warwick and joined WMG in November 2017 as Professor of Data Science where he leads the Data Science Group and co-leads the Centre for Applied Artificial Intelligence. He is a Fellow of The Alan Turing Institute and holds a Visiting Professor position in Biomedical Engineering at King's College London.; Statistical modelling; data science; machine and deep learning; reinforcement learning ; ,Statistical modelling; data science; machine and deep learning; reinforcement learning ; ,['N/A'],Professor Giovanni Montana 
glynn-winskel,Data structures; Mathematical physics; Multi-agent systems; Multi-agent reasoning; Game theory; Parallel computing; Reinforcement learning; Verification; Probabilistic programming; Uncertainty quantification; Causality; Probability; Algebra; Calculus & analysis; Geometry & topology; ,Glynn Winskel rejoined the University of Cambridge Computer Laboratory as professor in 2000. This followed 12 years as professor of computer science at Aarhus University. There he was one of a small number of researchers in Denmark to be awarded funding to head a research centre in Basic Research in Computer Science. He originally read mathematics at the University of Cambridge and mathematical logic at the University of Oxford before turning to computer science for his PhD at the University of Edinburgh (completed 1980). This was followed by a period as a Royal Society postdoctoral fellow; when he was invited by Dana Scott to join his new group at Carnegie Mellon University.; In 1984 he left Pittsburgh to take up a lectureship at the University of Cambridge; becoming reader in 1987; leaving for a professorship in Aarhus in 1988. His book `The Formal Semantics of programming languages' (MIT Press) is used internationally and available in Italian; Chinese and Japanese. He sees his research as developing the mathematics with which to understand and analyze computation; its nature; power and limitations. He is probably best known for his work generalising the methodology of domain theory and denotational semantics to concurrent computation; and as the main developer of event structures. He was awarded an Advanced Grant by the European Research Council 'Events; Causality and Symmetry---the next generation semantics' in 2011. He a member of the Academia Europaea.; Computation today is highly distributed and interactive; often probabilistic; and sometimes based in quantum theory or biology. Traditional models fall short: they are either too low level (as with 'Turing machines') or have abstracted too early from operational and quantitative concerns (the case with domain theory; the classical foundation of denotational semantics).; A major objective pursued by Glynn Winskel has been to provide a comprehensive mathematical understanding of interaction; seeing it as the way to reconnect theories of computation with their operational roots. His distributed games arguably provide the most versatile foundation for denotational semantics we have. As a fellow at The Alan Turing Institute he will explore with others how to push semantic and logical techniques into new areas such as machine learning.; ,Computation today is highly distributed and interactive; often probabilistic; and sometimes based in quantum theory or biology. Traditional models fall short: they are either too low level (as with 'Turing machines') or have abstracted too early from operational and quantitative concerns (the case with domain theory; the classical foundation of denotational semantics).; A major objective pursued by Glynn Winskel has been to provide a comprehensive mathematical understanding of interaction; seeing it as the way to reconnect theories of computation with their operational roots. His distributed games arguably provide the most versatile foundation for denotational semantics we have. As a fellow at The Alan Turing Institute he will explore with others how to push semantic and logical techniques into new areas such as machine learning.; ,['N/A'],Professor Glynn Winskel 
goran-nenadic,Information retrieval; Natural language processing; Social media; ,Goran is a Professor of Computer Science at the University of Manchester. He received a BSc and MPhil in Computer Science and Mathematics from the University of Belgrade in 1993 and 1997 respectively; and a PhD in Computer Science from the University of Salford in 2003. Prior to his appointment to a lectureship in Manchester; he was a researcher at universities in Belgrade; Salford and UMIST. Goran leads the UK healthcare text analytics network (Healtex) and is the founding Editor-in-Chief of Journal of Biomedical Semantics.; Goran's research focuses on making sense of large-scale free-text data by combing rule-based and data-intensive approaches. His work mainly aims at engineering deep features to train machine learning algorithms to process free-text documents in a variety of domains; including healthcare; biotechnology; legal text; veterinary sciences; social media and scientific literature. He works with a number of hospitals; charities and industry on unlocking evidence contained in clinical narrative and healthcare social media; and has taken part in major international clinical text mining challenges. His current foci are on temporal clinical information extraction and anonymisation of clinical free-text data.; Goran has also worked on extraction and curation of biomedical information from the literature in the variety of domains; including information presented in tabular (semi-structured) forms. Finally; he is also interested in integrative data analytics that combines multi-modal data streams to uncover new patterns (specifically combining complementary structured and unstructured data).; ,Goran's research focuses on making sense of large-scale free-text data by combing rule-based and data-intensive approaches. His work mainly aims at engineering deep features to train machine learning algorithms to process free-text documents in a variety of domains; including healthcare; biotechnology; legal text; veterinary sciences; social media and scientific literature. He works with a number of hospitals; charities and industry on unlocking evidence contained in clinical narrative and healthcare social media; and has taken part in major international clinical text mining challenges. His current foci are on temporal clinical information extraction and anonymisation of clinical free-text data.; Goran has also worked on extraction and curation of biomedical information from the literature in the variety of domains; including information presented in tabular (semi-structured) forms. Finally; he is also interested in integrative data analytics that combines multi-modal data streams to uncover new patterns (specifically combining complementary structured and unstructured data).; ,['N/A'],Professor Goran Nenadic 
graham-cormode,['N/A'],Graham Cormode is a Professor in Computer Science at the University of Warwick and the Turing University Lead for Warwick. He works on research topics in data management; privacy and big data analysis. Previously; he was a principal member of technical staff at AT&T Labs-Research in the USA.  He serves as an associate editor for ACM Transactions on Database Systems (TODS); and the Journal of Discrete Algorithms.;  ; Graham will work on a variety of topics at the Turing including:; ,Graham will work on a variety of topics at the Turing including:; ,['N/A'],Professor Graham Cormode 
greg-mcinerny,Communications; Human computer interface; Visualisation (Computer systems & architectures); Visualisation (Programming languages); Data science of government & politics; Ethics; Research methods; Monte Carlo methods; Modelling (Statistical methods & theory); ,Following a PhD in Ecology and Evolution; Greg developed his interests in science; software and visualisation at Microsoft Research and then the Department of Computer Science at the University of Oxford. His current position at the Centre for Interdisciplinary Methodologies (CIM); at the University of Warwick; has allowed him to expand his capacity as an interdisciplinary researcher; bringing new social and cultural influences into his work. At CIM Greg works alongside colleagues trained in social science; computer science; architecture; media theory; ethnography; psychology; geography and a range of interdisciplinary approaches to research.; Greg has a focus on information visualisation - in terms of design; methods and theory - and works on advancing the understanding of visualisation as a communication medium and advancing visualisation techniques for analytical tools and communication devices. He has an ongoing interest in what a ‘full stack’ understanding of visualisation would be; e.g. a holistic understanding of visualisation as a joint function of data; design; perception; cognition; cultures; and socio-technical systems.; ,Greg has a focus on information visualisation - in terms of design; methods and theory - and works on advancing the understanding of visualisation as a communication medium and advancing visualisation techniques for analytical tools and communication devices. He has an ongoing interest in what a ‘full stack’ understanding of visualisation would be; e.g. a holistic understanding of visualisation as a joint function of data; design; perception; cognition; cultures; and socio-technical systems.; ,['N/A'],Dr Greg McInerny 
guangtao-fu,Operations research; Robotics; Control theory; Neural networks; Systems theory; Neural & evolutionary computing; Deep learning; Reinforcement learning; Stochastic optimisation; Uncertainty quantification; Monte Carlo methods; Probability; ,Guangtao Fu is Associate Professor of water infrastructure systems at the Centre for Water Systems; University of Exeter and a Royal Society Industry Fellow working with Northumbrian Water Limited. He has a Bachelor's degree and a Masters in hydraulic engineering from Shandong University; and a PhD in water resources engineering from Dalian University of Technology; China. Before becoming a member of academic staff; he worked as a researcher at University of Bristol; Imperial College London; and University of Exeter.; He focuses on developing and applying new computer models; data analytics and artificial intelligence tools to tackle urban water infrastructure challenges. His work has advanced our understanding of key processes in urban water systems through data analytics and artificial intelligence and developed new algorithms and tools to improve the long-term resilience and sustainability of water infrastructure. Current research areas include smart water and wastewater infrastructure; integrated modelling; real time control of urban water systems; flood risk and resilience; and green infrastructure.; ,He focuses on developing and applying new computer models; data analytics and artificial intelligence tools to tackle urban water infrastructure challenges. His work has advanced our understanding of key processes in urban water systems through data analytics and artificial intelligence and developed new algorithms and tools to improve the long-term resilience and sustainability of water infrastructure. Current research areas include smart water and wastewater infrastructure; integrated modelling; real time control of urban water systems; flood risk and resilience; and green infrastructure.; ,['N/A'],Professor Guangtao Fu 
guy-nason,Data structures; Numerical analysis; Pattern recognition; Unsupervised learning; Visualisation (Programming languages); Data science of government & politics; Social networks; Non-parametric & semi-parametric methods; Time series; Calculus & analysis; ,Guy Nason graduated with a first-class BSc in Statistics from the University of Bath in 1988 and also obtained an Undergraduate Certificate in Education; which confers Qualified Teacher Status. Afterwards; he obtained a Diploma in Mathematical Statistics (with Distinction) from the University of Cambridge. He returned to Bath and obtained his PhD in 1992; supervised by Robin Sibson. In 1993 he undertook a two-year postdoctoral position; with Bernard Silverman and spent one more year in Bath; before moving to Bristol in 1993. In 1994 he took up a Lectureship in Statistics in Bristol’s School of Mathematics; becoming Reader in 2000 and Professor in 2002. He became Head of the Statistics Group at Bristol in 2006 and then Head of the School of Mathematics for the period 2008-12. In 2016 he became Director of the Institute for Statistical Science at Bristol until 2018.; He has spent sabbatical periods at the University of Chicago; Stanford University and the Australian National University. He has served as Associate Editor for Computational Statistics; Journal of the Royal Statistical Society; Series B; Statistica Sinica; Biometrika and Applied and Computational Harmonic Analysis and also served as Secretary of the Royal Statistical Society’s Research Section. He was Programme Chair of the Royal Statistical Society’s 175th Anniversary Conference in Edinburgh in 2009. Recently; he has been a Vice President (Academic Affairs) of the Royal Statistical Society; a member of its Executive Committee and an Elected Member of its Council.; Nason’s proposed research programme focuses on economic statistic time series. In Stream 1: he will investigate new ways of producing more timely economic statistics by using new data sources. For example; the ambitious goal of producing weekly or daily inflation estimates from web-scraped data sources The future challenges he wishes to address and solve are (i) how to fuse series from multiple origins at multiple rates - a very general problem; (ii) creation of associated transaction volume or volume-proxy models; (iii) how to incorporate key exogenous variables and administrative data; (iv) integration of advanced inference engines; but tensioning against computational cost; especially for generating daily estimates with large amounts of price data.Stream 2: will create new network-informed time series models; producing more realistic models; where estimation can actually be efficiently executed in high-dimensional situations. Such new network autoregressive integrated moving average (NARIMA) process models exploit the joint information available in a multivariate time series and its associated network. These novel models have shown comparative success in; e.g.; forecasting simple disease processes across the UK.; ,Nason’s proposed research programme focuses on economic statistic time series. In Stream 1: he will investigate new ways of producing more timely economic statistics by using new data sources. For example; the ambitious goal of producing weekly or daily inflation estimates from web-scraped data sources The future challenges he wishes to address and solve are (i) how to fuse series from multiple origins at multiple rates - a very general problem; (ii) creation of associated transaction volume or volume-proxy models; (iii) how to incorporate key exogenous variables and administrative data; (iv) integration of advanced inference engines; but tensioning against computational cost; especially for generating daily estimates with large amounts of price data.Stream 2: will create new network-informed time series models; producing more realistic models; where estimation can actually be efficiently executed in high-dimensional situations. Such new network autoregressive integrated moving average (NARIMA) process models exploit the joint information available in a multivariate time series and its associated network. These novel models have shown comparative success in; e.g.; forecasting simple disease processes across the UK.; ,,Professor Guy Nason 
haeran-cho,High dimensional inference; Non-parametric & semi-parametric methods; Time series; ,Haeran Cho has been a Lecturer in Statistical Science in the School of Mathematics at the University of Bristol since 2013. She received a BSc degree in Statistics from Seoul National University; a PhD degree in Statistics from London School of Economics in 2011; and was a post doctoral research officer in the same department from 2011 to 2013. She currently serves on the Editorial Board for the Journal of the Korean Statistical Society and the Journal of Time Series Analysis.; Haeran Cho's research interests include sparse modelling of non-stationary data. In particular; her research focuses on developing computationally efficient algorithms for multiple change-points at which some stochastic properties of the data undergo changes; and understanding their theoretical properties. In addition; she also works on modelling of high-dimensional data with serial dependence via factor analysis; and energy data modelling.; ,Haeran Cho's research interests include sparse modelling of non-stationary data. In particular; her research focuses on developing computationally efficient algorithms for multiple change-points at which some stochastic properties of the data undergo changes; and understanding their theoretical properties. In addition; she also works on modelling of high-dimensional data with serial dependence via factor analysis; and energy data modelling.; ,['N/A'],Dr Haeran Cho 
hakan-ferhatosmanoglu,Databases; Information retrieval; Deep learning; Social media; ,Hakan Ferhatosmanoglu is a Professor and Deputy Head of Computer Science at the University of Warwick. He received his PhD from University of California; Santa Barbara in 2001. He worked as a tenured faculty member at the Ohio State U. (OSU); Bilkent U.; visited Max Planck Institute (MPI) for Informatics and MPI for Software Systems. His research has been supported by government agencies including the US Department of Energy and US National Science Foundation; and companies including Pfizer and IBM. He has published in leading venues of data systems and analytics research and served in the technical committees of VLDB; KDD; SIGMOD; ICDE; ICDM; ECML/PKDD; CIKM; SIGIR. His past advisees moved to work in faculty positions at major research universities; and companies including Google; Amazon; Intel; Microsoft; Teradata; Oracle.; He received several research career awards from the US Department of Energy; US National Science Foundation; Turkish Academy of Sciences; and Alexander von Humboldt Foundation in Germany.; His research interest is in building software systems for scalable data analytics. His work addresses data and systems challenges in scaling machine learning; integrating multiple sources of information; and protecting the privacy of data subjects. An example application is learning aggregate mobility patterns over multiple sources of spatio-temporal data. He has closely worked with industrial partners in developing commercial data analytics and machine learning solutions.; ,His research interest is in building software systems for scalable data analytics. His work addresses data and systems challenges in scaling machine learning; integrating multiple sources of information; and protecting the privacy of data subjects. An example application is learning aggregate mobility patterns over multiple sources of spatio-temporal data. He has closely worked with industrial partners in developing commercial data analytics and machine learning solutions.; ,['N/A'],Professor Hakan Ferhatosmanoglu 
hao-ni,['N/A'],Dr Hao Ni is a senior lecturer in financial mathematics at UCL since September 2016. Prior to this she was a visiting postdoctoral researcher at ICERM and Department of Applied Mathematics at Brown University from 2012/09 to 2013/05 and continued her postdoctoral research at the Oxford-Man Institute of Quantitative Finance until 2016. She finished her D.Phil. in mathematics in 2012 under the supervision of Professor Terry Lyons at University of Oxford.; Her research interests include stochastic analysis; financial mathematics and machine learning. More specifically she is interested in non-parametric modelling effects of data streams through rough paths theory and statistical models. Rough paths theory is a non-linear extension of classical theory of control differential equations to model highly oscillatory systems; and the core concept in rough paths theory is the signature of a path; which can be used as useful features for learning to summarize sequential data in terms of its effect. Moreover; she is also interested in its applications; e.g. online Chinese handwritten character and financial data streams.; ,Her research interests include stochastic analysis; financial mathematics and machine learning. More specifically she is interested in non-parametric modelling effects of data streams through rough paths theory and statistical models. Rough paths theory is a non-linear extension of classical theory of control differential equations to model highly oscillatory systems; and the core concept in rough paths theory is the signature of a path; which can be used as useful features for learning to summarize sequential data in terms of its effect. Moreover; she is also interested in its applications; e.g. online Chinese handwritten character and financial data streams.; ,['N/A'],Dr Hao Ni 
harry-van-der-weijde,['N/A'],Dr Harry van der Weijde is an applied economist and operations researcher. He is currently Chancellor’s Fellow (tenure-track lecturer) in the School of Engineering of the University of Edinburgh; having previously held research positions at University of Cambridge and the Vrije Universiteit Amsterdam. He holds a PhD in Spatial Economics from the Vrije Universiteit Amsterdam and an MSc in Economics from the University of Edinburgh. In his research; he tries to answer economic questions about engineering systems (primarily energy and transportation) using mathematical tools such as optimisation and complementarity modelling.; At the Turing; Dr van der Weijde will look at data science applications to energy systems. In particular; he hopes to collaborate with colleagues in statistics; mathematics and informatics to develop methods that can quantify the monetary value of new data sources in energy applications. The energy industry is very excited about new data sources; such as smart meter data; but somewhat surprisingly; nobody seems to know what their value is to planners; system operators and utilities; and which parts of the enormous new datasets (which cannot always be used in their entirety) are particularly useful.; ,At the Turing; Dr van der Weijde will look at data science applications to energy systems. In particular; he hopes to collaborate with colleagues in statistics; mathematics and informatics to develop methods that can quantify the monetary value of new data sources in energy applications. The energy industry is very excited about new data sources; such as smart meter data; but somewhat surprisingly; nobody seems to know what their value is to planners; system operators and utilities; and which parts of the enormous new datasets (which cannot always be used in their entirety) are particularly useful.; ,['N/A'],Dr Harry van der Weijde 
hatice-gunes,Robotics; Neural networks; Databases; Human computer interface; Applications (Machine learning); Computer vision; Deep learning; Pattern recognition; Reinforcement learning; Supervised learning; Unsupervised learning; Cognitive science; Research methods; Social psychology; Time series; Modelling (Statistical methods & theory); ,Hatice Gunes is an Associate Professor (University Senior Lecturer) in the Department of Computer Science and Technology; University of Cambridge. Her research expertise is in the areas of affective computing and social signal processing that lie at the crossroad of multimodal interaction; computer vision; signal processing; and machine learning fields applied to computer/robot mediated human-human interactions and human-robot interactions.; Her research work develops novel computational frameworks for analysing and understanding human behaviour; social signals and affect from facial expressions; vocal nuances; body posture/ gesture; and physiological signals; and for modelling these phenomena for creating socio-emotionally intelligent games; assistive technologies; virtual agents and robotic systems.; She has published over 100 papers in these areas. Her current research vision is to embrace the challenges present in the area of health and empower the lives of people through technology by continuing her research with applications to social robotics and wellbeing.; She has recently been awarded the prestigious EPSRC Early Career Fellowship (2019-2024) to investigate adaptive robotic emotional intelligence for wellbeing. Gunes is the President of the Association for the Advancement of Affective Computing (AAAC); the General Co-Chair of ACII 2019; and the Program Co-Chair of ACM/IEEE HRI 2020 and IEEE FG 2017. She is the Chair of the Steering Board of IEEE Transactions on Affective Computing; and has served as an Associate Editor of IEEE Transactions on Affective Computing; IEEE Transactions on Multimedia; and Image and Vision Computing Journal.; Dr Gunes aims to undertake research to answer the following questions: (i) how we can devise novel approaches to advance the interaction and adaptation capabilities of autonomous robots (e.g.; humanoid robots; autonomous surface vehicles etc.); and (ii) how to utilise the adaptive intelligent robotic framework(s) for fostering human wellbeing. This will entail the design and evaluation of machine learning (ML) and artificial intelligence (AI) architectures on mobile robotic frameworks for continuous perception (sensing); inference (reasoning) and adaptation (action).; ,Dr Gunes aims to undertake research to answer the following questions: (i) how we can devise novel approaches to advance the interaction and adaptation capabilities of autonomous robots (e.g.; humanoid robots; autonomous surface vehicles etc.); and (ii) how to utilise the adaptive intelligent robotic framework(s) for fostering human wellbeing. This will entail the design and evaluation of machine learning (ML) and artificial intelligence (AI) architectures on mobile robotic frameworks for continuous perception (sensing); inference (reasoning) and adaptation (action).; ,['N/A'],Dr Hatice Gunes 
he-sun,Graph theory; Algebra; Combinatorics; Geometry & topology; ,He Sun is a senior lecturer in Algorithms and Complexity in the School of Informatics; University of Edinburgh. He obtained his PhD (2009) at Fudan University; under the supervision of Hong Zhu. He held positions at the Max Planck Institute for Informatics from 2010 to 2015; and the University of Bristol from 2015 to 2017. His research interests range over the fields of algorithms; machine learning and data mining; applied probability and statistics; and matrix analysis. A continuing theme of his research is applying these techniques to designing efficient spectral algorithms for massive graphs and datasets; as well as studying their applications in different settings.; He Sun's main research focus at the Turing is to advance the understanding of fundamental spectral techniques; to improve the state-of-the-art of fundamental graph algorithms in more practical settings; and to implement and evaluate these algorithms on real-word datasets. This research plan consists of foundational; algorithmic and applied aspects as summarised below: (1) Advance the foundations of spectral graph theory through the connections between graphs and manifolds; and develop the spectral theory for directed graphs (2) Algorithmic studies for matrix sparsification and graph clustering; with applications in dynamic and streaming settings (3) Develop algorithmic libraries for fundamental spectral algorithms; ,He Sun's main research focus at the Turing is to advance the understanding of fundamental spectral techniques; to improve the state-of-the-art of fundamental graph algorithms in more practical settings; and to implement and evaluate these algorithms on real-word datasets. This research plan consists of foundational; algorithmic and applied aspects as summarised below: (1) Advance the foundations of spectral graph theory through the connections between graphs and manifolds; and develop the spectral theory for directed graphs (2) Algorithmic studies for matrix sparsification and graph clustering; with applications in dynamic and streaming settings (3) Develop algorithmic libraries for fundamental spectral algorithms; ,['N/A'],Dr He Sun 
heather-harrington,Applied mathematics; Dynamical systems & differential equations; Mathematical modelling; Deterministic (Mathematical modelling); Algebra; Geometry & topology; ,Dr Heather Harrington obtained her PhD in Applied Mathematics  at Imperial College London under the supervision of  Jaroslav Stark and Dorothy Buck. She stayed at  Imperial College London as a postdoc before moving to University of Oxford; UK.; Heather Harrington currently is a Royal Society University Research Fellow and Associate Professor of AppliedAlgebra and Data Science. Last year; Dr Harrington was awarded the London Mathematical Society Whitehead Prize `for her outstanding contributions to mathematical biology which have generated new biological insights using novel applications of topological and algebraic techniques’.; Dr Harrington's research focuses on the problem of reconciling models and data by extracting information about the structure of models and the shape of data. She develops models and methods to study primarily biological and chemical systems; however; her work is also applied towards engineering; medical; physical and social problems. Such analysis often requires working with data. Dr Harrington integrates techniques from a variety of disciplines such as computational algebraic geometry and topology; statistics; optimisation; network theory; and dynamical systems. She has applied these methods to different levels of organisation; for example; studying biological processes and their dysfunction at molecular; tissue and population scales.;  ; ,Dr Harrington's research focuses on the problem of reconciling models and data by extracting information about the structure of models and the shape of data. She develops models and methods to study primarily biological and chemical systems; however; her work is also applied towards engineering; medical; physical and social problems. Such analysis often requires working with data. Dr Harrington integrates techniques from a variety of disciplines such as computational algebraic geometry and topology; statistics; optimisation; network theory; and dynamical systems. She has applied these methods to different levels of organisation; for example; studying biological processes and their dysfunction at molecular; tissue and population scales.;  ; ,['N/A'],Dr Heather Harrington 
helen-ogden,Supervised learning; Social networks; Statistical methods & theory; Uncertainty quantification; Non-parametric & semi-parametric methods; Asymptotic (Statistical methods & theory); Modelling (Statistical methods & theory); ,Helen Ogden is an Lecturer in Statistics in the School of Mathematical Sciences at the University of Southampton; and a member of the Southampton Statistical Sciences Research Institute.  She obtained her PhD from the Department of Statistics; University of Warwick in 2014; then continued at Warwick as a Research Fellow; as part of 'Intractable Likelihood: New Challenges from Modern Applications'; a project held jointly between Bristol; Lancaster; Oxford and Warwick universities. She moved to the University of Southampton as a Lecturer in Statistics in September 2016.; Helen's research interests include statistical theory; modelling and computation. Much of her work involves models in which the distribution of the response is assumed to depend on the values of latent (or unobserved) attributes of each the items being observed. She is particularly interested in models which involve interactions between these latent variables; including models for competitions played between pairs of players; for social network data; and for educational testing and learning analytics.;  ; ,Helen's research interests include statistical theory; modelling and computation. Much of her work involves models in which the distribution of the response is assumed to depend on the values of latent (or unobserved) attributes of each the items being observed. She is particularly interested in models which involve interactions between these latent variables; including models for competitions played between pairs of players; for social network data; and for educational testing and learning analytics.;  ; ,['N/A'],Dr Helen Ogden 
henry-s-thompson,Communications; Natural language processing; Identity management; Cognitive science; Linguistics; Research methods; ,"Henry S. Thompson is Professor of Web Informatics; based in the Institute for Language; Cognition and Computation in the School of Informatics; University of Edinburgh. He studied at the University of California at Berkeley between 1968 and 1980; earning degrees in both Computer Science (BA; MSc) and Linguistics (MA; PhD). His research interests have ranged widely; originally in the areas of speech and language processing; more recently focusing on understanding and articulating the architectures of the web.; He retains an interest in the fundamental goals of cognitive science as originally conceived; believing that both sub-symbolic and symbolic mechanisms have a part to play in an adequate account of human cognition. He is committed to the proposition that our professional responsibilities and the success of our intellectual pursuits both depend on keeping our humanity clearly in view.; He was a member of the SGML Working Group of the World Wide Web Consortium (W3C) which designed XML; a major contributor to the core concepts of XSLT and W3C XML Schema and a member of the XML Core and XML Processing Model Working Groups at the W3C. He was elected five times to the W3C TAG (Technical Architecture Group); serving from 2005 to 2014; and continues to contribute to web standards through the W3C and IETF.; Thompson has been studying the role of URIs in the architecture of the web since his time on the W3C Technical Architecture Group. His work starts from the observation that reference is ""the connective tissue of scholarship""; and the fact that web links are increasingly the preferred mechanism for reference in scholarly work. Finding or making and recording connections is constitutive of scholarship of all kinds.; The recent enormous growth of material available to scholars; beginning with the advent of the web and accelerating as a result of widespread movement towards open content; has made the business of making connections at the same time more important and more difficult. More important; because connections impose structure; and without structure the undifferentiated mass of new information would be unusable. More difficult; because in many fields existing methodologies are not capable of dealing with the scale and complexity of information now available to them.; In many fields scholars are struggling to manage what may seem like it ought to be easy: giving things names. So it's not surprising that many disciplines have recognised a requirement for good names and set about designing systems for naming the things that matter to them. What is surprising is how often these efforts have failed. Thompson's current work is focused on developing reliable measures of naming-system success; leading to useful guidelines on how to get it right.; ","Thompson has been studying the role of URIs in the architecture of the web since his time on the W3C Technical Architecture Group. His work starts from the observation that reference is ""the connective tissue of scholarship""; and the fact that web links are increasingly the preferred mechanism for reference in scholarly work. Finding or making and recording connections is constitutive of scholarship of all kinds.; The recent enormous growth of material available to scholars; beginning with the advent of the web and accelerating as a result of widespread movement towards open content; has made the business of making connections at the same time more important and more difficult. More important; because connections impose structure; and without structure the undifferentiated mass of new information would be unusable. More difficult; because in many fields existing methodologies are not capable of dealing with the scale and complexity of information now available to them.; In many fields scholars are struggling to manage what may seem like it ought to be easy: giving things names. So it's not surprising that many disciplines have recognised a requirement for good names and set about designing systems for naming the things that matter to them. What is surprising is how often these efforts have failed. Thompson's current work is focused on developing reliable measures of naming-system success; leading to useful guidelines on how to get it right.; ",['N/A'], Henry S Thompson 
henry-wynn,Statistical methods & theory; Optimisation; ,Henry Wynn is the Principal Investigator at LSE on the EU-funded projects: CELSIUS (Combined Efficient Large Scale Integrated Urban Systems) and ReUseHeat (Recovery of Urban Excess Heat). He leads his own research group; the Decision Support and Risk Group (DSRG). He was head of the Department of Statistics from 2003 to 2006; and from 2000 to 2005 was also part-time Scientific Co-Director of EURANDOM; the international stochastics institute attached to Eindhoven Technical University (TUE); in the Netherlands.            ; He was a Lecturer and then Reader at Imperial College. He became Professor of Mathematical Statistics in 1985 at City University; London; and was Dean of Mathematics there from 1987 to 1995. At City University he co-founded the Engineering Design Centre of which he was co-Director and facilitated the introduction of new degrees; notably the MSc in Quality Improvement and System Reliability and the MSc in Research Methods and Statistics.; In 1995 he moved to the University of Warwick as Founding Director of the Risk Initiative and Statistical Consultancy Unit (RISCU); which he helped build to a leading centre of its kind; well-supported by a range of research grants. He was a founding president of the European Network for Business and Industrial Statistics (ENBIS); which now has over a thousand members and a successful annual conference.            ; He holds the Guy Medal in Silver from the Royal Statistical Society; the Box Medal from the European Network for Business and Industrial Statistics (ENBIS); is an Honorary Fellow of the Institute of Actuaries; a Fellow of the Institute of Mathematical Statistics; and has been awarded an Emeritus Fellowship by the Leverhulme Trust. He was awarded the Exzellenzstipendium des Landes Oberösterreich by the governor of Upper-Austria.            ; He was appointed chair of the Scientific Board of the Norwegian statistical research centre; Statistics for Innovation (sfi)2 in 2007.; ,['N/A'],['N/A'],Professor Henry Wynn 
hou-duo-qi,Numerical (Algorithms); Operations research; Machine learning; Optimisation; Convex programming; Nonlinear programming; ,Hou-Duo Qi is a professor in optimization at the School of Mathematics; the University of Southampton. He mainly works on optimization problems over correlations and distances among multivariate variables. He develops both theory and algorithms that strike a balance between theoretical computational complexity and practical computational speed aiming to deal with large scale data. One such a problem of the prototype is computing the nearest correlation matrix arising from finance. His recent work is on Euclidean distance matrix optimization that has applications in sensor network localization; molecular conformation; dimension reduction and data visualization.; Prior to joining Southampton University in 2004; He was QEII (Queen Elizabeth II) Fellow at the University of New South Wales; supported by the Australian Research Council. Professor Qi graduated from Peking University in Statistics (1990); and obtained PhD in Operational Research and Control Theory in Chinese Academy of Sciences (1996).; Distances among a number of items decide their relative positions. Global Positioning System (GPS) is one of such examples. The procedure of converting distances information to locations involves a branch of mathematics called distance geometry. It is known that we only need some of pairwise distances in order to determine the intrinsic structure among items due to the rigidity theory in mathematics. However; practical applications only have partial noisy distances. For example; in a larger network; a node only communicates with its neighbors. The current of research of Houduo Qi is to reconstruct the true structure of networks determined by the available noisy distances. His is main tool is optimization; which optimizes certain quantities of interest while keeping the distances information as accurate as possible. His research has found applications in sensor network localization; molecular conformation; dimension reduction; embedding on spheres and data visualization.; ,Distances among a number of items decide their relative positions. Global Positioning System (GPS) is one of such examples. The procedure of converting distances information to locations involves a branch of mathematics called distance geometry. It is known that we only need some of pairwise distances in order to determine the intrinsic structure among items due to the rigidity theory in mathematics. However; practical applications only have partial noisy distances. For example; in a larger network; a node only communicates with its neighbors. The current of research of Houduo Qi is to reconstruct the true structure of networks determined by the available noisy distances. His is main tool is optimization; which optimizes certain quantities of interest while keeping the distances information as accurate as possible. His research has found applications in sensor network localization; molecular conformation; dimension reduction; embedding on spheres and data visualization.; ,['N/A'],Professor Hou-Duo Qi 
hui-guo,Data structures; Cognitive science; Probabilistic programming; Probability; Simulation; Modelling (Statistical methods & theory); Monte Carlo methods; High dimensional inference; Causality; Uncertainty quantification; Parallel computing; Databases; Neural networks; Graph theory; Applications (Machine learning); ,Hui Guo is a senior lecturer in Biostatistics at the Centre for Biostatistics; University of Manchester. She holds a PhD in Statistics from Cambridge University where she also worked as a research associate. She obtained her MSc and PG Diploma in Statistics from UCL.; Hui's research focuses on causal inference from observational studies. Her current work is on identification of causal genes of clinical outcomes of interest using summary statistics from large-scale genetic association data. She is also interested in optimising predictive models and exploring genetic causal pathways and/or networks of certain diseases.; ,Hui's research focuses on causal inference from observational studies. Her current work is on identification of causal genes of clinical outcomes of interest using summary statistics from large-scale genetic association data. She is also interested in optimising predictive models and exploring genetic causal pathways and/or networks of certain diseases.; ,['N/A'],Dr Hui Guo 
hujun-yin,Robotics; Neural networks; Neuroscience; Pattern formation; Information retrieval; Neural & evolutionary computing; Computer vision; Deep learning; Pattern recognition; Graph theory; Stochastic optimisation; Verification; Cognitive science; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Time series; Probability; ,Hujun Yin is a Reader in data science and machine learning in the School of Electrical and Electronic Engineering at The University of Manchester. He received his PhD in neural networks from University of York in 1996; having obtained his MSc in signal processing and BEng in electronic engineering from Southeast University. He has a wide range of experience in signal/image analysis and data analytics and has had interdisciplinary projects in bioinformatics and neuroinformatic. His recent focus is in image recognition and deep learning with industrial applications.; His research interests range from theories and applications of neural networks; self-organising and deep learning systems in particular; image processing; enhancement and recognition; face recognition; nonstationary signal or time series modelling and prediction; dimensionality reduction and manifold learning. Though his core expertise is in unsupervised and manifold learning; he has developed a variety of methods in a wide range of fields such as gene expressions analysis; protein peptide spectral sequencing; neural signal decoding; signal/image based industrial monitoring; financial time series modelling; robust image feature extraction; as well as hyperspectral image analysis for plant monitoring.; Recently he is particularly interested in solving practical; industrial problems using deep learning frameworks; where unsupervised or data-independent means can be derived for efficient learning. Targeted applications include robust recognition of deformed image objects; imaging inverse problems; and enhanced modelling interpretation for noisy and intermittent signals.; ,His research interests range from theories and applications of neural networks; self-organising and deep learning systems in particular; image processing; enhancement and recognition; face recognition; nonstationary signal or time series modelling and prediction; dimensionality reduction and manifold learning. Though his core expertise is in unsupervised and manifold learning; he has developed a variety of methods in a wide range of fields such as gene expressions analysis; protein peptide spectral sequencing; neural signal decoding; signal/image based industrial monitoring; financial time series modelling; robust image feature extraction; as well as hyperspectral image analysis for plant monitoring.; Recently he is particularly interested in solving practical; industrial problems using deep learning frameworks; where unsupervised or data-independent means can be derived for efficient learning. Targeted applications include robust recognition of deformed image objects; imaging inverse problems; and enhanced modelling interpretation for noisy and intermittent signals.; ,['N/A'],Dr Hujun Yin 
hywel-williams,Multi-agent reasoning; Machine learning; Natural language processing; Social networks; Simulation; ,Hywel received his PhD in Complex Systems from University of Leeds in 2006. Since then he has worked in the departments of Environmental Science and Computer Science at University of East Anglia; before moving to Biosciences at University of Exeter in 2011. In 2017 he moved across campus to his current position in Computer Science; where he is a part of the new Institute for Data Science & Artificial Intelligence.; His research career has applied complex systems thinking and computational methods to problems in artificial intelligence; Earth system science; ecology; evolution; and more recently; social sciences. This interdisciplinary mix is unified by a methodological focus on simulation; network analysis and machine learning.; Current research interests focus primarily on the analysis of complex data from the wneteb and social media; with a particular emphasis on environmental issues. This work includes network analysis and text mining to understand online political and environmental debates; modelling/predicting collective attention in social media; understanding social biases in online news consumption; and using web data to track natural hazards.; ,Current research interests focus primarily on the analysis of complex data from the wneteb and social media; with a particular emphasis on environmental issues. This work includes network analysis and text mining to understand online political and environmental debates; modelling/predicting collective attention in social media; understanding social biases in online news consumption; and using web data to track natural hazards.; ,['N/A'],Dr Hywel Williams 
iain-styles,Information theory (Applied mathematics); Neural networks; Pattern formation; Neural & evolutionary computing; Applications (Machine learning); Computer vision; Deep learning; Pattern recognition; Semi-supervised learning; Unsupervised learning; Graph theory; Causality; High dimensional inference; Modelling (Statistical methods & theory); Geometry & topology; ,Iain Styles is a Senior Lecturer in the School of Computer Science at the University of Birmingham. His research interests are in the development of new computational methods for understanding complex biological experiments; with current interest centred on studying the biological role; dynamics; and structure-function relationship of proteins using techniques including single molecule microscopy; mass spectrometry; and alanine scanning mutagenesis. His methodological approaches are drawn from classical image analysis; statistical machine learning; and computational topology which he combines with classical simulation techniques such as molecular dynamics to build biologically interpretable models from large-scale data.; He is currently the co-Director of the EPSRC-funded Centres for Doctoral Training in Physical Science for Healthcare; and Deputy Director (Birmingham) of COMPARE - the Centre of Membrane Proteins and Receptor; an interdisciplinary research centre established jointly by the Universities of Birmingham and Nottingham to develop novel methods for studying single membrane proteins. His research has been supported by funding from EPSRC; the British Heart Foundation; the European Union; and the Dunhill Medical Trust. He was awarded his PhD in Theoretical Condensed Matter Physics from Birmingham in 2003.; Iain Styles’ current research interests are in developing new techniques for understanding high-content experimental data in discovery-driven biology. A major current focus is to take advantage of the central organising principle of compositionality in biology and to construct compositional representations of data that reflect the underlying biological structure. Such a representation would allow us to answer fundamental questions: What are the constituent parts? How are they organised with respect to each other? How does that organisation change over time?; Dr Styles is currently developing new methodological approaches to construct these models that draw on ideas from topology; statistical machine learning; and hypergraph theory. The testbeds for the new methods will be i) mass spectrometry imaging; where we aim to extract parts that correspond to chemical species and; compositions of parts that correspond to different tissue sub-types and ii) single molecule microscopy; where parts correspond to individual protein species; and their compositions to different components of the cell’s structure.; ,Iain Styles’ current research interests are in developing new techniques for understanding high-content experimental data in discovery-driven biology. A major current focus is to take advantage of the central organising principle of compositionality in biology and to construct compositional representations of data that reflect the underlying biological structure. Such a representation would allow us to answer fundamental questions: What are the constituent parts? How are they organised with respect to each other? How does that organisation change over time?; Dr Styles is currently developing new methodological approaches to construct these models that draw on ideas from topology; statistical machine learning; and hypergraph theory. The testbeds for the new methods will be i) mass spectrometry imaging; where we aim to extract parts that correspond to chemical species and; compositions of parts that correspond to different tissue sub-types and ii) single molecule microscopy; where parts correspond to individual protein species; and their compositions to different components of the cell’s structure.; ,['N/A'],Dr Iain Styles 
ian-craddock,Compression (Algorithms); Multi-agent systems; Evolution & adaptation; Neural networks; Neuroscience; Communications; Databases; Parallel computing; Human computer interface; Neural & evolutionary computing; Visualisation (Computer systems & architectures); Computer vision; Deep learning; Reinforcement learning; Semi-supervised learning; Supervised learning; Unsupervised learning; Differential privacy; Visualisation (Programming languages); Ethics; Research methods; Time series; ,"Ian Craddock is a Professor in the Faculty of Engineering at the University of Bristol. He is Institutional Lead for Digital Health; leading a programme of investment and research development across six faculties of his University.He leads the EPSRC funded ""SPHERE"" IRC programme; one of the UK's largest digital health research projects. His track record includes the successful commercialisation of a medical device.He was employed by Toshiba corporation as Managing Director of their research laboratory in Bristol from 2011 to late 2018.; Ian's research interests including embedded AI for constrained devices such as wearables; the use of AI in clinical decision support; the analysis and visualisation of time-domain streaming data and the interface between health; AI and society.; ",Ian's research interests including embedded AI for constrained devices such as wearables; the use of AI in clinical decision support; the analysis and visualisation of time-domain streaming data and the interface between health; AI and society.; ,['N/A'],Professor Ian Craddock 
ian-hall,['N/A'],Ian Hall is a reader of mathematical statistics at the University of Manchester. He gained his PhD at the University of Exeter in vortex dynamics; and subsequently worked for 15 years at Public Health England (PHE). In PHE he lead a mathematical modelling team planning for emergency responses to emerging infectious disease outbreaks. This included developing inferential tools to infer infection hazard area from early cases and planning for outbreaks such as smallpox and pandemic influenza. He is a member of the Department of Health's scientific pandemic influenza modelling subgroup (SPI-M).; His Turing related research involves an interest in understanding population movement at a range of scales; operational research to support outbreak mitigation strategies and real time modelling during outbreaks to forecast future case numbers.; ,His Turing related research involves an interest in understanding population movement at a range of scales; operational research to support outbreak mitigation strategies and real time modelling during outbreaks to forecast future case numbers.; ,['N/A'],Dr Ian Hall 
ian-horrocks,['N/A'],Ian Horrocks is a Professor of Computer Science at the University of Oxford and a Visiting Professor in the Department of Informatics at the University of Oslo. He is a Fellow of the Royal Society; a member of Academia Europaea; an ECCAI Fellow and a Fellow of the British Computer Society.  ; His research interests include logic-based knowledge representation and reasoning and semantic technologies; with a particular focus on ontology languages and applications. He was an author of the OIL; DAML+OIL; and OWL ontology language standards; chaired the W3C working group that standardised OWL 2; and developed many of the algorithms; optimisation techniques and reasoning systems that underpin OWL applications. His recent work includes query answering over ontologies and very large data sets; and applications in domains such as engineering; oil and gas; finance and medicine.; ,His research interests include logic-based knowledge representation and reasoning and semantic technologies; with a particular focus on ontology languages and applications. He was an author of the OIL; DAML+OIL; and OWL ontology language standards; chaired the W3C working group that standardised OWL 2; and developed many of the algorithms; optimisation techniques and reasoning systems that underpin OWL applications. His recent work includes query answering over ontologies and very large data sets; and applications in domains such as engineering; oil and gas; finance and medicine.; ,['N/A'],Professor Ian Horrocks 
ildar-farkhatdinov,Dynamical systems & differential equations; Robotics; Control theory; Game theory; Neuroscience; Nonlinear dynamics; Systems theory; Human computer interface; ,Dr Ildar Farkhatdinov is a Lecturer in Robotics at the School of Electrical Engineering and Computer Science at Queen Mary University of London (QMUL) and an Honorary Lecturer at the Department of Bioengineering of Imperial College London. Before joining QMUL he was a research associate at Imperial College of London where he was involved in human-robot interaction research in European projects BALANCE; SYMBITRON and COGIMON. He got his Ph.D. with distinction in Robotics from University Pierre and Marie Curie; Paris VI Sorbonne (Paris; France); M.Sc. in Mechanical Engineering from Korea University of Technology and Education (1st grade; Cheonan; South Korea); and B.Sc. with honours in Automation and Control from Moscow State University of Technology STANKIN (Moscow; Russia).; His primary research interests are in the field of human-robot/computer interaction; in particular; haptics; teleoperation; human sensory-motor system; as well as in design and control of robotic systems. He currently works on human balance control and its implementation for lower limb exoskeletons.; Ildar is interested in human-robot interaction. This includes development and evaluating ergonomic interfaces based on haptic; vestibular and visual sensory channels. The application areas include wearable robots; assistive machines and virtual reality.; ,Ildar is interested in human-robot interaction. This includes development and evaluating ergonomic interfaces based on haptic; vestibular and visual sensory channels. The application areas include wearable robots; assistive machines and virtual reality.; ,['N/A'],Dr Ildar Farkhatdinov 
ioanna-manolopoulou,['N/A'],Ioanna has been a Lecturer in Statistics at University College London since 2012. She completed her PhD at the University of Cambridge in 2008 under the guidance of Simon TavarÈ and Steve Brooks. She then joined SAMSI (Statistical and Applied Mathematical Sciences Institute) as Postdoctoral Fellow and Duke University as Visiting Assistant Professor; working with Mike West and Sayan Mukherjee.; Ioanna's work focuses on developing Bayesian modelling and inferential tools for complex data; involving space and time processes with inhomogeneous features. Her work has been applied to various applications including customer science; ecology; health economics and accounting. Methodologically; one of her directions within the ATI will be to develop new methods for dealing with data with complex dependencies or which have not been collected completely randomly. In terms of applications; her work in retail analytics aims to provide novel customer and product segmentations as well as devise more tailored prediction algorithms. She is also interested in health economics; in particular identifying population-wide treatment effects and studying the effect of uncertainty in public health decision-making.; ,Ioanna's work focuses on developing Bayesian modelling and inferential tools for complex data; involving space and time processes with inhomogeneous features. Her work has been applied to various applications including customer science; ecology; health economics and accounting. Methodologically; one of her directions within the ATI will be to develop new methods for dealing with data with complex dependencies or which have not been collected completely randomly. In terms of applications; her work in retail analytics aims to provide novel customer and product segmentations as well as devise more tailored prediction algorithms. She is also interested in health economics; in particular identifying population-wide treatment effects and studying the effect of uncertainty in public health decision-making.; ,['N/A'],Dr Ioanna Manolopoulou 
ioannis-kosmidis,Complexity (Algorithms); Numerical (Algorithms); Neuroscience; Applications (Machine learning); Semi-supervised learning; Supervised learning; Unsupervised learning; Stochastic optimisation; Visualisation (Programming languages); High dimensional inference; Monte Carlo methods; Non-parametric & semi-parametric methods; Time series; Asymptotic (Statistical methods & theory); Estimation theory; Modelling (Statistical methods & theory); ,"Ioannis Kosmidis is a Reader in Data Science at the Department of Statistics; University of Warwick and a Turing Fellow at The Alan Turing Institute; the UK's national institute for data science and artificial intelligence.; He obtained his BSc in Statistics from the Athens University of Economics and Business in 2004. He was awarded a PhD in Statistics in 2007 at the Department of Statistics; University of Warwick with a thesis titled ""Bias reduction in exponential family nonlinear models"". He then held an appointment as a CRiSM Research Fellow until 2010. In September 2010; he joined the Department of Statistical Science at University College London as a Lecturer (equivalent to Assistant Professor); and he got promoted to Senior Lecturer (equivalent to Associate Professor) in 2015. He moved back to University of Warwick as a Reader in Data Science in January 2018.; Ioannis' theoretical and methodological research focuses on optimal estimation and inference from complex statistical models; penalized and pseudo-likelihood methods and clustering. A particular focus of his work is the development of efficient; in terms of computational complexity and implementation; algorithms for applying the methods he develops to prominent data-analytic scenarios. He is doing extensive work in producing corresponding; well-documented; open-source software that delivers the methodological advances to the data science community and beyond (see his software page for information). Ioannis also actively engages in a range of cross-disciplinary applications (e.g. applications in earthquake engineering; finance; sport and health analytics; and neuroscience); particularly in settings where statistical modelling and the associated algorithms can impact policy-making. He is a founding member of the Data science for sports; activity; and well-being Turing Interest Group; before which he led and run the Statistics in Sports and Health research group at UCL between 2014 and 2017.He is an associate editor for Biometrika; Statistics and Computing and the Journal of Statistical Software. Detailed; up-to-date information on his research; teaching; enabling and engaging activities can be found on his website and his CV.; ",Ioannis' theoretical and methodological research focuses on optimal estimation and inference from complex statistical models; penalized and pseudo-likelihood methods and clustering. A particular focus of his work is the development of efficient; in terms of computational complexity and implementation; algorithms for applying the methods he develops to prominent data-analytic scenarios. He is doing extensive work in producing corresponding; well-documented; open-source software that delivers the methodological advances to the data science community and beyond (see his software page for information). Ioannis also actively engages in a range of cross-disciplinary applications (e.g. applications in earthquake engineering; finance; sport and health analytics; and neuroscience); particularly in settings where statistical modelling and the associated algorithms can impact policy-making. He is a founding member of the Data science for sports; activity; and well-being Turing Interest Group; before which he led and run the Statistics in Sports and Health research group at UCL between 2014 and 2017.He is an associate editor for Biometrika; Statistics and Computing and the Journal of Statistical Software. Detailed; up-to-date information on his research; teaching; enabling and engaging activities can be found on his website and his CV.; ,['N/A'],Dr Ioannis Kosmidis 
ioannis-papastathopoulos,['N/A'],Ioannis was born in Athens; Greece; and received his undergraduate training in Statistics and Insurance Science at the University of Piraeus. Then he received his MSc in Statistics and PhD in Statistics from Lancaster University and held a Brunel Research Fellowship in Statistics at the School of Mathematics; University of Bristol.  ; Ioannis's Turing research related interests and activities focus on the broad area of extreme value modelling and inference for rare and catastrophic events. Extreme events are multi-dimensional in nature; can cause havoc for the people affected and typically result in large financial losses. A wide variety of geophysical data is now acquired with very broad wavelength ranges from satellites and many other sources. This has led to increasingly complex and large datasets which require new methodological tools to analyse.; Iaonnis is particularly interested in exploring novel approaches for developing such tools and in identifying and communicating risks imposed by the society; economy and nature. His research interests include conditional dependence structures and graphical models; high-dimensional inference; scalable data fitting algorithms;  and spatio-temporal extreme value modelling.; ,Ioannis's Turing research related interests and activities focus on the broad area of extreme value modelling and inference for rare and catastrophic events. Extreme events are multi-dimensional in nature; can cause havoc for the people affected and typically result in large financial losses. A wide variety of geophysical data is now acquired with very broad wavelength ranges from satellites and many other sources. This has led to increasingly complex and large datasets which require new methodological tools to analyse.; Iaonnis is particularly interested in exploring novel approaches for developing such tools and in identifying and communicating risks imposed by the society; economy and nature. His research interests include conditional dependence structures and graphical models; high-dimensional inference; scalable data fitting algorithms;  and spatio-temporal extreme value modelling.; ,['N/A'],Dr Ioannis Papastathopoulos 
irene-ng,Data structures; Dynamical systems & differential equations; Pattern formation; Databases; Differential privacy; Management science; Research methods; Calculus & analysis; ,Professor Irene Ng is a Professor of Marketing and Service Systems at WMG; University of Warwick. She is also the Director of HATLAB at University of Warwick (50%) and the Chief Executive of HAT Data Exchange (50%). Her research is in economic engineering and service systems. She designs and engineer data platforms; transactions; economic and business models and specialise in the understanding of value and markets for the data and digital economy. Her PhD was from the National University of Singapore and her work is in pricing; value; economic models and service systems. Her interest is in economic engineering and the design of markets in the digital economy; combining economics; business; behaviours and technology in the space of data and AI.; Work in market design through 'microeconomic engineering' (Roth; 1991; 2008) have shown that transactions and institutions matter and could be redesigned to engender better market outcomes. Such engineering can be applied to personal data as an asset; so that it can be better used by organisations; governments and individuals from healthcare to finance without being encumbered by privacy; distrust and fear. Since digital personal data consist of bitstrings; and is created by the technology that collects it; it is possible that rights to the data could be retained by individuals if they legally owned the technological artefact that acquired such data. Such a technological artefact is the HAT (https://www.hubofallthings.com/).; The personal data within HATs (termed as person- controlled personal data - PPD) are a different asset class from the personal data sitting within organisations (termed as OPD). PPD is data where intellectual property rights and excludability of the data (control) is with individuals; even if the original source is from a corporation. This is important because property rights is said to be the most important factor for markets to exist; as markets not only enable the exchange of a good; but trade the various exclusive rights associated with the good in terms of its use; exclusion and alienability. The HAT allow individuals themselves to be a 'data controller' and 'data processor'. Through the individual's control of containerised microservices; individuals can give rights to the personal data within their database for their own benefit; deriving income from it or transferring it through a 'data debit' for services; using standard APIs.; As the HAT platform scales; there is an opportunity to obtain an empirical understanding of personal data exchanges and how the market evolves; especially for data signals. HAT Data Exchange; as the technology provider of HATs and through which all HAT metadata APIs are lodged; is willing to provide access to the data exchange as they are interested to establish a 'bloomberg-style' dashboard of data exchanges (much like stock exchange trades) within the HAT ecosystem. This would enable an understanding of the value of PPD to HAT owners; organisations; government and indeed; society; to the level of data granularity that has never been possible. It also enables the study of 'matching' - economic models focusing on who does what and who gets what; particularly when there is scarcity and allocation is an issue (Niederle; Roth and So?nmez; 2007). Within the 2 years of the Turing Fellowship; Professor Irene Ng aims to (1) create a taxonomic catalogue of PPD value based on the data transactions between HAT owners and apps to contribute to the understanding of its use (2) Study the longitudinal evolution of the PPD market in terms of how the HAT platform achieves 'matching' - creating thickness; reducing congestion and making the market safe (cf. Niederle er al.; 2007). The output of this research would be published and made a resource for future understanding of the economic; personal and societal value of personal data and its market.; Advisory Board member; Lloyds Register CTF International Fellow; Service Research Centre; Karlstad; Sweden Advisory Board member; NHH Centre for Service Innovation Senior Member; Wolfson College Cambridge ESRC/InnovateUK Caucus Thought Leader (2016-17) ESRC/NIHR Public Sector Placement Fellow (Apr - Sep 2010) ESRC/AIM Service Fellow (Oct 2008 - Sep 2010) ESRC/AIM Lead Service Fellow (Oct 2008 - Dec 2009); ,Work in market design through 'microeconomic engineering' (Roth; 1991; 2008) have shown that transactions and institutions matter and could be redesigned to engender better market outcomes. Such engineering can be applied to personal data as an asset; so that it can be better used by organisations; governments and individuals from healthcare to finance without being encumbered by privacy; distrust and fear. Since digital personal data consist of bitstrings; and is created by the technology that collects it; it is possible that rights to the data could be retained by individuals if they legally owned the technological artefact that acquired such data. Such a technological artefact is the HAT (https://www.hubofallthings.com/).; The personal data within HATs (termed as person- controlled personal data - PPD) are a different asset class from the personal data sitting within organisations (termed as OPD). PPD is data where intellectual property rights and excludability of the data (control) is with individuals; even if the original source is from a corporation. This is important because property rights is said to be the most important factor for markets to exist; as markets not only enable the exchange of a good; but trade the various exclusive rights associated with the good in terms of its use; exclusion and alienability. The HAT allow individuals themselves to be a 'data controller' and 'data processor'. Through the individual's control of containerised microservices; individuals can give rights to the personal data within their database for their own benefit; deriving income from it or transferring it through a 'data debit' for services; using standard APIs.; As the HAT platform scales; there is an opportunity to obtain an empirical understanding of personal data exchanges and how the market evolves; especially for data signals. HAT Data Exchange; as the technology provider of HATs and through which all HAT metadata APIs are lodged; is willing to provide access to the data exchange as they are interested to establish a 'bloomberg-style' dashboard of data exchanges (much like stock exchange trades) within the HAT ecosystem. This would enable an understanding of the value of PPD to HAT owners; organisations; government and indeed; society; to the level of data granularity that has never been possible. It also enables the study of 'matching' - economic models focusing on who does what and who gets what; particularly when there is scarcity and allocation is an issue (Niederle; Roth and So?nmez; 2007). Within the 2 years of the Turing Fellowship; Professor Irene Ng aims to (1) create a taxonomic catalogue of PPD value based on the data transactions between HAT owners and apps to contribute to the understanding of its use (2) Study the longitudinal evolution of the PPD market in terms of how the HAT platform achieves 'matching' - creating thickness; reducing congestion and making the market safe (cf. Niederle er al.; 2007). The output of this research would be published and made a resource for future understanding of the economic; personal and societal value of personal data and its market.; ,Advisory Board member; Lloyds Register CTF International Fellow; Service Research Centre; Karlstad; Sweden Advisory Board member; NHH Centre for Service Innovation Senior Member; Wolfson College Cambridge ESRC/InnovateUK Caucus Thought Leader (2016-17) ESRC/NIHR Public Sector Placement Fellow (Apr - Sep 2010) ESRC/AIM Service Fellow (Oct 2008 - Sep 2010) ESRC/AIM Lead Service Fellow (Oct 2008 - Dec 2009); ,Professor Irene Ng 
isaac-taylor,Ethics; ,Isaac Taylor is a Research Fellow in the Defence and Security Programme. He is a philosopher looking at the ethical issues surrounding military applications of AI.Prior to joining The Alan Turing Institute; he was a Scholar in Residence at the University of Colorado Boulder and a Postdoctoral Fellow at Goethe University Frankfurt. He holds a DPhil in Political Theory from the University of Oxford. He has previously worked in a number of complementary areas in moral and political philosophy; and his book; The Ethics of Counterterrorism; was published by Routledge in 2018.; ,['N/A'],['N/A'],Dr Isaac Taylor 
ivan-palomares-carrascosa,Management science; Ethics; Data science of government & politics; Cognitive science; Research methods; Social media; Information retrieval; Reinforcement learning; Applications (Machine learning); ,Iván Palomares Carrascosa is a Lecturer in Data Science and Artificial Intelligence with the School of Computer Science; University of Bristol; UK. Since November 2018; he is also a Fellow of The Alan Turing Institute; where he and his team members investigate personalisation methods for assisting citizens to engage with healthy habit engagement and development; and smart cities applications. He currently leads the Decision Support and Recommender Systems research group at the University of Bristol; where he supervises PhD candidates; postdoctoral and visiting researchers. His research interests include data-driven and intelligent approaches for recommender systems; personalisation for leisure and tourism in smart cities; large group decision making and consensus; data fusion; opinion dynamics and human-machine decision support.Iván received his two MSc degrees in Computer Science (with Faculty and Nationwide Distinctions) and Soft Computing & Intelligent Systems (Hons); from the University of Jaen; Spain and University of Granada; Spain; in 2009 and 2011 respectively. He received his PhD degree in Computer Science with Nationwide distinctions from the University of Jaen; Spain; in 2014. His research results have been published in top journals and conference proceedings; including IEEE Transactions on Fuzzy Systems; Applied Soft Computing; International Journal of Intelligent Systems; Information Fusion; Knowledge-Based Systems; Data and Knowledge Engineering; and Renewable & Sustainable Energy Reviews; amongst others. He serves as a reviewer in numerous top-tier international journals in related areas to decision support systems.; Iván has published two books; the last of which is the first compilation of research on Large Group Decision Making to date.; Maintaining a healthy and active lifestyle to prevent the development of chronic diseases; physical or mental problems; constitutes an important societal challenge nowadays. This is particularly evident in large cities; as they are often characterised by a busy and dynamic lifestyle. In such environments; fostering citizen engagement with suitable (and usually tailored) daily activity patterns that they like; would therefore be a key factor to improving their overall wellbeing. Providing personalised recommendations on daily life activities (exercising; eating; etc.) that drive the development of healthy habits; might not only impede the development of diseases but also help maintaining a positive physical and mental state of wellbeing in their daily lives.; Iván's proposed research aims at investigating how Recommender System approaches can help creating personalisation services for engaging users with a healthier and active lifestyle. Techniques involved include intelligent data fusion; preference and behaviour modelling; social and sensor data analysis; and multi-criteria decision making.; Iván has expertise in the following areas:; ,Maintaining a healthy and active lifestyle to prevent the development of chronic diseases; physical or mental problems; constitutes an important societal challenge nowadays. This is particularly evident in large cities; as they are often characterised by a busy and dynamic lifestyle. In such environments; fostering citizen engagement with suitable (and usually tailored) daily activity patterns that they like; would therefore be a key factor to improving their overall wellbeing. Providing personalised recommendations on daily life activities (exercising; eating; etc.) that drive the development of healthy habits; might not only impede the development of diseases but also help maintaining a positive physical and mental state of wellbeing in their daily lives.; Iván's proposed research aims at investigating how Recommender System approaches can help creating personalisation services for engaging users with a healthier and active lifestyle. Techniques involved include intelligent data fusion; preference and behaviour modelling; social and sensor data analysis; and multi-criteria decision making.; Iván has expertise in the following areas:; ,,Dr Iván Palomares Carrascosa 
jacek-brodzki,['N/A'],"Jacek Brodzki is Professor of Pure Mathematics at the University of Southampton and an expert in topology; topological data analysis; and has strong interests and experience in direct applications of pure mathematics. He has gained his DPhil in Pure Mathematics at Oxford under the guidance of Daniel Quillen. At Southampton; he has created and leads a research group in applied topology that investigates theoretical foundations and applications of topological data analysis; including applications in medicine; biology; computer science; and chemistry. He has a very considerable experience in leading interdisciplinary projects based on applications of pure mathematics. He is the PI of the EPSRC-sponsored project ""Joining the Dots: From Data to Insight"" which forges connections between topological data analysis; machine learning and statistics with emphasis on applications to complex problems in the sciences and medicine. He is a co-founder and Director of the Centre for Geometry; Topology and Applications at Southampton. This new initiative will create a focus for research in the area of overlap between Geometry; Topology; with a strong interest in applications and will serve as a point of contact for future collaborations. ; The modern world thrives on data. While we still tend to think of data as numbers; most of the data created in modern science; medicine; biology; in digital economy; in commerce; digital security is vastly more varied: it can be a set of images; a collection of objects; or a dynamic profile of an evolving system; it can be sound or video recordings;  it can take the form of tweets; or posts on discussion forums. A common thread in modern data science is the analysis; classification and comparison of complex shapes. This is why topology; a branch of mathematics dedicated to the study of shape; is so useful in this context. It has been developed to provide numerical characteristics that allow the classification and comparison of complex structures. Its flexibility; computational power; and impressive visualisation tools have been used in recent breakthrough results in the study of cancer; asthma; chronic lung diseases; and many others. Jacek Brodzki is interested in developing new applications of topological ideas as well as foundational research into the methodology itself. His current research interests include the study of the uses of topology to detect efficient architectures of neural networks. ; ",The modern world thrives on data. While we still tend to think of data as numbers; most of the data created in modern science; medicine; biology; in digital economy; in commerce; digital security is vastly more varied: it can be a set of images; a collection of objects; or a dynamic profile of an evolving system; it can be sound or video recordings;  it can take the form of tweets; or posts on discussion forums. A common thread in modern data science is the analysis; classification and comparison of complex shapes. This is why topology; a branch of mathematics dedicated to the study of shape; is so useful in this context. It has been developed to provide numerical characteristics that allow the classification and comparison of complex structures. Its flexibility; computational power; and impressive visualisation tools have been used in recent breakthrough results in the study of cancer; asthma; chronic lung diseases; and many others. Jacek Brodzki is interested in developing new applications of topological ideas as well as foundational research into the methodology itself. His current research interests include the study of the uses of topology to detect efficient architectures of neural networks. ; ,['N/A'],Professor Jacek Brodzki 
jack-stilgoe,Robotics; Applications (Machine learning); Deep learning; Data science of government & politics; Ethics; ,Dr Jack Stilgoe is an Associate Professor in Science and Technology Studies at University College London. He has spent his professional life in the overlap between science policy research and science policy practice; first at UCL’s department of Science and Technology Studies; then at the think tank Demos; then the Royal Society and then the University of Exeter.At Demos; he initiated and ran a range of projects; advising and working with organisations such as BIS; Defra; the Food Standards Agency; EPSRC; BBSRC; MRC; Practical Action; the Environment Agency; the European Space Agency; Unilever and Pfizer. At the Royal Society; he ran the study that produced the influential report The Scientific Century. He has published and presented his academic and policy research to audiences around the world. Working with academics; policymakers; the media and others; he has helped advance debates on emerging technologies; public engagement with science; the use of expert advice and the value of innovation.; Jack Stilgoe's project explores how machine learning is being developed and used for self-driving cars. In particular; it asks how social learning – the processes by which society learns about new technologies – can be maximised. The project will ask: ; ,Jack Stilgoe's project explores how machine learning is being developed and used for self-driving cars. In particular; it asks how social learning – the processes by which society learns about new technologies – can be maximised. The project will ask: ; ,['N/A'],Dr Jack Stilgoe 
james-cheney,Databases; Verification; Probabilistic programming; Software framework development; Ethics; Causality; ,James Cheney is a Reader in the Laboratory for Foundations of Computer Science; University of Edinburgh. He earned his PhD at Cornell University in 2004; moved to Edinburgh as a postdoctoral researcher and in 2008 was awarded a Royal Society University Research Fellowship. He currently holds an ERC Consolidator Grant and his research has also been funded by Microsoft Research; Google; LogicBlox; the US Air Force Office of Scientific Research; DARPA; EPSRC and the National Cyber Security Centre Research Institute on Verified Trustworthy Software Systems.; Dr Cheney's research spans topics in programming languages; databases; verification; and (recently) security. His main research interests relate to provenance and data curation and connected topics such as information flow and slicing in programming languages; cross-tier web and database programming; justification and explainability in databases; configuration management; or other complex systems; and mining or anomaly detection for security applications.; ,Dr Cheney's research spans topics in programming languages; databases; verification; and (recently) security. His main research interests relate to provenance and data curation and connected topics such as information flow and slicing in programming languages; cross-tier web and database programming; justification and explainability in databases; configuration management; or other complex systems; and mining or anomaly detection for security applications.; ,['N/A'],Dr James Cheney 
jared-tanner,['N/A'],Jared Tanner is Professor of the Mathematics of Information at the University of Oxford and the Turing University Lead for Oxford. He obtained his PhD (2002) in applied mathematics at the University of California at Los Angeles; and was a postdoctoral fellow at the University of California at Davis (Maths) and Stanford University (Stats.) where he worked with David L. Donoho. Prior to joining the University of Oxford in 2012 he was Professor of the Mathematics of Information at the University of Edinburgh (2007-2012). ; He is founding editor-in-chief of Information and Inference: A Journal of the IMA; whose mission is to publish high quality mathematically oriented articles furthering the understanding of the theory; methods of analysis; and algorithms for information and data. He is also on the editorial board for Applied and Computational Harmonic Analysis; Multiscale modelling and simulation A SIAM Interdisciplinary Journal; and was an associate editor for the Princeton Companion to Applied Mathematics. His research has appeared in the Proc Natl Acad Sci USA; Phil Trans Royal Soc A; and other leading journals.; Jared Tanner’s research concerns extracting models of high dimensional date which reveal of the essential information in the data.  Specific contributions include the derivation of sampling theorems in compressed sensing using techniques from stochastic geometry and the design and analysis of efficient algorithms for matrix completion which minimise over higher dimensional subspaces as the reliability of the data warrants.  These techniques allow more efficient information acquisition as well as the ability to cope with missing data. Recent interests include new models for low dimensional structure in heterogeneous data and topological data analysis.; ,Jared Tanner’s research concerns extracting models of high dimensional date which reveal of the essential information in the data.  Specific contributions include the derivation of sampling theorems in compressed sensing using techniques from stochastic geometry and the design and analysis of efficient algorithms for matrix completion which minimise over higher dimensional subspaces as the reliability of the data warrants.  These techniques allow more efficient information acquisition as well as the ability to cope with missing data. Recent interests include new models for low dimensional structure in heterogeneous data and topological data analysis.; ,['N/A'],Professor Jared Tanner 
jason-rentfrow,Multi-agent reasoning; Human computer interface; Deep learning; Speech recognition; Cognitive science; Data science of government & politics; Ethics; Research methods; Social media; Social psychology; ,Jason Rentfrow is Reader in Personality and Individual Differences in the Department of Psychology at the University of Cambridge. He is also a Fellow of Fitzwilliam College. His research concerns person-environment interactions and focuses on the ways in which personality is expressed in everything from people's preferences for music to the places in which they live. A related interest concerns the development of new methods for studying behavioural manifestations of personality; including the use of online social media and mobile sensors.; His research has been published in international peer-reviewed journals; presented at international scientific conferences; and featured in radio; television; and print media; including BBC; NPR; CNN; The New York Times; Los Angeles Times; Sunday Times; The Economist; Boston Globe; Washington Post; Psychology Today; and Science. He has also worked as a consultant and scientific advisor for several organisations; including the BBC and EMI Music; as well as as tech start-ups; including Emotech and Neener.; Data from mobile sensors and online social media are among the most prominent sources of information about human activity. Therefore; efforts to understand the psychological information that such data can reveal will have significant implications for politics; economics; healthcare; and industry. To that end; Jason's aims as a Turing Fellow are to establish collaborations with researchers from different disciplines to work on research concerned with evaluating the validity of mobile sensor data and online social media data for making predictions about the psychological characteristics of users.; Given the Institute's strong focus on machine learning; one of his goals is to focus on research concerned with the types of psychological information that can be gleaned from mobile sensor data. Using mobile sensor data for several thousand users; one question that he plan to examine is whether valid information about personality or mood can be inferred from the behavioural and contextual data gathered from mobile sensors.; Another goal is to focus on how social media data can be used to measure the psychological characteristics of places. Places vary considerably on a range of important political; economic; social; and health outcomes. However; a limitation of most geographical research in this area is that the spatial resolution of the data is rather course; making it hard to obtain reliable estimates at a fine-grain level of analysis (e.g.; neighbourhoods). Online social media (e.g.; Twitter; Flickr) as well as data from Google Street View has the potential to overcome that limitation.; ,Data from mobile sensors and online social media are among the most prominent sources of information about human activity. Therefore; efforts to understand the psychological information that such data can reveal will have significant implications for politics; economics; healthcare; and industry. To that end; Jason's aims as a Turing Fellow are to establish collaborations with researchers from different disciplines to work on research concerned with evaluating the validity of mobile sensor data and online social media data for making predictions about the psychological characteristics of users.; Given the Institute's strong focus on machine learning; one of his goals is to focus on research concerned with the types of psychological information that can be gleaned from mobile sensor data. Using mobile sensor data for several thousand users; one question that he plan to examine is whether valid information about personality or mood can be inferred from the behavioural and contextual data gathered from mobile sensors.; Another goal is to focus on how social media data can be used to measure the psychological characteristics of places. Places vary considerably on a range of important political; economic; social; and health outcomes. However; a limitation of most geographical research in this area is that the spatial resolution of the data is rather course; making it hard to obtain reliable estimates at a fine-grain level of analysis (e.g.; neighbourhoods). Online social media (e.g.; Twitter; Flickr) as well as data from Google Street View has the potential to overcome that limitation.; ,['N/A'],Dr Jason Rentfrow 
jatinder-singh,Data structures; Systems theory; Communications; Databases; Parallel computing; Human computer interface; Operating systems; Real time computing; Differential privacy; Identity management; Verification; Data science of government & politics; Ethics; Research methods; Social media; ,Dr Jat Singh is an EPSRC Research Fellow at the Department of Computer Science & Technology; University of Cambridge. There he leads the newly-formed 'Compliant and Accountable Systems' research group; and is also co-investigator of the Microsoft Cloud Computing Research Centre; a tech-legal collaboration with QMUL. He also co-chairs the Cambridge Trust & Technology Initiative; which drives research exploring the dynamics of trust and distrust in relation to internet technologies; society and power. Jat is active in the tech-policy space; serving on advisory councils for the UK Government and Financial Conduct Authority. He completed his PhD in Computer Science at the University in Cambridge; has several years of commercial experience in the areas of health and legal systems; and has studied some law.; Jat's research concerns taking an interdisciplinary (tech-legal) approach to tackling issues of governance; control; agency; accountability and trust regarding emerging technology. Key areas of focus include security; privacy; transparency and accountability; particularly in the context of cloud and distributed environments (Internet of Things).; ,Jat's research concerns taking an interdisciplinary (tech-legal) approach to tackling issues of governance; control; agency; accountability and trust regarding emerging technology. Key areas of focus include security; privacy; transparency and accountability; particularly in the context of cloud and distributed environments (Internet of Things).; ,['N/A'],Dr Jatinder Singh 
jean-baptiste-cazier,Complexity (Algorithms); Numerical (Algorithms); Dynamical systems & differential equations; Pattern formation; Applications (Machine learning); Deep learning; Natural language processing; Pattern recognition; Unsupervised learning; Data science of government & politics; Geometry & topology; ,"Prof. Jean-Baptiste Cazier; chair of Bioinformatics; Director of the Centre for Computational Biology; UoB; is originally trained in mathematical modelling. He joined the field of human genetics when introduced to this fast-evolving field in an academic spirit of research and excellence in Iceland at deCode Genetics; developing further methods combining linkage and case-control association to identify genes responsible for common complex diseases. While employed by Cancer Research UK; he worked alongside experts in most aspects of bioinformatics and biostatistics in the context of cancer and collaborated with scientists and clinicians on genome-wide association; copy number variations or high-throughput sequencing; especially of colorectal cancer and leukaemia which led to the identification of many genomic susceptibility variants conferring higher; susceptibility and progression; risk of various cancers.; Joining the Wellcome Trust Centre for Human Genetics in Oxford; where he developed new methods such as a population genetics analysis for genome-wide association in a Middle Eastern cohort. These further studies in population stratification and admixture mapping to perform more accurate analysis across heterogeneous cohorts led to new analytical methods being developed in collaboration with the Department of Statistics. After acting as joint Head of the Bioinformatics and Statistical Genetics Core; he supervised the development of analytical approaches and tools for the analysis of whole genome sequencing projects (WGS500) with a special focus on immune disorders and cancers. He then joined the Department of Oncology to create a Bioinformatics group to both provide support to the department and lead independently funded research.; In 2014; Jean-Baptiste Cazier joined the University of Birmingham taking up the new chair of Bioinformatics to create the Centre for Computational Biology. This university-wide effort aims to promote excellence in Computational Biology; Data Science for the Life Sciences; and Bioinformatics across the range of fundamental and applied sciences; in both the University and allied Health Care arenas. In this post he has developed a Population Diversity approach and now leads the Population Diversity domain of the 100;000 Genome Project and extended his collaborations from Lebanon to Chilean; Brazilian; Indian and Chinese populations. He has been receiving funds from numerous grants internal; national such as MRC; BBSRC; Cancer Research UK; Wellcome Trust; Innovate UK; as well as international EU H2020; worth more than £14M.; Prof Cazier leads the ""Population Diversity at varying scales"" project with The Alan Turing Institute. The ambitious goal of improving the Health of Individuals; in all their diversity; can be achieved by the confluence of the complementary fields of population genetics; epidemiology; socio-economics; clinical and environmental studies underpinned by the commonality of data sciences at various scales. Each of these domains; in isolation or in pair; aspires to improve the population well-being through a diversity of approaches. We are therefore proposing to define a novel framework to allow the characterisation; integration; comparison of the underlying structures found in the diversity of data; across data types; quantitative (discrete or continuous) or qualitative (ordinal or nominal); scale and studies. ; This project aims to define a novel mathematical framework to make use of existing datasets across studies; independently of their examined conditions; datatype; scale and location. With this novel approach; we expect to obtain a better understanding of the underlying relationship between datatypes in various context; thus enabling the integration and projection of diverse and sparse datasets in any context. In fine; understanding underlying data structure and homologies will improve analysis; streamline collection and minimise cost and noise. While there are on-going efforts to select features from complex biomedical datasets; we aim to go one step beyond; and explore the structure of the information rather than datasets themselves; allowing us to compare and transpose these findings from study to study. ; ","Prof Cazier leads the ""Population Diversity at varying scales"" project with The Alan Turing Institute. The ambitious goal of improving the Health of Individuals; in all their diversity; can be achieved by the confluence of the complementary fields of population genetics; epidemiology; socio-economics; clinical and environmental studies underpinned by the commonality of data sciences at various scales. Each of these domains; in isolation or in pair; aspires to improve the population well-being through a diversity of approaches. We are therefore proposing to define a novel framework to allow the characterisation; integration; comparison of the underlying structures found in the diversity of data; across data types; quantitative (discrete or continuous) or qualitative (ordinal or nominal); scale and studies. ; This project aims to define a novel mathematical framework to make use of existing datasets across studies; independently of their examined conditions; datatype; scale and location. With this novel approach; we expect to obtain a better understanding of the underlying relationship between datatypes in various context; thus enabling the integration and projection of diverse and sparse datasets in any context. In fine; understanding underlying data structure and homologies will improve analysis; streamline collection and minimise cost and noise. While there are on-going efforts to select features from complex biomedical datasets; we aim to go one step beyond; and explore the structure of the information rather than datasets themselves; allowing us to compare and transpose these findings from study to study. ; ",['N/A'],Professor Jean-Baptiste Cazier 
jeanine-houwing-duistermaat,Modelling (Statistical methods & theory); ,['N/A'],Jeanine's work is motivated by ongoing collaborations with clinicians; biologists; psychologists and chemists. Random effects models is the research theme which is the basis of her work especially in family data. Within the EU funded FP7 Consortium MIMOmics; which she coordinates; methods for integrated analysis of multiple omics data are being developed. Finally using data from electronic patient records for risk prediction and study design is a new and interesting research topic in statistics.; ,['N/A'],Professor Jeanine Houwing-Duistermaat 
jian-qing-shi,Data structures; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Simulation; Time series; ,Dr Jian Qing Shi is a Reader in Statistics at the School of Mathematics; Statistics & Physics at Newcastle University; UK. Before coming to Newcastle; he worked at Universities of Glasgow and Warwick. He received his PhD in Statistics from the Chinese University of Hong Kong in 1996. His research interests include nonlinear functional data analysis with applications in engineering and medical research; analysis of missing data and covariance structure analysis. He was the assistant director of the EPSRC Centre of Doctoral Training for Cloud Computing for Big Data at Newcastle University.; Dr Jian Qing Shi's research at The Alan Turing Institute specialises in developing novel statistical models to capture information from complex process data with very large scales; such as free-living accelerometer data and movement data; and to develop predictive; classification or clustering tools for precision monitoring and timely diagnoses of neurological conditions and disorders; enabling real-time evidence-based decision making. Their applications focus on developing low-cost monitoring and diagnosis tools for upper limb rehabilitation after stroke and age related neurodegenerative diseases (dementia and Parkinson's disease); both involving neurological disorders. The research uses wearable sensors; such as multi-axis accelerometers and wireless controllers; to offer a low cost solution for continuous and unobtrusive collection of process data enabling to develop a home-based data collection and on-line real-time decision making system.; ,Dr Jian Qing Shi's research at The Alan Turing Institute specialises in developing novel statistical models to capture information from complex process data with very large scales; such as free-living accelerometer data and movement data; and to develop predictive; classification or clustering tools for precision monitoring and timely diagnoses of neurological conditions and disorders; enabling real-time evidence-based decision making. Their applications focus on developing low-cost monitoring and diagnosis tools for upper limb rehabilitation after stroke and age related neurodegenerative diseases (dementia and Parkinson's disease); both involving neurological disorders. The research uses wearable sensors; such as multi-axis accelerometers and wireless controllers; to offer a low cost solution for continuous and unobtrusive collection of process data enabling to develop a home-based data collection and on-line real-time decision making system.; ,['N/A'],Dr Jian Qing Shi 
jianxin-pan,Data structures; Numerical analysis; Neural networks; Pattern formation; Deep learning; Pattern recognition; Convex programming; Software framework development; Uncertainty quantification; High dimensional inference; Simulation; Time series; Estimation theory; Probability; ,Jianxin Pan is Professor of Statistics in the School of Mathematics; The University of Manchester. He received his BSc in Mathematics from Yunnan University of China in 1983; MSc in Statistics from Chinese Academy of Sciences in 1987 and PhD in Statistics from Hong Kong Baptist University in 1996. He was a PDRA at the Rothamsted Experiment Station from 1996 to 1999 and Research Associate at The University of St Andrews from 1999 to 2000. Jianxin Pan was appointed as a Lecturer by Keele University from 2000 to 2002 and by University of Manchester in 2002. He was promoted to Senior Lecturer in 2004 and Reader in 2005.; Since August 2006; Jianxin Pan has been a full Professor of Statistics at The University of Manchester. He was the Head of the Probability and Statistics Group in the School of Mathematics (2009-2012); Fellow of the Royal Statistical Society (2007-) and Elected Member of the International Statistical Institute (2007-). He is currently an Associate Editor for Biometrics (2008-) and Biometrical Journal (2016-).; Professor Jianxin Pan has long-standing research interests in statistical modelling for complex and challenging data arising from health and medicine. He is particularly interested in modelling for longitudinal data; survival data; competing risk data; multistate event data; missing data; measurement error data; spatial data and high-dimensional data with complex structures. He has had research collaborations with academic clinicians on many diseases including cardiovascular disease; traumatic brain injury; arthritis; diabetes; HIV and cancer.; His research outcomes were published in high impacted journals in both statistics and medicine. He had research funding supports from various research councils including EPSRC; MRC; NIHR; NERC and EU. Jianxin Pan has recently been developing research interests in data-centric engineering; involving measuring the uniformity or non-uniformity of mixture for different metal materials and determining the time to uniformity; among others.; ,Professor Jianxin Pan has long-standing research interests in statistical modelling for complex and challenging data arising from health and medicine. He is particularly interested in modelling for longitudinal data; survival data; competing risk data; multistate event data; missing data; measurement error data; spatial data and high-dimensional data with complex structures. He has had research collaborations with academic clinicians on many diseases including cardiovascular disease; traumatic brain injury; arthritis; diabetes; HIV and cancer.; His research outcomes were published in high impacted journals in both statistics and medicine. He had research funding supports from various research councils including EPSRC; MRC; NIHR; NERC and EU. Jianxin Pan has recently been developing research interests in data-centric engineering; involving measuring the uniformity or non-uniformity of mixture for different metal materials and determining the time to uniformity; among others.; ,['N/A'],Professor Jianxin Pan 
jim-smith,Multi-agent systems; Artificial intelligence; Game theory; Machine learning; Natural language processing; Graph theory; Social data science; Ethics; Uncertainty quantification; Causality; High dimensional inference; Time series; Information theory (Statistical methods & theory); Modelling (Statistical methods & theory); Algebra; Geometry & topology; ,Jim Smith is a foundational Bayesian Statistician and Decision Analyst based at Warwick University. He started as a mathematical statistician specialising in dynamic dynamic models especially their implicit geometry and algebraic description and still publishes in these areas. However most of his work over the last twenty years or so lies on the interface between machine learning; statistics and operations research particularly within the realm of complex models and high dimensional data. He is especially skilled in developing bespoke graphical representations of problems - often dynamic ones such as dynamic Bayes nets; flow graphs multiregression dynamic models and most recently chain event graphs.; He has also written widely on Bayesian causal reasoning; developed methodologies for eliciting the structure of models from domain experts and developing decision support systems that integrate in a coherent way data sources that are very different form each other but pertain to the same processes. He has worked in a wide range of domains including; most recently; food security modelling; nuclear safety; biological regulation; brain connectivity; public health; military decision making and radicalisation processes. ; He is working under a number of themes within Turing. One stream of work is the statistical modelling of violent criminal populations of various kinds. Another stream of work focuses on forensic inference - its graphical representation and the combination of different evidence types. Finally he is CI on a project designed to help government use data sources wisely and in a balanced way. He has also developed an interest in natural language processing and is currently researching ways causal relationships can be extracted from natural language descriptions of text written by engineers.; ,He is working under a number of themes within Turing. One stream of work is the statistical modelling of violent criminal populations of various kinds. Another stream of work focuses on forensic inference - its graphical representation and the combination of different evidence types. Finally he is CI on a project designed to help government use data sources wisely and in a balanced way. He has also developed an interest in natural language processing and is currently researching ways causal relationships can be extracted from natural language descriptions of text written by engineers.; ,['N/A'],Professor Jim Smith 
jinghao-xue,Computer vision; Deep learning; Pattern recognition; High dimensional inference; ,Dr Jing-Hao Xue received a BEng degree in telecommunication and information systems in 1993 and a DrEng degree in signal and information processing in 1998; both from Tsinghua University. He received an MSc degree in medical imaging and an MSc degree in statistics; both from Katholieke Universiteit Leuven in 2004; and a PhD degree in statistics from the University of Glasgow in 2008. He has worked in the Department of Statistical Science at University College London since 2008; as a lecturer (2008) and senior lecturer (2014).; His research interests include statistical machine learning; high-dimensional data analysis; pattern recognition and image analysis.; ,His research interests include statistical machine learning; high-dimensional data analysis; pattern recognition and image analysis.; ,['N/A'],Dr Jinghao Xue 
joao-porto-de-albuquerque,Human computer interface; Deep learning; Data science of government & politics; Ethics; Management science; Research methods; Social media; ,João Porto de Albuquerque is Associate Professor and Impact Director at the Centre for Interdisciplinary Methodologies of the University of Warwick; for which he acts as Co-Director of the Centre for Urban Science and Progress London. He has an interdisciplinary background in computational; social and geographical sciences and conducts research in the interdisciplinary fields of Urban Analytics and Socio-spatial Data Science. He develops innovative methods to improve our understanding of sociotechnical urban environments; and his current research interests include digital participation; urbanisation in the global South; and sustainable development. He is alumnus fellow of the Alexander von Humboldt Foundation and since 2014 acts as Visiting Professor at the GIScience Chair of the Institute of Geography of Heidelberg University (Germany).; He previously held a post at the Institute of Mathematical and Computing Sciences of the University of São Paulo; Brazil; with he still maintains an honorary affiliation. He has secured research grants in excess of £7m (£1m as a PI) from diverse funding bodies such as the Economics and Social Science Research Council (ESRC); Engineering and Physical Sciences Research Council (EPSRC); Global Challenges Research Fund; National Institute for Health Research (NIHR); LUBW/Environment Agency of the State of Baden-Württemberg (Germany); CAPES and FAPESP (Brazil). He is a fellow of the Royal Geographical Society and a member of the Association for Information Systems; where he currently serves as President of the Special Interest Group on Organisational Research (SIGOSRA).; At the Turing; Dr João Porto de Albuquerque will be dedicated to advance interdisciplinary methods in sociospatial data science with applications to improving urban sustainable development; with a particular focus on urban resilience and health. His approach bridges computational methods (machine learning; geocomputation and spatial data analysis) and social research in the fields of social computing; urban geography and critical data studies. He is currently working on the development of new methods to generate and visualize spatial data on human settlements needed to model urban development; combining citizen participation with spatial data analysis; these methods will improve understanding of urban spatial patterns and intra-urban inequalities.; ,At the Turing; Dr João Porto de Albuquerque will be dedicated to advance interdisciplinary methods in sociospatial data science with applications to improving urban sustainable development; with a particular focus on urban resilience and health. His approach bridges computational methods (machine learning; geocomputation and spatial data analysis) and social research in the fields of social computing; urban geography and critical data studies. He is currently working on the development of new methods to generate and visualize spatial data on human settlements needed to model urban development; combining citizen participation with spatial data analysis; these methods will improve understanding of urban spatial patterns and intra-urban inequalities.; ,['N/A'],Dr Joao Porto de Albuquerque 
john-ainsworth,Data structures; Multi-agent systems; Robotics; Neural networks; Neuroscience; Systems theory; Communications; Databases; Parallel computing; Human computer interface; Information retrieval; Real time computing; Identity management; Software framework development; Cognitive science; Simulation; ,John Ainsworth is Professor of Health Informatics; University of Manchester and Director of the Farr Institute in the North of England. His research focuses on harnessing computing technology to enhance data science; applying data analytics to improve health services; and applying emerging computing technologies to create novel healthcare interventions. He is also the director of the Connected Health Cities programme; an initiative to unlock the power of information to transform health and social care services across the North of England.; He has a varied career with degrees in physics; cognitive science; health informatics and has worked in industry as a software engineer. John is involved in numerous research projects; but with a singular focus: to use computing and information technology to improve the health of the population.; There are three main themes to Ainsworth's research. The first is using information technology to support the scientific method. The process of deriving new knowledge from raw data is mediated by information technology; which means every step in the process can be captured digitally; shared and copied at minimal cost. It enables truly open and transparent science.; His second research theme is using information technology to enable co-production of health by patients and professionals. We live in an age where computing and connectivity are ubiquitous and pervasive. This presents new opportunities for generating high-resolution longitudinal data from patients in their everyday lives. How we transform this data into knowledge that can be used to make decisions about care by both patients and professionals is a research area that will transform healthcare.; The third theme is using information technology to optimise health systems. Currently we assume that healthcare evidence is time invariant; and applies equally to everyone. We treat diseases in isolation and produce evidence in the same way; limiting the possibility for combination and holistic treatment of the patient. Health systems need to be re-engineered to deliver timely; actionable information to the relevant actor.; ,There are three main themes to Ainsworth's research. The first is using information technology to support the scientific method. The process of deriving new knowledge from raw data is mediated by information technology; which means every step in the process can be captured digitally; shared and copied at minimal cost. It enables truly open and transparent science.; His second research theme is using information technology to enable co-production of health by patients and professionals. We live in an age where computing and connectivity are ubiquitous and pervasive. This presents new opportunities for generating high-resolution longitudinal data from patients in their everyday lives. How we transform this data into knowledge that can be used to make decisions about care by both patients and professionals is a research area that will transform healthcare.; The third theme is using information technology to optimise health systems. Currently we assume that healthcare evidence is time invariant; and applies equally to everyone. We treat diseases in isolation and produce evidence in the same way; limiting the possibility for combination and holistic treatment of the patient. Health systems need to be re-engineered to deliver timely; actionable information to the relevant actor.; ,['N/A'],Professor John Ainsworth 
john-pearson,Dynamical systems & differential equations; Numerical analysis; Operations research; Control theory; Neural networks; Pattern formation; Real time computing; Convex programming; Nonlinear programming; Stochastic optimisation; Simulation; ,Dr John Pearson was appointed to a Lectureship in the School of Mathematics at the University of Edinburgh in August 2017; having previously been a Lecturer in Mathematics at the University of Kent from April 2015 to July 2017. He held an EPSRC Fellowship on 'Fast Solvers for Real-World PDE-Constrained Optimization' from June 2015 to May 2018. From 2013-2015 Dr Pearson held a Whittaker Research Fellowship at Edinburgh; prior to this he was an EPSRC Doctoral Prize holder at the University of Oxford; having completed a DPhil in numerical analysis at Oxford in 2013.; Dr Pearson's research lies at the intersection of data science; operational research; optimization; and fast numerical algorithms; including highly parallelisable methods. A significant focus of his; in recent years; has been the rapid and robust solution of huge-scale matrix systems of equations arising from optimisation problems constrained by partial differential equations and; increasingly; stochastic partial differential equations. For these problems; one is required to solve systems which are highly influenced by and dependent on data which is generated by the behaviour of physical systems.; Examples of scientific applications Dr Pearson has examined are problems in computational and systems biology (pattern formation; chemotaxis; reaction-diffusion mechanisms); which require intensive computations based on large-scale (real-world) data being fed into the algorithms; and medical imaging problems; which require models and algorithms for processing data acquired from scans. A current programme of research he is undertaking concerns the rapid computational solution of optimal control problems under uncertainty; such formulations with stochastic inputs are used to quantify statistics of the response of a physical system.; Dr Pearson was awarded an EPSRC Fellowship in 2015; under the 'Mathematical Aspects of Operational Research' scheme. He was also awarded an IMA Leslie Fox Prize in Numerical Analysis (2nd Prize) in 2015; the University of Kent prize for Early Career Research in the Faculty of Sciences in 2016; a European Science Foundation grant for an extended research visit to the Max-Planck-Institut in Magdeburg; an EPSRC Doctoral Prize for his research as a DPhil student; a Whittaker Fellowship from the University of Edinburgh; three prizes for talks at conferences; and a Clothworker's scholarship (on two occasions) at Oxford.; ,Dr Pearson's research lies at the intersection of data science; operational research; optimization; and fast numerical algorithms; including highly parallelisable methods. A significant focus of his; in recent years; has been the rapid and robust solution of huge-scale matrix systems of equations arising from optimisation problems constrained by partial differential equations and; increasingly; stochastic partial differential equations. For these problems; one is required to solve systems which are highly influenced by and dependent on data which is generated by the behaviour of physical systems.; Examples of scientific applications Dr Pearson has examined are problems in computational and systems biology (pattern formation; chemotaxis; reaction-diffusion mechanisms); which require intensive computations based on large-scale (real-world) data being fed into the algorithms; and medical imaging problems; which require models and algorithms for processing data acquired from scans. A current programme of research he is undertaking concerns the rapid computational solution of optimal control problems under uncertainty; such formulations with stochastic inputs are used to quantify statistics of the response of a physical system.; ,Dr Pearson was awarded an EPSRC Fellowship in 2015; under the 'Mathematical Aspects of Operational Research' scheme. He was also awarded an IMA Leslie Fox Prize in Numerical Analysis (2nd Prize) in 2015; the University of Kent prize for Early Career Research in the Faculty of Sciences in 2016; a European Science Foundation grant for an extended research visit to the Max-Planck-Institut in Magdeburg; an EPSRC Doctoral Prize for his research as a DPhil student; a Whittaker Fellowship from the University of Edinburgh; three prizes for talks at conferences; and a Clothworker's scholarship (on two occasions) at Oxford.; ,Dr John Pearson 
john-robson,['N/A'],Dr John Robson is a Clinical Reader in Primary Care Research & Development & Turing Fellow based at Queen Mary University of London. ; John's research interests focus around cardiovascular disease and diabetes; equity and delivery of primary care; and improvement in disease management. His recent and ongoing research projects include: Described equity of provision for cardiovascular disease; diabetes and COPD. Taken up by local Primary Care Trusts; QDScore: descriptive data from east London; Funded by Primary Care Trusts; and reducing self-monitoring of blood glucose in type-2 diabetes: Improvement project evaluation 3 Primary Care Trusts.; ,['N/A'],['N/A'],Dr John Robson 
john-suckling,['N/A'],Dr Suckling is a physicist with over 25 years’ experience in medical imaging. His research career began at the Royal Marsden Hospital working on new hardware and software for positron emission tomography. After a short period at University College London developing teaching materials for the biomechanics of the hand and arm; John returned to brain imaging at the Institute of Psychiatry; Kings College London when the then new techniques of structural and functional neuroimaging were challenging long-held views on mental health disorders. He continues this work in the Department of Psychiatry; University of Cambridge; applying neuroimaging to investigate diverse psychopathological conditions.; Globally; psychiatric disorders are the leading cause of years lived with disability. Currently diagnostic structures rely on clustering clinically observed symptoms; and their evolution over time. This approach is not naturally aligned with underlying neurobiological systems; making it difficult to bring to bear advances in neuroscience. Data driven approaches to large datasets of symptoms have demonstrated a natural decomposition with a general vulnerability factor to mental health disorders; and sub-labels with prima facie validity. Similar; stable decompositions of imaging features would be significant evidence for a new; biological basis to psychiatric diagnosis.; John is a Turing Fellow through his co-supervision of Alexander Campbell; a doctoral student at the Turing.; ,Globally; psychiatric disorders are the leading cause of years lived with disability. Currently diagnostic structures rely on clustering clinically observed symptoms; and their evolution over time. This approach is not naturally aligned with underlying neurobiological systems; making it difficult to bring to bear advances in neuroscience. Data driven approaches to large datasets of symptoms have demonstrated a natural decomposition with a general vulnerability factor to mental health disorders; and sub-labels with prima facie validity. Similar; stable decompositions of imaging features would be significant evidence for a new; biological basis to psychiatric diagnosis.; John is a Turing Fellow through his co-supervision of Alexander Campbell; a doctoral student at the Turing.; ,['N/A'],Dr John Suckling 
jon-crowcroft,Data structures; Data science of government & politics; Research methods; Social networks; Social media; Communications; Parallel computing; Databases; Information retrieval; Operating systems; ,Jon Crowcroft has been the Marconi Professor of Communications Systems in the Computer Laboratory since October 2001. He has worked in the area of Internet support for multimedia communications for over 30 years. Three main topics of interest have been scalable multicast routing; practical approaches to traffic management; and the design of deployable end-to-end protocols. Current active research areas are Opportunistic Communications; Social Networks; and techniques and algorithms to scale infrastructure-free systems. He leans towards a “build and learn” paradigm for research.; He graduated in Physics from Trinity College; University of Cambridge in 1979; gained an MSc in Computing in 1981 and PhD in 1993; both from UCL. He is a Fellow the Royal Society; a Fellow of the ACM; a Fellow of the British Computer Society; a Fellow of the IET and the Royal Academy of Engineering and a Fellow of the IEEE.; He likes teaching; and has published a few books based on learning materials.; Jon spent the last two years as chair of the Institute's Programme Committee; where a significant ongoing task has been the mapping of strategy for the Turing. As the Turing continues to grow; in his new role as 'Researcher at Large'; Jon will continue in this task. With a remit to range over the wider set of activities at the Turing; he will be finding what the Institute does well and uncovering gaps where it needs to do more. The role will also involve helping with the Strategic Priorities Fund cross-cutting theme on tools; practices and systems.; Computing Systems at scale are the basis for much of the excitement over Data Science; but there are many challenges to continue to address ever larger amounts of data; but also to provide tools and techniques implemented in robust software; that are usable by statisticians and machine learning experts without themselves having to become experts in cloud computing. This vision of distributed computing only really works for “embarrassingly parallel” scenarios. The challenge for the research community is to build systems to support more complex models and algorithms that do not so easily partition into independent chunks; and to give answers in near real-time on a given size data centre efficiently.; Users want to integrate different tools (for example; R on Spark); don’t want to have to program for fault tolerance; yet as their tasks & data grow; will have to manage this; meanwhile; data science workloads don’t resemble traditional computer science batch or single-user interactive models. These systems put novel requirements on data centre networking operating systems; storage systems; databases; and programming languages and runtimes. As a communications systems researcher for 30 years; I am also interested in specific areas that involve networks; whether as technologies (the Internet; Transportation etc); or as observed phenomena (Social Media); or in abstract (graphs).; ,Computing Systems at scale are the basis for much of the excitement over Data Science; but there are many challenges to continue to address ever larger amounts of data; but also to provide tools and techniques implemented in robust software; that are usable by statisticians and machine learning experts without themselves having to become experts in cloud computing. This vision of distributed computing only really works for “embarrassingly parallel” scenarios. The challenge for the research community is to build systems to support more complex models and algorithms that do not so easily partition into independent chunks; and to give answers in near real-time on a given size data centre efficiently.; Users want to integrate different tools (for example; R on Spark); don’t want to have to program for fault tolerance; yet as their tasks & data grow; will have to manage this; meanwhile; data science workloads don’t resemble traditional computer science batch or single-user interactive models. These systems put novel requirements on data centre networking operating systems; storage systems; databases; and programming languages and runtimes. As a communications systems researcher for 30 years; I am also interested in specific areas that involve networks; whether as technologies (the Internet; Transportation etc); or as observed phenomena (Social Media); or in abstract (graphs).; ,['N/A'],Professor Jon Crowcroft 
jonathan-cave,Game theory; Privacy & trust; ,Jonathan Cave has been Senior Teaching Fellow in Economics at the University of Warwick since 1994. For more than 30 years; he also worked for the RAND Corporation most recently as Senior Research Fellow at RAND Europe. He has previously been a visiting professor; research fellow and lecturer at several universities in the US; including UC Los Angeles. Before entering academia; he was an economist at the Bank of England and later the US Federal Trade Commission. Jonathan is a Member of Defra’s Science Advisory Council Exotic Disease Subgroup. Jonathan holds no other public appointments and has not undertaken any party political activity.; Combined voting/economic games; regulatory design and evaluation; implementation and mechanism design; telecommunications and internet policy; governance of livestock disease; complexity in (esp.) healthcare systems; games played on networks; procurement and innovation; sustainable development; privacy; identity and trust; eGovernment.; ,Combined voting/economic games; regulatory design and evaluation; implementation and mechanism design; telecommunications and internet policy; governance of livestock disease; complexity in (esp.) healthcare systems; games played on networks; procurement and innovation; sustainable development; privacy; identity and trust; eGovernment.; ,['N/A'],Dr Jonathan Cave 
jonathan-phillips,Dynamical systems & differential equations; Neural networks; Neuroscience; Human computer interface; Graph theory; ,Dr J.J. Phillips joined The Living Systems Institute at the University of Exeter as a Senior Research Fellow in 2017. There he established a research group focused on protein functional switching and biochemical information processing. Prior to this; he previously held an industrial fellowship at MedImmune/AstraZeneca developing approaches to analyse protein dynamics. His postdoctoral research was in synthetic biology; using statistical mechanics models to rationally engineer protein structural switches and prior to this he completed a PhD in protein folding at the University of Cambridge.; Dr J.J. Phillips' main research interest is to understand how information is processed at a molecular level in natural; biomedical and artificial systems. He aims to understand and control the dynamic behaviour of protein molecules to perform biochemical computing.; ,Dr J.J. Phillips' main research interest is to understand how information is processed at a molecular level in natural; biomedical and artificial systems. He aims to understand and control the dynamic behaviour of protein molecules to perform biochemical computing.; ,['N/A'],Dr Jonathan Phillips 
jonathan-ward,Dynamical systems & differential equations; Multi-agent systems; Multi-agent reasoning; Nonlinear dynamics; Reinforcement learning; Graph theory; Uncertainty quantification; Simulation; ,Jonathan Ward is a Lecturer in Applied Mathematics at the University of Leeds. He studied Physics and Astrophysics as an undergraduate at the University of Bristol; UK; where he also completed his PhD on the mathematical modelling of traffic flow. He held post-doctoral positions at the University of Limerick; Ireland; and the University of Reading; UK; working in the fields of Network Science; Complex Systems and Industrial Applied Mathematics.; Jonathan is interested in developing mathematical methods for agent-based models; particularly those that model human behaviour. He plans to investigate how ideas from machine learning might be incorporated into agent-based models to improve their predictive power whilst retaining explanatory insight.; ,Jonathan is interested in developing mathematical methods for agent-based models; particularly those that model human behaviour. He plans to investigate how ideas from machine learning might be incorporated into agent-based models to improve their predictive power whilst retaining explanatory insight.; ,['N/A'],Dr Jonathan Ward 
jose-miguel,['N/A'],José Miguel is a University Lecturer in Machine Learning at the Department of Engineering in the University of Cambridge; UK. Before; he was a postdoc in the Harvard Intelligent Probabilistic Systems group at Harvard University; working with Ryan Adams; and a postdoc in the Machine Learning Group at the University of Cambridge (UK); working with Zoubin Ghahramani. Jose Miguel completed his Ph.D. and M.Phil. in Computer Science at the Computer Science Department in Universidad Autónoma de Madrid (Spain); where he also obtained a B.Sc. in Computer Science; with a special prize to the best academic record on graduation.; Part of José's research is in the application of machine learning to the efficient solution of expensive optimisation problems. For example; in optimal design in engineering; where the goal is to obtain better and more effective products. In many of these design problems the analytic form of the objective function is unknown and its evaluations are very expensive. Bayesian optimisation (BO) methods can reduce the number of evaluations required to solve the aforementioned problems. In his research José aims at designing novel methods for Bayesian optimisation that will accelerate optimal design problems across a wide range of engineering areas.; ,Part of José's research is in the application of machine learning to the efficient solution of expensive optimisation problems. For example; in optimal design in engineering; where the goal is to obtain better and more effective products. In many of these design problems the analytic form of the objective function is unknown and its evaluations are very expensive. Bayesian optimisation (BO) methods can reduce the number of evaluations required to solve the aforementioned problems. In his research José aims at designing novel methods for Bayesian optimisation that will accelerate optimal design problems across a wide range of engineering areas.; ,['N/A'],Dr Jose Miguel 
jose-vicente-rodriguez-mora,['N/A'],José Vicente Rodríguez Mora received his PhD from MIT. Currently he is a Professor of Economics at the University of Edinburgh. In the past he worked at Universitat Pompeu Fabra and Southampton. He also made long academic visits to Stockholm University; the University of Minnesota and the Minneapolis Fed. When not at work; his favourite thing to do is to ski down a mountain; preferably along with his children.; Professor Rodríguez Mora is interested in the theory and empirics of macroeconomics with a particular focus on aspects of what is sometimes called ‘social economics’; but by no means restricted to it. He has worked extensively on issues related to inheritance and social mobility. Their measurement; their causation and their effects on allocation of resources and macroeconomics. He has also worked on political economy; unemployment; the relationship between the financial sector and resource (mis)allocation; GDP measurement; international and inter-regional trade and their relationship to the costs of independence; and the economics of information.;  ; ,['N/A'],['N/A'],Professor Jose Vicente Rodríguez Mora 
joss-wright,['N/A'],Dr. Joss Wright gained his PhD in Computer Science at the University of York; where his work focused on the description and analysis of anonymous communication mechanisms. Following this; he spent time at the University of Siegen in Germany examining security and privacy issues in cloud computing.; Joss’ interests lie in the area of anonymous and censorship-resistant communications; cryptographic fundamentals and in the wider field of privacy enhancing technologies; their applications and their implications. At the OII; Joss is working on the “Being There” project; which looks at privacy in public spaces; and a Google-funded project analysing Internet censorship.; ,Joss’ interests lie in the area of anonymous and censorship-resistant communications; cryptographic fundamentals and in the wider field of privacy enhancing technologies; their applications and their implications. At the OII; Joss is working on the “Being There” project; which looks at privacy in public spaces; and a Google-funded project analysing Internet censorship.; ,['N/A'],Dr Joss Wright 
jotun-hein,Stochastic (Mathematical modelling); ,Jotun lived in Denmark until 1985; except for short periods in England; Austria and Italy. Then he had a very long series of postdoctoral positions in North Carolina; California; Montreal; Japan and a few other places. From 1991 to 2001 he was a lecturer at Aarhus University and in 2001 he moved to Oxford; where he has been ever since; except for sabbaticals in New Zealand; Berkeley and Chapel Hill.; Stochastic and algorithmic aspects of molecular evolution; origins of life and population genetics. Most of Jotun's interests involve molecular evolution and molecular population genetics.; ,Stochastic and algorithmic aspects of molecular evolution; origins of life and population genetics. Most of Jotun's interests involve molecular evolution and molecular population genetics.; ,['N/A'],Professor Jotun Hein 
judy-wajcman,Social data science; Data science of government & politics; ,Judy Wajcman is the Anthony Giddens Professor of Sociology at the London School of Economics (LSE). Prior to joining the LSE in 2009; she was the Professor of Sociology in the Research School of Social Sciences at the Australian National University. She has held posts in Cambridge; Edinburgh; Manchester; Sydney; Warwick; most recently holding the Mellon Fellowship at the Center for Advanced Study in the Behavioral Sciences at Stanford University (2017-18). ; She was President of the Society for Social Studies of Science (2009-2011) and is currently a Visiting Professor at the Oxford Internet Institute. In 2013; she was the recipient of the William F. Ogburn Career Achievement Award of the American Sociological Association. Professor Wajcman has an honorary doctorate from the University of Geneva; and she is a Fellow of the British Academy. In 2018; she received the Oxford Internet Institute’s Lifetime Achievement Award ‘in recognition of her contributions to the field of the social study of science and technology'. ; She is a member of the AI100 Standing Committee.; Judy has published widely in the fields of science and technology studies; feminist theory; work and organisations. She is probably best known for her analysis of the gendered nature of technology. Her books include The Social Shaping of Technology; Feminism Confronts Technology; Managing Like a Man: Women and Men in Corporate Management; TechnoFeminism; The Politics of Working Life; The Sociology of Speed; and Pressed for Time: The Acceleration of Life in Digital Capitalism; which was awarded the 2017 Ludwik Fleck prize by the Society for the Social Studies of Science. Her work has been translated into Chinese; French; German; Greek; Korean; Japanese; Portuguese; Spanish and Russian. Her current research is about the impact of digital technologies on the experience of time in everyday life.; ,Judy has published widely in the fields of science and technology studies; feminist theory; work and organisations. She is probably best known for her analysis of the gendered nature of technology. Her books include The Social Shaping of Technology; Feminism Confronts Technology; Managing Like a Man: Women and Men in Corporate Management; TechnoFeminism; The Politics of Working Life; The Sociology of Speed; and Pressed for Time: The Acceleration of Life in Digital Capitalism; which was awarded the 2017 Ludwik Fleck prize by the Society for the Social Studies of Science. Her work has been translated into Chinese; French; German; Greek; Korean; Japanese; Portuguese; Spanish and Russian. Her current research is about the impact of digital technologies on the experience of time in everyday life.; ,['N/A'],Professor Judy Wajcman 
julia-brettschneider,Applied mathematics; Operations research; Control theory; Machine learning; Applications (Machine learning); Ethics; Management science; High dimensional inference; Non-parametric & semi-parametric methods; Estimation theory; Modelling (Statistical methods & theory); ,Julia Brettschneider is associate professor of statistics at the University of Warwick. She obtained her PhD in Mathematics from Humboldt University Berlin working on probability theory before she moved to postdoc positions working with genomic data at Eurandom (Eindhoven; Netherlands) and at the University of California at Berkeley (USA). Before starting in Warwick; she worked as assistent professor in the Department of Math/Stats and the Department of Epidemiology at Queen's University in Kingston )Ontario; Canada). ; Firstly; Julia is working on a mathematically rigorous framework to simultaneously describe a normative probabilistic and an empirically based behavioural models for decision making. This is relevant to automatic decision making and to human-machine interactions. Applications include health care; agriculture and hiring processes.; Secondly; together with her colleague Professor Wilfrid Kendall; she has been developing quality assessment methods for X-ray detectors in CT machines based on spatial point process modelling of dead pixel distributions. Using some of her R-code; the Turing research software engineering groups has built the webtool DetectorChecker which provides a detector quality report a wide community of users. She is currently working on more advanced methods describing a variety of pixel states and their spatio-temporal evolution.; Thirdly; she is working on rigorous mathematical and statistical foundations for modelling localisation and colocalisation of proteins in confocal microscopic images and videos. Once formalised; these scores can be used in machine learning tasks; e.g. classification of large protein screening studies in drug development or disease detection from medical images.  ; ,Firstly; Julia is working on a mathematically rigorous framework to simultaneously describe a normative probabilistic and an empirically based behavioural models for decision making. This is relevant to automatic decision making and to human-machine interactions. Applications include health care; agriculture and hiring processes.; Secondly; together with her colleague Professor Wilfrid Kendall; she has been developing quality assessment methods for X-ray detectors in CT machines based on spatial point process modelling of dead pixel distributions. Using some of her R-code; the Turing research software engineering groups has built the webtool DetectorChecker which provides a detector quality report a wide community of users. She is currently working on more advanced methods describing a variety of pixel states and their spatio-temporal evolution.; Thirdly; she is working on rigorous mathematical and statistical foundations for modelling localisation and colocalisation of proteins in confocal microscopic images and videos. Once formalised; these scores can be used in machine learning tasks; e.g. classification of large protein screening studies in drug development or disease detection from medical images.  ; ,['N/A'],Dr Julia Brettschneider 
julia-handl,Multi-agent systems; Operations research; Multi-agent reasoning; Nonlinear programming; Management science; Monte Carlo methods; Simulation; Time series; ,Julia Handl obtained a Bsc (Hons) in Computer Science from Monash University in 2001; an MSc degree in Computer Science from the University of Erlangen-Nuremberg in 2003; and a PhD in Bioinformatics from the University of Manchester in 2006. From 2007 to 2011; she held an MRC Special Training Fellowship at the University of Manchester and the University of Washington; and she is now a Senior Lecturer (Associate Professor) in the Decision and Cognitive Sciences Research Centre at the Alliance Manchester Business School. Her research includes theoretical and empirical work related to the development and use of data-mining and optimisation approaches in applications ranging from protein structure prediction over market segmentation to healthcare settings.; Julia's research focuses on the interface between data science and optimisation; with an emphasis on the design of smarter; data-driven meta-heuristic optimisers. Traditionally a data-poor discipline; the appropriate integration of available data into non-linear global optimisation techniques (specifically meta-heuristics) is an increasingly pressing problem. The problem-specific customisation of meta-heuristic optimisers is an essential step for focusing the search and achieving scalability of standard of-the-shelf techniques to large-scale industrially relevant problems. Historically; this would have been achieved through the careful manual; design of various meta-heuristic components by an optimisation expert.; The automatic integration of data offers an alternative approach and promises the opportunity to move beyond the capabilities of a human in identifying and capturing complex relationships; and integrating this information fully into the search process. Relevant data in an optimisation setting may arise in a variety of forms including e.g. historical data capturing observed co-variation between pairs of decision variables; data describing previously seen distributions of individual variables; or collections of candidate solutions for simplified or different instances of the problem.; Julia's research will deliver novel methods for data-driven optimisation; and thereby assist the Institute's agenda of driving advances in fundamental data science methodology. The development of these approaches is intrinsically linked to applications; as the nature of the data that can be exploited during optimisation will vary by problem. Understanding and fully defining generalisable classes of data features that can support data-driven optimisation; and designing suitable methodologies for each of these classes; is therefore key. The value of the resulting; general methods will be illustrated using selected applications. Applications of data-driven optimisation arise across a range of challenge areas; especially science; engineering and healthcare.; ,Julia's research focuses on the interface between data science and optimisation; with an emphasis on the design of smarter; data-driven meta-heuristic optimisers. Traditionally a data-poor discipline; the appropriate integration of available data into non-linear global optimisation techniques (specifically meta-heuristics) is an increasingly pressing problem. The problem-specific customisation of meta-heuristic optimisers is an essential step for focusing the search and achieving scalability of standard of-the-shelf techniques to large-scale industrially relevant problems. Historically; this would have been achieved through the careful manual; design of various meta-heuristic components by an optimisation expert.; The automatic integration of data offers an alternative approach and promises the opportunity to move beyond the capabilities of a human in identifying and capturing complex relationships; and integrating this information fully into the search process. Relevant data in an optimisation setting may arise in a variety of forms including e.g. historical data capturing observed co-variation between pairs of decision variables; data describing previously seen distributions of individual variables; or collections of candidate solutions for simplified or different instances of the problem.; Julia's research will deliver novel methods for data-driven optimisation; and thereby assist the Institute's agenda of driving advances in fundamental data science methodology. The development of these approaches is intrinsically linked to applications; as the nature of the data that can be exploited during optimisation will vary by problem. Understanding and fully defining generalisable classes of data features that can support data-driven optimisation; and designing suitable methodologies for each of these classes; is therefore key. The value of the resulting; general methods will be illustrated using selected applications. Applications of data-driven optimisation arise across a range of challenge areas; especially science; engineering and healthcare.; ,['N/A'],Dr Julia Handl 
julian-hough,Multi-agent systems; Robotics; Neural networks; Human computer interface; Deep learning; Natural language processing; Probabilistic programming; Cognitive science; Linguistics; Time series; Probability; ,Julian Hough is a Lecturer in the Cognitive Science group in the School of Electronic Engineering and Computer Science at Queen Mary University of London (QMUL). Hough received his PhD at QMUL before working at Bielefeld University as a post-doc. Hough researches dialogue modelling and dialogue systems with a focus on incremental processing and disfluency.; Julian Hough is a researcher who builds computational models of human communication with natural language and physical action from human-human data; and then implements them in artificial systems to make interaction more natural. Hough makes the claim that anyone who is afraid that Artificial Intelligence is a done deal; the Turing Test is solved and that the machines are coming for us is welcome to interact with state-of-the-art systems which interpret and generate speech and gesture- they will probably walk away rather relieved; and with more understanding as to why research into this area is still required.; Hough's particular focus is on incremental processing and interactive responsiveness. This includes analyzing the units of meaning in spoken conversation and making machines process using these same units; looking at how people 'make mistakes' in communication; and how best machines can recover from these situations.; ,Julian Hough is a researcher who builds computational models of human communication with natural language and physical action from human-human data; and then implements them in artificial systems to make interaction more natural. Hough makes the claim that anyone who is afraid that Artificial Intelligence is a done deal; the Turing Test is solved and that the machines are coming for us is welcome to interact with state-of-the-art systems which interpret and generate speech and gesture- they will probably walk away rather relieved; and with more understanding as to why research into this area is still required.; Hough's particular focus is on incremental processing and interactive responsiveness. This includes analyzing the units of meaning in spoken conversation and making machines process using these same units; looking at how people 'make mistakes' in communication; and how best machines can recover from these situations.; ,['N/A'],Dr Julian Hough 
jun-chen,Multi-agent systems; Operations research; Robotics; Control theory; Neural networks; Systems theory; Human computer interface; Neural & evolutionary computing; Graph theory; Stochastic optimisation; ,Dr Jun Chen is now a Lecturer in Engineering Science at QMUL. He received his PhD degree from the University of Sheffield. He has published more than 50 scientific papers in areas of multi-objective optimisation; interpretable fuzzy systems; data-driven modelling; and intelligent transportation systems. Dr Chen was among the first researchers to investigate the trade-off between taxi time and fuel consumption in airport ground movements (EP/H004424/2); and proposed the Active Routing (AR) concept. AR forms the cornerstone of a major ongoing EPSRC funded project (EP/N029496/1; EP/N029356/1 and EP/N029577/1; in total in excess of £1M) for which Dr Chen is the lead PI (with BAEs; AirFrance-KLM; Rolls Royce; Manchester and Zurich Airports; and Simio plc.).; He has also been the PI on four industrial projects with Anglian Water; and was the CI on three Innovate UK projects (with IMS and Tesco plc; and Siemens). In 2018; he becomes a Turing Fellow. Dr Chen's research interests lie at the interface of Computer Science; Operational Research and Control Engineering. Over the years; Dr Chen has consolidated and structured his research along three themes: modelling; optimisation and decision; four cross-cutting application areas: intelligent decision support for airport operations; human factors & brain-computer interface; future flight deck automation systems and intelligent sewer networks; sponsored by EPSRC and industry and supported by the well-balanced research team consisting of senior academics; post-doctoral researchers and PhD students.; We live in a mobile and networked era; with people; goods; services and information moving on unprecedented scales. Airline passengers are forecast to double; to six billion in 2030 compared with 3.1 billion in 2013. By that time; many airports will reach their maximum capacity resulting in a pressure to fully utilise the available resources. By 2030; container traffic volume is forecast to be 183% higher than in 2005 and the number of HGVs to have doubled over the same period; raising safety; congestion and environmental concerns. On Black Friday; 2014; Amazon UK website recorded orders for more than 5.5 million goods; with an average of 64 items sold online per second; imposing great pressure on delivery and information transfer systems. In the face of this fiercely increasing trend; the incentive for more efficient; cost effective and environmentally sustainable ways of movement is evident.; We must have scalable and flexible multi-objective routing and scheduling systems to safely and efficiently coordinate movements within complex and dynamic environments. Dr Chen's research will focus on developing scalable and flexible multi-objective routing and scheduling methods; addressing a range of complex transportation and logistics problems. This project emerged from the Fellow's previous study in routing and scheduling for multi agents.; Initial results reveal a type of routing and scheduling problem that requires multiple options to be available on the same route and cooperation across interconnected sub-problems. Each of these options represents the most appropriate solution at a particular time within dynamic and uncertain environments. Overall performance is improved by integrating sub-problems. This type of routing and scheduling problem can be formulated in a densely connected multi-objective multigraph; presenting a new and exciting challenge to existing automated search methodologies due to the enlarged search space.; ,We live in a mobile and networked era; with people; goods; services and information moving on unprecedented scales. Airline passengers are forecast to double; to six billion in 2030 compared with 3.1 billion in 2013. By that time; many airports will reach their maximum capacity resulting in a pressure to fully utilise the available resources. By 2030; container traffic volume is forecast to be 183% higher than in 2005 and the number of HGVs to have doubled over the same period; raising safety; congestion and environmental concerns. On Black Friday; 2014; Amazon UK website recorded orders for more than 5.5 million goods; with an average of 64 items sold online per second; imposing great pressure on delivery and information transfer systems. In the face of this fiercely increasing trend; the incentive for more efficient; cost effective and environmentally sustainable ways of movement is evident.; We must have scalable and flexible multi-objective routing and scheduling systems to safely and efficiently coordinate movements within complex and dynamic environments. Dr Chen's research will focus on developing scalable and flexible multi-objective routing and scheduling methods; addressing a range of complex transportation and logistics problems. This project emerged from the Fellow's previous study in routing and scheduling for multi agents.; Initial results reveal a type of routing and scheduling problem that requires multiple options to be available on the same route and cooperation across interconnected sub-problems. Each of these options represents the most appropriate solution at a particular time within dynamic and uncertain environments. Overall performance is improved by integrating sub-problems. This type of routing and scheduling problem can be formulated in a densely connected multi-objective multigraph; presenting a new and exciting challenge to existing automated search methodologies due to the enlarged search space.; ,['N/A'],Dr Jun Chen 
jun-wang,Multi-agent systems; Artificial intelligence; Information retrieval; Machine learning; Deep learning; Reinforcement learning; ,Jun Wang is Professor; Computer Science; University College London; and Founding Director of MSc Web Science and Big Data Analytics. He is also Co-founder and Chief Scientist in MediaGamma Ltd; a UCL start-up company focusing on AI for intelligent audience decision making.; Prof. Jun Wang’s main research interests are in the areas of AI and intelligent systems; including (multiagent) reinforcement learning; deep generative models; and their diverse applications on information retrieval; recommender systems and personalization; data mining; smart cities; bot planning; computational advertising etc. His team won the first global real-time bidding algorithm contest with 80+ participants worldwide. Jun has published over 100 research papers and is a winner of multiple “Best Paper” awards. He was a recipient of the Beyond Search – Semantic Computing and Internet Economics award by Microsoft Research and also received Yahoo! FREP Faculty award. He has served as an Area Chair in ACM CIKM and ACM SIGIR. His recent service includes co-chair of Artificial Intelligence; Semantics; and Dialog in ACM SIGIR 2018. MediaGamma has received the UCLB One-to-Watch award 2016.; He has more than 15 years track records of advising the industry and knowledge transfer. He was a technical advisor for startups such as Last.Fm; Passiv Systems; Massive Analytic; Context Scout; and Polecat; and had various projects with BT; Microsoft; Yahoo!; Alibaba; Didi etc.; Prof. Wang obtained his PhD degree in Delft University of Technology; the Netherlands; MSc degree in National University of Singapore; Singapore; and Bachelor degree in Southeast University; Nanjing; China.; AI and Machine Learning; (Multi-agent) Reinforcement Learning and Control; and Neural Generative Models;; Statistical Modeling of Information Retrieval; and Dynamic Information Retrieval;; Data Mining; Personalization; and Collaborative Filtering (Recommender Systems);; Computational Advertising and Real-time Bidding.; ,AI and Machine Learning; (Multi-agent) Reinforcement Learning and Control; and Neural Generative Models;; Statistical Modeling of Information Retrieval; and Dynamic Information Retrieval;; Data Mining; Personalization; and Collaborative Filtering (Recommender Systems);; Computational Advertising and Real-time Bidding.; ,['N/A'],Professor Jun Wang 
karyn-morrissey,Data structures; Databases; Visualisation (Computer systems & architectures); Research methods; Causality; Simulation; Modelling (Statistical methods & theory); Probability; ,Dr Karyn Morrissey is a Senior Lecturer whose research focuses on understanding the impact of socio-economic and environmental inequalities on health outcomes; using data both big and small. An economist by background her multi-disciplinary approach centres on the application of computational methodologies such as simulation models; econometric models and geo-computation models.; Dr Karyn Morrissey is a Senior Lecturer whose research focuses on understanding the impact of socio-economic and environmental inequalities on health outcomes; using data both big and small. An economist by background her multi-disciplinary approach centres on the application of computational methodologies such as simulation models; econometric models and geo-computation models.; ,Dr Karyn Morrissey is a Senior Lecturer whose research focuses on understanding the impact of socio-economic and environmental inequalities on health outcomes; using data both big and small. An economist by background her multi-disciplinary approach centres on the application of computational methodologies such as simulation models; econometric models and geo-computation models.; ,['N/A'],Dr Karyn Morrissey 
kaspar-althoefer,Robotics; Neural networks; Human computer interface; Real time computing; Computer vision; Deep learning; Pattern recognition; Reinforcement learning; ,Professor Althoefer is an experienced roboticist leading competitively-funded research on soft robotics; intelligent micro-sensing systems and interaction dynamics modelling with applications in minimally invasive surgery; assistive technologies and human-robot interaction at Queen Mary University of London; acquired more than £5.5M as PI from national/international funding bodies and successfully completed 21 PhD projects.; Professor Althoefer's research team currently comprising 10 postdoctoral research associates and PhD students is involved in funded collaborative research with leading London hospitals and European research organisations creating novel robot-assisted solutions for ergonomically-optimised human-robot interaction; intelligent manipulation based on embedded force and tactile sensing and novel stiffness-controllable robot structures.; Over the last decade; the team has built a large portfolio of projects in application-oriented research for the healthcare and manufacturing sectors with funding from organisations such as EPSRC; European Commission (including coordination of two EU-projects); Wellcome Trust and UK-based charities; exceeding £30M and producing more than 250 peer-reviewed papers.; The field of human-robot interaction (HRI) has seen a dramatic development over the last decade; with a wide range of researchers moving the field forward considerable. HRI has gained in importance with an observed need for robots that can operate in the vicinity of humans or even get in contact with them. Soft material robotics has a lot to offer in this context. Because of their compliance; robots made from soft materials are considered much safer than their rigid component counterparts.; There is a general consensus that pneumatically or hydraulically actuated robot arms made from soft materials have low computational requirements concerning the computation of the interaction forces when in contact with their environment. Their inherent compliance makes them suitable especially for tasks that require the handling and manipulation of fragile objects. The robot arms adapt and conform to the objects that they are in contact with and the interaction forces can be determined as an easy-to-compute function of the pressures in the robot's actuation chambers.; This simplification in the way objects are handled owing to the robot's structure and material properties is considered an example of morphological computation. Due to the highly nonlinear behaviour of the materials used; such as silicone rubber or tailored textiles stretched by compressed air; the relationship between the robot's pose and the control commands (actuator pressures) are difficult to describe using analytical models. Employing deep learning methods; the relationship between the control commands and the pose of the robot structure in the context of a possibly highly dynamic scenario can be determined.; ,The field of human-robot interaction (HRI) has seen a dramatic development over the last decade; with a wide range of researchers moving the field forward considerable. HRI has gained in importance with an observed need for robots that can operate in the vicinity of humans or even get in contact with them. Soft material robotics has a lot to offer in this context. Because of their compliance; robots made from soft materials are considered much safer than their rigid component counterparts.; There is a general consensus that pneumatically or hydraulically actuated robot arms made from soft materials have low computational requirements concerning the computation of the interaction forces when in contact with their environment. Their inherent compliance makes them suitable especially for tasks that require the handling and manipulation of fragile objects. The robot arms adapt and conform to the objects that they are in contact with and the interaction forces can be determined as an easy-to-compute function of the pressures in the robot's actuation chambers.; This simplification in the way objects are handled owing to the robot's structure and material properties is considered an example of morphological computation. Due to the highly nonlinear behaviour of the materials used; such as silicone rubber or tailored textiles stretched by compressed air; the relationship between the robot's pose and the control commands (actuator pressures) are difficult to describe using analytical models. Employing deep learning methods; the relationship between the control commands and the pose of the robot structure in the context of a possibly highly dynamic scenario can be determined.; ,['N/A'],Professor Kaspar Althoefer 
katharine-robson-brown,['N/A'],Kate Robson Brown is Professor of Biological Anthropology within the Faculty of Arts and the Faculty of Engineering; and is Turing University Lead for Bristol. She obtained her PhD in phylogenetics from the University of Cambridge (Newnham College; 1995); and held the Graham Robertson Research Fellowship at Downing College. She is the Director of the Jean Golding Institute for Data Science and Data Intensive Research at the University of Bristol.; Kate is also Co-Chair of the Institute's Research and Innovation Advisory Committee; which steers scientific direction at the Turing.; Kate’s research explores the microstructure of living tissues and their response to changing and extreme environments; innovating methodologies for the capture; computational modelling; analysis and interpretation of data describing complex material and structural characterisation. Biological anthropological applications of these methods include forensic identification; the regulation of hard tissue growth and development; and the study of biomechanical systems in extinct species. Engineering applications include employing the ontogeny of tissue microstructure as a model of programmed transformation in 4D materials; biomimetics in engineering design; and multi-scale modelling of complex hierarchical structures and systems.; ,Kate’s research explores the microstructure of living tissues and their response to changing and extreme environments; innovating methodologies for the capture; computational modelling; analysis and interpretation of data describing complex material and structural characterisation. Biological anthropological applications of these methods include forensic identification; the regulation of hard tissue growth and development; and the study of biomechanical systems in extinct species. Engineering applications include employing the ontogeny of tissue microstructure as a model of programmed transformation in 4D materials; biomimetics in engineering design; and multi-scale modelling of complex hierarchical structures and systems.; ,['N/A'],Professor Katharine Robson Brown 
kathrin-glau,Dynamical systems & differential equations; Numerical analysis; Operations research; Robotics; Operating systems; Real time computing; Stochastic optimisation; Data science of government & politics; Ethics; Social media; High dimensional inference; Monte Carlo methods; Simulation; Probability; ,Kathrin Glau currently is a Lecturer in Financial Mathematics at Queen Mary University of London and a Fellow at EPFL (Ecole Polythechnique Federale Lausanne) co-funded by Marie Skłodowska-Curie. Between 2011 and 2017 she was Junior Professor at the Technical University of Munich. Prior to this she worked as a postdoctoral university assistant at the chair of Professor Walter Schachermayer at the University of Vienna. In September 2010 she completed her doctoral thesis on the topic of Feynman-Kac representations for option pricing in Lévy models at the chair of Ernst Eberlein.; Kathrin's research in computational finance reaches across the borders of finance; stochastic analysis and numerical analysis. At the core of her current research is the design and implementation of complexity reduction techniques for finance. Key to her approach is the decomposition of algorithms in an offline phase; which is a learning step; and a fast and accurate online phase. The methods range from model order reduction of parametric partial differential equations to learning algorithms and are designed to facilitate such diverse tasks as uncertainty quantification and calibration; real-time pricing; real-time risk monitoring; and intra-day stress testing. For example she developed a pricing algorithm based on Magic Point Integration.; TUM Junior Fellow 2011-2017 EPFL Fellow co-funded by Marie Sk?odowska-Curie 2018-2019; ,Kathrin's research in computational finance reaches across the borders of finance; stochastic analysis and numerical analysis. At the core of her current research is the design and implementation of complexity reduction techniques for finance. Key to her approach is the decomposition of algorithms in an offline phase; which is a learning step; and a fast and accurate online phase. The methods range from model order reduction of parametric partial differential equations to learning algorithms and are designed to facilitate such diverse tasks as uncertainty quantification and calibration; real-time pricing; real-time risk monitoring; and intra-day stress testing. For example she developed a pricing algorithm based on Magic Point Integration.; ,TUM Junior Fellow 2011-2017 EPFL Fellow co-funded by Marie Sk?odowska-Curie 2018-2019; ,Dr Kathrin Glau 
kenneth-heafield,['N/A'],Kenneth Heafield is a Lecturer in the School of Informatics at the University of Edinburgh where he leads a machine translation group. He wrote the KenLM library for efficient n-gram language modeling and now works to make neural machine translation faster and higher-quality.  He holds a PhD from Carnegie Mellon's School of Computer Science and did a postdoc at Stanford.; Kenneth combines natural language processing and systems to make and scale models for machine translation.  Neural networks have improved translation quality and their ability to learn; or not learn; natural language phenomena is an open research area.  However; the high computational cost of training neural network models has slowed experimentation and has forced researchers to decrease training data sizes; sometimes by three orders of magnitude.  Through the Alan Turing Institute; I am collaborating with Intel to accelerate neural networks and challenge the HPC community with real-world natural language processing tasks.; ,Kenneth combines natural language processing and systems to make and scale models for machine translation.  Neural networks have improved translation quality and their ability to learn; or not learn; natural language phenomena is an open research area.  However; the high computational cost of training neural network models has slowed experimentation and has forced researchers to decrease training data sizes; sometimes by three orders of magnitude.  Through the Alan Turing Institute; I am collaborating with Intel to accelerate neural networks and challenge the HPC community with real-world natural language processing tasks.; ,['N/A'],Dr Kenneth Heafield 
kirstie-whitaker,['N/A'],Kirstie discovered the wonder of brain imaging at the University of British Columbia during a masters degree in Medical Physics. She completed a PhD in Neuroscience at the University of California; Berkeley in 2012 and joins the Turing Institute from a postdoctoral fellowship at the University of Cambridge in the Department of Psychiatry. She is an Fulbright scholarship alumna and 2016/17 Mozilla Fellow for Science. Kirstie was named; with her collaborator Petra Vertes; as a 2016 Global Thinker by Foreign Policy magazine.; Adolescence is a period of human brain growth and high incidence of mental health disorders. Kirstie's research uses magnetic resonance images to understand changes in the brain's structure and function that underlie the emergence of schizophrenia and depression. Her work considers the brain as a network and investigates how different brain regions work together. She is particularly passionate about ensuring that work is reproducible and can be replicated in independent data sets. Her focus at the Turing Institute is to improve the generalisability of research findings so they may be translated to the clinic and used by public health policy makers.; ,Adolescence is a period of human brain growth and high incidence of mental health disorders. Kirstie's research uses magnetic resonance images to understand changes in the brain's structure and function that underlie the emergence of schizophrenia and depression. Her work considers the brain as a network and investigates how different brain regions work together. She is particularly passionate about ensuring that work is reproducible and can be replicated in independent data sets. Her focus at the Turing Institute is to improve the generalisability of research findings so they may be translated to the clinic and used by public health policy makers.; ,['N/A'],Dr Kirstie Whitaker 
kody-law,Numerical (Algorithms); Numerical analysis; Deep learning; Supervised learning; Unsupervised learning; Stochastic (Mathematical modelling); Stochastic optimisation; High dimensional inference; Monte Carlo methods; Non-parametric & semi-parametric methods; ,Kody J.H. Law is a professor of applied mathematics at the School of Mathematics at the University of Manchester; specialising in computational applied mathematics. He received his PhD in mathematics in 2010 from the University of Massachusetts; Amherst; and subsequently held positions as a postdoc at the University of Warwick; and senior mathematician at King Abdullah University of Science and Technology and Oak Ridge National Laboratory.; He has published in the areas of computational applied mathematics; physics; and dynamical systems. His current research interests are focused on the fertile intersection of mathematics and statistics; in particular; inverse uncertainty quantification: data assimilation; filtering; and Bayesian inverse problems.; Kody's current research interest lies in the intersection between mathematics and statistics; enabled by computation; and driven by applications.; Coming from a background in computational applied mathematics and partial differential equations; his data endeavours began with inverse problems and data assimilation; both from a classical perspective as well as a probabilistic Bayesian perspective. In this context; data is used to infer the parameters and state of a model which has been derived on first principles; for example to describe a physical system. Prototypical examples are meteorology and subsurface geophysics; where the objective might be either weather prediction or contaminant source inversion and transport; respectively.; Current focal topics are (i) the design and application of stochastic simulation algorithms for solving these types of problems; based on principles of integrated numerical and statistical analysis; and (ii) the design and application of purely data-driven algorithms for inferring surrogate models (machines) in science and engineering applications. ; ,Kody's current research interest lies in the intersection between mathematics and statistics; enabled by computation; and driven by applications.; Coming from a background in computational applied mathematics and partial differential equations; his data endeavours began with inverse problems and data assimilation; both from a classical perspective as well as a probabilistic Bayesian perspective. In this context; data is used to infer the parameters and state of a model which has been derived on first principles; for example to describe a physical system. Prototypical examples are meteorology and subsurface geophysics; where the objective might be either weather prediction or contaminant source inversion and transport; respectively.; Current focal topics are (i) the design and application of stochastic simulation algorithms for solving these types of problems; based on principles of integrated numerical and statistical analysis; and (ii) the design and application of purely data-driven algorithms for inferring surrogate models (machines) in science and engineering applications. ; ,['N/A'],Professor Kody Law 
korbinian-strimmer,Machine learning; Statistical methods & theory; High dimensional inference; Information theory (Statistical methods & theory); ,"Korbinian Strimmer has been Professor of Statistics at the University of Manchester since 2017. Before he was Reader in Biostatistics at Imperial College London; Associate Professor in Medical Statistics and Bioinformatics at the University of Leipzig; and Emmy Noether Group Leader in Statistics at the University of Munich.;  ; Korbinian Strimmer is interested in complex statistical models and methods at interface of statistics; data science and machine learning; with application to high-dimensional data analysis in biology and medicine. He is a strong advocate of open source software and reproducible science. Correspondingly; he and his collaborators have published numerous biostatistics and bioinformatics software many of which are widely used.; Korbinian Strimmer has been included four times consecutively (in 2014; 2015; 2016 and 2017) in the ISI list of ""Highly Cited Researchers"" in the field of ""Computer Science"".; ",Korbinian Strimmer is interested in complex statistical models and methods at interface of statistics; data science and machine learning; with application to high-dimensional data analysis in biology and medicine. He is a strong advocate of open source software and reproducible science. Correspondingly; he and his collaborators have published numerous biostatistics and bioinformatics software many of which are widely used.; ,"Korbinian Strimmer has been included four times consecutively (in 2014; 2015; 2016 and 2017) in the ISI list of ""Highly Cited Researchers"" in the field of ""Computer Science"".; ",Professor Korbinian Strimmer 
kostas-zygalakis,['N/A'],He received his PhD in computational stochastic differential equations from University of Warwick at 2009 and held postdoctoral positions at the Universities of Cambridge; Oxford and the Swiss Federal Institute of Technology; Lausanne. In 2011 he was awarded a Leslie Fox Prize (IMA UK) in numerical analysis. Before joining Edinburgh as a lecturer in the mathematics of Data Science in 2016; he held a lectureship in Applied Mathematics at the University of Southampton.  ; His research is on the interplay between numerical analysis and computational statistics. In particular; he is interested in the properties of long time approximation to stochastic differential equations and their connection to sampling algorithms. Furthermore; he has recently started working on problems related to Data Assimilation and Bayesian inverse problems; as well as applications of ideas of numerical analysis to machine learning algorithms.; ,His research is on the interplay between numerical analysis and computational statistics. In particular; he is interested in the properties of long time approximation to stochastic differential equations and their connection to sampling algorithms. Furthermore; he has recently started working on problems related to Data Assimilation and Bayesian inverse problems; as well as applications of ideas of numerical analysis to machine learning algorithms.; ,['N/A'],Dr Kostas Zygalakis 
krasimira-tsaneva-atanasova,Numerical (Algorithms); Dynamical systems & differential equations; Numerical analysis; Applications (Machine learning); Deterministic (Mathematical modelling); Dynamic/static (Mathematical modelling); ,Krasimira Tsaneva-Atanasove earned her undergraduate and MSc degrees in mathematics at the University of Plovdiv; Bulgaria from 1991 until 1996. In September 2001 she started a PhD in applied mathematics at the University of Auckland; New Zealand. After completing her PhD in October 2004 she spent 18 months as a post-doctoral fellow at the Laboratory of Biological Modelling; National Institutes of Health; USA and another 15 months as a post-doctoral fellow at the Department of Mathematics and the Department of Biology at Ecole Normale Superieure in Paris; France. Krasimira joined the Department of Engineering Mathematics at the University of Bristol in October 2007 as a lecturer and was promoted to a Reader in Applied Mathematics in 2012. She moved to the College of Engineering; Mathematics and Physical Sciences; University of Exeter in July 2013 where she is currently a Professor of Mathematics for Healthcare.; As part of her research Krasimira focuses on novel applications of mathematics to enable the development of quantitative methods for healthcare and healthcare technologies. She is passionate about projects that could make a difference to experimental scientists and clinicians; and potentially society through the applications of mathematics for personalised prediction and decision in prevention; diagnosis or treatment of health-related conditions. She has developed and analysed various mathematical models in order to study certain aspects of cellular signal-transduction pathways such as calcium dynamics; hormone signalling and neuronal excitability.; More recently she has also engaged in collaborative projects in the area of movement science; experimental psychology and healthcare technologies. She has a long-standing interest in applied dynamical systems theory; numerical continuation; scientific computing and data-driven modelling. As a Turing Fellow; Krasimira hopes to leverage on her experience in collaborative projects and consolidate her research activities in data analytics and its applications to healthcare.; An example would be her pilot work on developing virtual reality-based diagnostics tools for mental health disorders; which has been tested in France. In order to successfully bring this to the UK she needs to engage with relevant software engineers and industrial partners across the UK and expand further her expertise in advanced statistical and machine learning techniques. The Turing provides an exceptional pool of experts and available contacts to enable the successful implementation of the above-mentioned ideas.; ,As part of her research Krasimira focuses on novel applications of mathematics to enable the development of quantitative methods for healthcare and healthcare technologies. She is passionate about projects that could make a difference to experimental scientists and clinicians; and potentially society through the applications of mathematics for personalised prediction and decision in prevention; diagnosis or treatment of health-related conditions. She has developed and analysed various mathematical models in order to study certain aspects of cellular signal-transduction pathways such as calcium dynamics; hormone signalling and neuronal excitability.; More recently she has also engaged in collaborative projects in the area of movement science; experimental psychology and healthcare technologies. She has a long-standing interest in applied dynamical systems theory; numerical continuation; scientific computing and data-driven modelling. As a Turing Fellow; Krasimira hopes to leverage on her experience in collaborative projects and consolidate her research activities in data analytics and its applications to healthcare.; An example would be her pilot work on developing virtual reality-based diagnostics tools for mental health disorders; which has been tested in France. In order to successfully bring this to the UK she needs to engage with relevant software engineers and industrial partners across the UK and expand further her expertise in advanced statistical and machine learning techniques. The Turing provides an exceptional pool of experts and available contacts to enable the successful implementation of the above-mentioned ideas.; ,['N/A'],Professor Krasimira Tsaneva-Atanasova 
krys-latuszynski,['N/A'],Krys Latuszynski is a Royal Society University Research Fellow and Jeff Harrison Early Career Professor of Statistics at the University of Warwick. Prior to this; he was a Research Fellow at the Universities of Warwick and Toronto. He graduated from the University of Warsaw (BSc; MSc; PhD in mathematics) and Warsaw School of Economics (BSc; MSc in econometrics).;  ; His research is in the area of computational statistics with particular focus on design and analysis of Markov chain Monte Carlo and related algorithms for high dimensional problems and unbiased inference for intractable likelihood problems; in particular diffusion process.; ,His research is in the area of computational statistics with particular focus on design and analysis of Markov chain Monte Carlo and related algorithms for high dimensional problems and unbiased inference for intractable likelihood problems; in particular diffusion process.; ,['N/A'],Dr Krys Latuszynski 
lars-andersen-bratholm,Neural networks; Applications (Machine learning); Deep learning; Software framework development; High dimensional inference; Simulation; Time series; Modelling (Statistical methods & theory); ,"Lars Bratholm is a Post-Doctoral Research Associate joint between the Mathematics and Chemistry department at the University of Bristol since 2017. He works on developing and applying machine learning models to predict molecular properties in an attempt to elucidate the mechanics behind observed experimental data. His PhD in Chemistry was obtained at the University of Copenhagen in 2016.; While machine-learning of energies of molecules is now a well established field; little work has been done on using machine-learning to predict how a molecule will evolve over time. In collaboration with The Alan Turing Institute; Lars will aim to open up a new research field coined ""Phase-space machine-learning"" that tries to address this. To kick-start interest in the field; a data-science competition will be hosted on Kaggle; where a prize will be offered to the modelers that are best able to predict how a set of molecular systems evolve in time.; ","While machine-learning of energies of molecules is now a well established field; little work has been done on using machine-learning to predict how a molecule will evolve over time. In collaboration with The Alan Turing Institute; Lars will aim to open up a new research field coined ""Phase-space machine-learning"" that tries to address this. To kick-start interest in the field; a data-science competition will be hosted on Kaggle; where a prize will be offered to the modelers that are best able to predict how a set of molecular systems evolve in time.; ",['N/A'],Dr Lars Andersen Bratholm 
leon-danon-0,Dynamical systems & differential equations; Mathematical physics; Multi-agent systems; Multi-agent reasoning; Game theory; Nonlinear dynamics; Visualisation (Computer systems & architectures); Applications (Machine learning); Deterministic (Mathematical modelling); Dynamic/static (Mathematical modelling); Graph theory; Stochastic (Mathematical modelling); Ethics; Management science; Social networks; Uncertainty quantification; Monte Carlo methods; Simulation; Time series; ,Leon Danon is an interdisciplinary scientist interested in collective behaviour; complex systems; networks and public health. He studies human populations; contacts and social interactions; motivated by applications in public health. He is often guided by the study of large; relevant datasets that capture statistical patterns which can be understood using statistical physics; mathematical modelling approaches and data science. His particular focus is on infectious disease epidemiology and the subtle interplay between human behaviour and disease transmission.; He is also interested in collaboration networks in music; infectious disease surveillance and the design of clinical trials. ; At The Alan Turing Institute; Leon is planning to tackle pressing problems in public health that can be understood from a data intensive perspective. ; How and why do clinical decisions get made? What influences those decisions? Are there practical steps that can be taken to improve patient outcomes and at the same improve the cost efficiency of a public health care system?  ; Leon is especially keen in building relationships across disciplines; especially in areas that are currently under-explored where human-centric information is available. Please do get in touch to discuss. ; ,At The Alan Turing Institute; Leon is planning to tackle pressing problems in public health that can be understood from a data intensive perspective. ; How and why do clinical decisions get made? What influences those decisions? Are there practical steps that can be taken to improve patient outcomes and at the same improve the cost efficiency of a public health care system?  ; Leon is especially keen in building relationships across disciplines; especially in areas that are currently under-explored where human-centric information is available. Please do get in touch to discuss. ; ,['N/A'],Dr Leon Danon 
leonardo-bottolo,['N/A'],Dr Leonardo Bottolo is Reader in Statistics for Biomedicine at the University of Cambridge. He received his PhD in Methodological Statistics from the University of Trento; Italy; in 2001. Before joining the University of Cambridge; he was appointed Senior Lecturer in Statistics in the Department of Mathematics; Imperial College. He worked as postdoc in the Mathematical Genetics group; University of Oxford and at the Institute of Mathematical Sciences; Imperial College.; Dr Leonardo Bottolo has considerable experience of methodological and algorithmic aspects of data integration and feature selection in genetics and system biology for high-dimensional data. In continuity with his experience in population genetics gained in Oxford he implemented new multivariate approaches for the detection of systemic associations between different types of -omics data. Currently he is further developing these statistical tools with novel MCMC samplers to make them fully operational on current large genetics data and on future high-throughput Next Generation Sequencing. The implementation of these algorithms benefits from recent advances in parallel computing of massive data sets.; ,Dr Leonardo Bottolo has considerable experience of methodological and algorithmic aspects of data integration and feature selection in genetics and system biology for high-dimensional data. In continuity with his experience in population genetics gained in Oxford he implemented new multivariate approaches for the detection of systemic associations between different types of -omics data. Currently he is further developing these statistical tools with novel MCMC samplers to make them fully operational on current large genetics data and on future high-throughput Next Generation Sequencing. The implementation of these algorithms benefits from recent advances in parallel computing of massive data sets.; ,['N/A'],Dr Leonardo Bottolo 
levi-john-wolf,Multi-agent systems; Operations research; Applications (Machine learning); Computer vision; Deep learning; Semi-supervised learning; Unsupervised learning; Graph theory; Stochastic (Mathematical modelling); Probabilistic programming; Visualisation (Programming languages); Data science of government & politics; Social networks; Research methods; Uncertainty quantification; Simulation; Modelling (Statistical methods & theory); Probability; ,Levi John Wolf is a Lecturer of Quantitative Human Geography at the University of Bristol. He holds affiliate appointments as a Fellow at the Center for Spatial Data Science at the University of Chicago & Affiliate Faculty at the University of California; Riverside Center for Geospatial Sciences. He completed his PhD at Arizona State University in the GeoDa Center for Spatial Analysis & Computation in 2017. He relocated to Bristol from Brooklyn in 2017. During and after his PhD; he worked at Nextdoor.com; Inc.; a social network for neighbourhoods; and CARTO; a leading spatial data science services company.; Dr Wolf's research for The Alan Turing Institute revolves around using machine learning to bound and link cities using new forms of data. The work examines how areas in which we live and work might not overlap well with the boundaries of city governments or federal agencies. By understanding where the formal city stops; we can gain a better understanding of urban areas that are left out of formal infrastructure; development; governance; and analysis areas.; ,Dr Wolf's research for The Alan Turing Institute revolves around using machine learning to bound and link cities using new forms of data. The work examines how areas in which we live and work might not overlap well with the boundaries of city governments or federal agencies. By understanding where the formal city stops; we can gain a better understanding of urban areas that are left out of formal infrastructure; development; governance; and analysis areas.; ,['N/A'],Dr Levi John Wolf 
lora-fleming,Communications; Databases; Ethics; ,Professor Lora E Fleming is a physician and epidemiologist with expertise in the environment and human health; she is based at the European Centre for Environment and Human Health (University of Exeter Medical School). The research and training at the European Centre is focused on an interdisciplinary approach to the interactions between the health of both humans and the environment. She has done collaborative interdisciplinary research around big data data mashups and environment and human health; including the MRC NERC funded MEDMI Project (https://www.data-mashup.org.uk).; ORCID: 0000-0003-1076-9967.; Professor Fleming is interested in the uses (and abuses) of big data in the context of environment and health nationally and internationally.; Professor Fleming very interested in the new trans/interdisciplinary metadiscipline of Oceans and Human Health; and she received the Ocean and Human Awards from the Edouard Delcroix Foundation and the IOC Bruun Award. Prof Fleming is leading the H2020 funded Projects; BlueHealth to explore the connections between blue environments and human health; and Seas; Oceans and Public Health in Europe (SOPHIE) to establish a network and create a strategic research agenda for Oceans and Human Health in Europe and beyond.; ,Professor Fleming is interested in the uses (and abuses) of big data in the context of environment and health nationally and internationally.; ,Professor Fleming very interested in the new trans/interdisciplinary metadiscipline of Oceans and Human Health; and she received the Ocean and Human Awards from the Edouard Delcroix Foundation and the IOC Bruun Award. Prof Fleming is leading the H2020 funded Projects; BlueHealth to explore the connections between blue environments and human health; and Seas; Oceans and Public Health in Europe (SOPHIE) to establish a network and create a strategic research agenda for Oceans and Human Health in Europe and beyond.; ,Professor Lora E. Fleming MD PhD MPH MSc FAAFP PFHEA
lorenzo-jamone,Robotics; Control theory; Evolution & adaptation; Human computer interface; Applications (Machine learning); Computer vision; Deep learning; Reinforcement learning; Cognitive science; Uncertainty quantification; ,Lorenzo Jamone is Lecturer in Robotics at the Queen Mary University of London; since 2016. He received his MS in computer engineering from the University of Genova in 2006 (with honors); and his PhD in humanoid technologies from the University of Genova and the Italian Institute of Technology (IIT) in 2010. He was a Research Fellow at the RBCS Department of the IIT in 2010; Associate Researcher at Takanishi Lab (Waseda University; Tokyo; Japan) from 2010 to 2012; and Associate Researcher at VisLab (the Computer Vision Laboratory of the Instituto de Sistemas e Robotica; Instituto Superior Tecnico; Lisbon; Portugal) from 2013 to 2016. ; Lorenzo's main research interests include: sensorimotor learning and control in humans and robots; robotic reaching; grasping; manipulation and tool use; force and tactile sensing; intelligent systems and cognitive developmental robotics.; ,Lorenzo's main research interests include: sensorimotor learning and control in humans and robots; robotic reaching; grasping; manipulation and tool use; force and tactile sensing; intelligent systems and cognitive developmental robotics.; ,['N/A'],Dr Lorenzo Jamone 
lorenzo-pellis,Dynamical systems & differential equations; Operations research; Evolution & adaptation; Nonlinear dynamics; Mathematical modelling; Deterministic (Mathematical modelling); Ensemble (Mathematical modelling); Stochastic (Mathematical modelling); Social networks; Uncertainty quantification; Monte Carlo methods; ,Lorenzo Pellis graduated in Mathematics at the Università di Trieste (Italy) before moving to London for a PhD and postdoc at the Centre for Outbreak Analysis and Modelling at Imperial College (2005-2012).; After another postdoc at the Systems Biology and Infectious Disease Epidemiology group at the University of Warwick (2012-2017); he was awarded a Sir Henry Dale Fellowship (Wellcome Trust + Royal Society); which he is now continuing in the School of Mathematics at the University of Manchester.; He became a Turing Fellow in Manchester in August 2018.; Lorenzo’s main research focuses on developing models to study the epidemiological and evolutionary consequences of coinfection (defined as the simultaneous presence in a host of two or more pathogen variants or species). Genetic data can help us better understanding within-host processes; but the epidemiological and evolutionary consequences remain elusive because of incomplete biological knowledge; but also because of a lack of suitable mathematical models that can capture simultaneously the within- and between-host scales. Using models to capture the complex feedback loops between such scales is crucial if we want to make full use of the limited (expensive; sometimes invasive) and typically indirect genetic and epidemiological data. Applications involve better understanding of:; Aside from his main project; Lorenzo has an interest in data science; and in particular statistical methods for parameter estimation. Rather than `big data’; the challenges in epidemiological and health-care data are mostly due to the large amount of missing data; which require statistically advanced and computationally demanding methods (e.g. data augmentation; reversible-jump MCMC).; Lorenzo Pellis has been awarded a prestigious Sir Henry Dale Fellowship in 2016 from the Wellcome Trust and the Royal Society.; Other awards/titles include:; ,Lorenzo’s main research focuses on developing models to study the epidemiological and evolutionary consequences of coinfection (defined as the simultaneous presence in a host of two or more pathogen variants or species). Genetic data can help us better understanding within-host processes; but the epidemiological and evolutionary consequences remain elusive because of incomplete biological knowledge; but also because of a lack of suitable mathematical models that can capture simultaneously the within- and between-host scales. Using models to capture the complex feedback loops between such scales is crucial if we want to make full use of the limited (expensive; sometimes invasive) and typically indirect genetic and epidemiological data. Applications involve better understanding of:; Aside from his main project; Lorenzo has an interest in data science; and in particular statistical methods for parameter estimation. Rather than `big data’; the challenges in epidemiological and health-care data are mostly due to the large amount of missing data; which require statistically advanced and computationally demanding methods (e.g. data augmentation; reversible-jump MCMC).; ,Lorenzo Pellis has been awarded a prestigious Sir Henry Dale Fellowship in 2016 from the Wellcome Trust and the Royal Society.; Other awards/titles include:; ,Dr Lorenzo Pellis 
luciano-floridi,['N/A'],Luciano Floridi is Professor of Philosophy and Ethics of Information at the University of Oxford; where he directs the Digital Ethics Lab (DELab) of the Oxford Internet Institute. He is also Faculty Fellow of the Alan Turing Institute and Chair of its Data Ethics research Group; and Chairman of the Ethics Advisory Board of the European Medical Information Framework. He seats on the EU's Ethics Advisory Group on Ethical Dimensions of Data Protection; on the Royal Society and British Academy Working Group on Data Governance; and on Google Advisory Board on “the right to be forgotten”. His areas of expertise include the philosophy of information; digital ethics; and the philosophy of technology. Among his recent books; all published by Oxford University Press: The Fourth Revolution - How the infosphere is reshaping human reality (2014); The Ethics of Information (2013); The Philosophy of Information (2011). ; Luciano's areas of research are: philosophy of information; information ethics (including data ethics and computer ethics); philosophy of AI; philosophy of technology; epistemology; and logic.; ,Luciano's areas of research are: philosophy of information; information ethics (including data ethics and computer ethics); philosophy of AI; philosophy of technology; epistemology; and logic.; ,['N/A'],Professor Luciano Floridi 
lucy-van-de-wiel,Robotics; Human computer interface; Ethics; Social media; ,Lucy van de Wiel is a Research Associate at the Reproductive Sociology Research Group (ReproSoc) at the University of Cambridge. For the next three years; she will lead the 'Extending In/Fertilities' network within ReproSoc's Wellcome-funded Changing Infertilities collaborative research project starting in autumn 2018. Her research focuses on egg freezing and the gender politics of ageing; the datafication of reproduction through the introduction of new data technologies in IVF; and the political economy of contemporary assisted reproduction. Her current book project is called Freezing Fertility: Oocyte Cryopreservation and the Gender Politics of Reproductive Ageing.; Alongside her research; she currently coordinates the Wellcome-funded Life in Glass public engagement project within ReproSoc; which includes the development of Dish Life; a mobile game app about stem cells; Reproductivities; an art exhibition about plant and human reproduction at Murray Edwards College; and Timeless; a fictional pop-up shop that was installed at the world's largest fertility trade shows.; She received her PhD in 2015 at the University of Amsterdam and won the 2016 ASCA Award for best dissertation as well as the 2017 Erasmus Research Prize. She pursued postgraduate studies as a HSP and Fulbright grantee in Rhetorics at the University of California; Berkeley; holds a Research MA in Cultural Analysis (cum laude) from the University of Amsterdam and an MA in Film Curating (with distinction) from the London Film School and London Consortium; University of London.; Lucy van de Wiel's research focuses on the interdisciplinary study of egg freezing; a reproductive technology used to 'preserve fertility' in women who may want to have genetically-related children later in life. Her research explores how the possibility of freezing one's eggs changes what it means to be fertile; and what it means to age; in the 21st century. This question is at the heart of her current book project on egg freezing; titled Freezing Fertility: Oocyte Cryopreservation and the Gender Politics of Reproductive Ageing.; At the Turing Institute; she is developing a new research project on the datafication of reproduction; which deals with the intersection of data technologies and reproductive technologies. This research explores how reproductive decisions such as which embryo to implant in the womb are increasingly made in conjunction with data technologies and the large data sets they generate. It is primarily concerned with data-driven embryo selection technologies; such as time-lapse embryo imaging and pre-implantation genetic screening. The project both considers the role of these technologies in patients' and medical professionals' treatment experiences and characterises the broader political-economic and socio-cultural drivers of their emergence in contemporary IVF.; ,Lucy van de Wiel's research focuses on the interdisciplinary study of egg freezing; a reproductive technology used to 'preserve fertility' in women who may want to have genetically-related children later in life. Her research explores how the possibility of freezing one's eggs changes what it means to be fertile; and what it means to age; in the 21st century. This question is at the heart of her current book project on egg freezing; titled Freezing Fertility: Oocyte Cryopreservation and the Gender Politics of Reproductive Ageing.; At the Turing Institute; she is developing a new research project on the datafication of reproduction; which deals with the intersection of data technologies and reproductive technologies. This research explores how reproductive decisions such as which embryo to implant in the womb are increasingly made in conjunction with data technologies and the large data sets they generate. It is primarily concerned with data-driven embryo selection technologies; such as time-lapse embryo imaging and pre-implantation genetic screening. The project both considers the role of these technologies in patients' and medical professionals' treatment experiences and characterises the broader political-economic and socio-cultural drivers of their emergence in contemporary IVF.; ,['N/A'],Dr Lucy van de Wiel 
luisa-orsini,Time series; Computing networks; Evolution & adaptation; Machine learning; Unsupervised learning; ,Dr Luisa Orsini is a Senior Lecturer/Associate Professor in Biosystems and Environmental Change at the University of Birmingham (UoB); and a time traveller. She studies the processes and mechanisms of evolutionary response to climate and other environmental factors with relevance to climate – pollution; anthropogenic land-use. To reconstruct long-term dynamics she applies high throughput technologies to sedimentary archives of inland waters; which have the unique advantage of preserving biological and environmental signals temporally. Moreover; she applies high throughput technologies to ‘resurrected’ specimens of the keystone species Daphnia magna (waterflea) to identify the molecular mechanisms that enable evolutionary changes through time and space. Dr Orsini strongly believes in bridging the science/policy divide. To this end; she works on biotechnology solutions for the removal of pharmaceuticals; pesticides and other suspended materials from wastewater and surface water.; Human health and well-being are intimately linked to environmental quality. Pollution is the largest environmental cause of disease and death in the world today; responsible for an estimated 9 million premature deaths. Detection and measures in the environment and health area are inadequate because the links between healthy environments and healthy humans are dynamic and complex. First; the variety of species and genes in a given habitat; called biodiversity; is the foundation of healthy ecosystems and of the services they provide; which underpin economic prosperity; social well-being and quality of life.; Biodiversity is declining at 1;000 times the natural rate; which is responsible for the decline of an estimated 60% of the Earth's ecosystem services (ES) over the last 50 years. Second; climate change significantly impacts biodiversity; causing habitat destruction and species loss; as well as shifts in community assembly and ecological structure. However; climate is only one of the factors impacting biodiversity. It is the synergistic action of climate and other environmental factors linked to human activities (e.g. land-use; pollution and species invasion) that ultimately impact biodiversity. Last but not least; biodiversity response to environmental change varies dramatically in space and time. Only by simultaneously investigating environmental change and biodiversity across space and time; the impact of such changes on ES directly linked to human health and wellbeing can be understood. Only by accurately forecasting the impact of loss of biodiversity on ES; can we begin to understand the long-term impact of polluted environments on human health.; As a Turing Fellow; Dr Orsini has the long term goal of improving human health and wellbeing by creating a novel evidence-based framework that enables the identification of actionable targets for ES conservation; and the prioritization of pollutants for mitigation interventions.; ,Human health and well-being are intimately linked to environmental quality. Pollution is the largest environmental cause of disease and death in the world today; responsible for an estimated 9 million premature deaths. Detection and measures in the environment and health area are inadequate because the links between healthy environments and healthy humans are dynamic and complex. First; the variety of species and genes in a given habitat; called biodiversity; is the foundation of healthy ecosystems and of the services they provide; which underpin economic prosperity; social well-being and quality of life.; Biodiversity is declining at 1;000 times the natural rate; which is responsible for the decline of an estimated 60% of the Earth's ecosystem services (ES) over the last 50 years. Second; climate change significantly impacts biodiversity; causing habitat destruction and species loss; as well as shifts in community assembly and ecological structure. However; climate is only one of the factors impacting biodiversity. It is the synergistic action of climate and other environmental factors linked to human activities (e.g. land-use; pollution and species invasion) that ultimately impact biodiversity. Last but not least; biodiversity response to environmental change varies dramatically in space and time. Only by simultaneously investigating environmental change and biodiversity across space and time; the impact of such changes on ES directly linked to human health and wellbeing can be understood. Only by accurately forecasting the impact of loss of biodiversity on ES; can we begin to understand the long-term impact of polluted environments on human health.; As a Turing Fellow; Dr Orsini has the long term goal of improving human health and wellbeing by creating a novel evidence-based framework that enables the identification of actionable targets for ES conservation; and the prioritization of pollutants for mitigation interventions.; ,['N/A'],Dr Luisa Orsini 
magda-osman,Cognitive science; Ethics; Causality; ,Dr Osman is a Reader in Experimental Psychology at Queen Mary University of London. She is also currently seconded part of her time to the Food Standards Agency working in the Science and Evidence Research Division on evidence based policy projects. ; Dr Osman is a decision sciences researcher with a strong background in judgement and decision-making research. Her recent focus is in applying her work to applied areas; particularly in the context of data privacy and the implications that this has for business and public decision-making regarding the rights associated with trading data; and has examined ways to improve the communication of risk and uncertainty (particularly in the domain of risk assessment); ,Dr Osman is a decision sciences researcher with a strong background in judgement and decision-making research. Her recent focus is in applying her work to applied areas; particularly in the context of data privacy and the implications that this has for business and public decision-making regarding the rights associated with trading data; and has examined ways to improve the communication of risk and uncertainty (particularly in the domain of risk assessment); ,['N/A'],Dr Magda Osman 
magnus-rattray,['N/A'],Magnus Rattray is Professor of Computational & Systems Biology at the University of Manchester. He graduated with a BSc in Mathematics & Physics (1992) and PhD in Computer Science (1996) from the University of Manchester. Postdoctoral work on the statistical mechanics theory of neural networks at the Neural Computing Research Group; Aston University; was followed by a Lectureship in Computer Science at the University of Manchester. He was Professor of Machine Learning & Statistical Bioinformatics at Sheffield University before his return to Manchester in 2012 where he is Director of the University of Manchester Data Science Institute.; Magnus uses probabilistic modelling and Bayesian inference techniques to study biological systems across a broad range of temporal and spatial scales; from gene expression in single cells to longitudinal population health data. Recent work includes methods to uncover oscillations from single-cell imaging time course data and the development of scalable Gaussian process models for pseudotime and branching process inference using single-cell RNA-Seq data. ; ,Magnus uses probabilistic modelling and Bayesian inference techniques to study biological systems across a broad range of temporal and spatial scales; from gene expression in single cells to longitudinal population health data. Recent work includes methods to uncover oscillations from single-cell imaging time course data and the development of scalable Gaussian process models for pseudotime and branching process inference using single-cell RNA-Seq data. ; ,['N/A'],Professor Magnus Rattray 
mahesh-marina,Verification; Identity management; Computing networks; Communications; Parallel computing; Operating systems; Robotics; Applications (Machine learning); ,Mahesh Marina is a Professor in the School of Informatics at the University of Edinburgh; where he also currently serves as the Director of the Institute for Computing Systems Architecture. Before joining Edinburgh in 2006; he had a two-year postdoctoral stint at the UCLA Computer Science Department. He received his PhD in Computer Science from the State University of New York at Stony Brook in 2004. He has previously held visiting researcher positions at ETH Zurich and at the Ofcom Headquarters in London. He is a Distinguished Member of the ACM and a Senior Member of the IEEE.; Prof Marina's research broadly falls in wireless networks; mobile systems; data privacy; applied machine learning and network security. His recent and on-going research focus is on various aspects of future 5G and beyond mobile networks (network slicing; spectrum sharing; universal access; cellular IoT; monitoring and analytics); mobile sensing and privacy. He is particularly interested in applications of machine learning towards service quality optimisation; automation and robustness of future network infrastructures.; ,Prof Marina's research broadly falls in wireless networks; mobile systems; data privacy; applied machine learning and network security. His recent and on-going research focus is on various aspects of future 5G and beyond mobile networks (network slicing; spectrum sharing; universal access; cellular IoT; monitoring and analytics); mobile sensing and privacy. He is particularly interested in applications of machine learning towards service quality optimisation; automation and robustness of future network infrastructures.; ,['N/A'],Professor Mahesh Marina 
maria-liakata,Natural language processing; Applications (Machine learning); ,Maria is an Associate Professor in Natural Language Processing (NLP) at the University of Warwick; Department of Computer Science. She is in receipt of a Turing AI fellowship award on Creating Time Sensitive Sensors from Language & Heterogeneous User-Generated Content (2019-2024). At the Alan Turing Institute she co-leads the NLP and data science for mental health interest groups and supervises PhD students. She is also leading projects which cover “Emotion sensing using heterogeneous mobile phone data”; “Language sensing for dementia monitoring & diagnosis” ; “Multi-modal processing for bipolar diagnosis” and “Opinion summarisation from social media”. ; Maria has a DPhil from the University of Oxford on learning pragmatic knowledge from text.  Her work has contributed to advances in knowledge discovery from corpora; automation of scientific experimentation and automatic extraction of information from the scientific literature. She has published widely both in NLP and interdisciplinary venues. Past awards include an IBM Faculty Award for work on emotion sensing from heterogeneous mobile phone data; being a co-investigator on the EU Project PHEME; which studied the spread of rumours in social media (2014-2017) and an Early Career Fellowship from the Leverhulme Trust (2010-2013) on reasoning with scientific articles.  ; Natural language processing (NLP); NLP for social and biomedical applications; analysis of multi-modal and heterogeneous data and especially the development of personalised longitudinal language processing; opinion mining and summarisation; rumour verification; biological text mining; computational semantics; scientific discourse analysis.; ,Natural language processing (NLP); NLP for social and biomedical applications; analysis of multi-modal and heterogeneous data and especially the development of personalised longitudinal language processing; opinion mining and summarisation; rumour verification; biological text mining; computational semantics; scientific discourse analysis.; ,['N/A'],Dr Maria Liakata DPhil
maria-wolters,['N/A'],Maria's research focuses on supporting people with long-term conditions live rich and meaningful lives. She has a background in computational linguistics and speech science (PhD; 2000; University of Bonn); human-computer interaction; assistive technology; and eHealth; and a long-term interest in statistics.; Maria is interested in why people collect health data. In particular; missing data is often missing for a reason. This could be because the smartphone is out of battery; or because one has no energy left to complete a diary entry. She wanta to find patterns of missing data and describe them qualitatively and quantitatively in a way that leads to new insight - in other words; treating missingness as information.; ,Maria is interested in why people collect health data. In particular; missing data is often missing for a reason. This could be because the smartphone is out of battery; or because one has no energy left to complete a diary entry. She wanta to find patterns of missing data and describe them qualitatively and quantitatively in a way that leads to new insight - in other words; treating missingness as information.; ,['N/A'],Dr Maria Wolters 
mariarosaria-taddeo,['N/A'], ; Dr. Mariarosaria Taddeo Researcher – Oxford Internet Institute; University of Oxford and a Fellow at the Alan Turing Institute. Her recent work focuses mainly on the ethical analysis of cyber security practices and information conflicts. Dr. Taddeo is the 2010 recipient of the Simon Award for Outstanding Research in Computing and Philosophy and of the 2013 World Technology Award for Ethics. She is Co-Investigator; PETRAS Hub for Internet of Things; a EPSRC project and serves editor-in-chief of Minds & Machines; in the executive editorial board of Philosophy & Technology; and is the President of the International Association of Computing and Philosophy. ; Her area of expertise is Information and Computer Ethics; although she has worked on issues concerning Philosophy of Information; Epistemology; and Philosophy of AI. She published several papers focusing on online trust; cyber security and cyber warfare and guest-edited a number of special issues of peer-reviewed international journals: Ethics and Information Technology; Knowledge; Technology and Policy; Philosophy & Technology. She also edited (with L. Floridi) a volume on ‘The Ethics of Information Warfare’ (Springer; 2014) and is currently writing a book on ‘The Ethics of Cyber Conflicts’ under contract for Routledge.; ,Her area of expertise is Information and Computer Ethics; although she has worked on issues concerning Philosophy of Information; Epistemology; and Philosophy of AI. She published several papers focusing on online trust; cyber security and cyber warfare and guest-edited a number of special issues of peer-reviewed international journals: Ethics and Information Technology; Knowledge; Technology and Policy; Philosophy & Technology. She also edited (with L. Floridi) a volume on ‘The Ethics of Information Warfare’ (Springer; 2014) and is currently writing a book on ‘The Ethics of Cyber Conflicts’ under contract for Routledge.; ,['N/A'],Dr Mariarosaria Taddeo 
marika-taylor,Complexity (Algorithms); Dynamical systems & differential equations; Mathematical physics; Machine learning; Supervised learning; Mathematical modelling; Stochastic (Mathematical modelling); Monte Carlo methods; Information theory (Statistical methods & theory); Geometry & topology; ,Marika Taylor studied for a PhD at Cambridge under the supervision of Stephen Hawking. After postdoctoral research in Cambridge and Utrecht; she became a staff member at the Institute of Theoretical Physics at the University of Amsterdam. She moved to the University of Southampton in 2012; as part of the Southampton Theory Astronomy and Gravity (STAG) initiative. She is currently head of Applied Mathematics and Theoretical Physics at Southampton.  ; Marika Taylor works on a range of topics in mathematical physics; including quantum field theory; string theory; holography; black holes and quantum information. Holography is a relationship between gravitational theories (geometry and topology) and quantum theories in one less dimension. In recent years; holography has led of very deep connections between hitherto distinct areas of science. For example; the quantum properties of black holes link together geometry; machine learning and quantum error correction; one of the goals of this Turing fellowship will be to build links between the mathematical physics community and those working on machine learning. The mathematical modelling that arises in statistical physics also has many unexploited applications to real world problems. The interdisciplinary environment of Turing will be ideal for developing connections to important problems such as the mathematical modelling of migration. ; Elected to DJA; Dutch Royal Society; 2009.; Minerva prize of Dutch Physics Research Council; FOM; 2009.; Leader of CERN Gen-HET working group.; ,Marika Taylor works on a range of topics in mathematical physics; including quantum field theory; string theory; holography; black holes and quantum information. Holography is a relationship between gravitational theories (geometry and topology) and quantum theories in one less dimension. In recent years; holography has led of very deep connections between hitherto distinct areas of science. For example; the quantum properties of black holes link together geometry; machine learning and quantum error correction; one of the goals of this Turing fellowship will be to build links between the mathematical physics community and those working on machine learning. The mathematical modelling that arises in statistical physics also has many unexploited applications to real world problems. The interdisciplinary environment of Turing will be ideal for developing connections to important problems such as the mathematical modelling of migration. ; ,Elected to DJA; Dutch Royal Society; 2009.; Minerva prize of Dutch Physics Research Council; FOM; 2009.; Leader of CERN Gen-HET working group.; ,Professor Marika Taylor 
mark-freestone,Pattern recognition; Reinforcement learning; Probabilistic programming; Research methods; Social psychology; Uncertainty quantification; Causality; Probability; ,Mark Freestone is Senior Lecturer in the Centre for Psychiatry; Wolfson Institute for Preventive Medicine; Queen Mary University of London. He has a first class honors degree in Social and Political Sciences fromt the University of Cambridge; and gained his PhD in Applied Social Science from the University of Nottingham in 2005. His core research interests include: psychosocial treatment of mental health; particularly personality disorder; forensic mental health; the epidemiology of violence; clinical presentation and sub-types of psychopathy; and criminological research relating to prisons and prisoners.; Dr Freestone's work with the Turing will follow three work packages:; ,Dr Freestone's work with the Turing will follow three work packages:; ,['N/A'],Dr Mark Freestone 
mark-gilthorpe,Complexity (Algorithms); Data structures; Multi-agent systems; Neural networks; Databases; Visualisation (Computer systems & architectures); Applications (Machine learning); Deep learning; Deterministic (Mathematical modelling); Graph theory; Stochastic (Mathematical modelling); Research methods; Causality; Monte Carlo methods; Simulation; Time series; Modelling (Statistical methods & theory); ,Mark is Professor of Statistical Epidemiology in the School of Medicine and the Leeds Institute for Data Analytics (LIDA); and a Fellow of the Alan Turing Institute for Data Science and Artificial Intelligence. Trained as a mathematical physicist; Mark's driving interest centres on improving our understanding of the observable world through modelling. After his PhD; he spent time as a consultant data analyst before being recruited into academia. Mark has since fashioned a programme of interdisciplinary research that spans the gap between theoretical and applied data analytics; focusing particularly on modelling complexity and highlighting and solving common analytical problems in observational research. Mark's research and teaching interests have converged around the insights and utility of causal inference methods; and how these might be integrated with machine learning and AI; he is also a recognised expert in latent variable modelling and the analysis of longitudinal data. As Deputy Director for LIDA (Research and Innovation); Mark is especially keen on further developing and promoting the unique collaborative and cross-disciplinary opportunities provided by LIDA. As a Turing Fellow; Mark is actively engaging across the Turing network to promote the development and application of causal inference methods.; Mark is seeking to understand complex relationships between individuals within their natural environment through the development and application of observational methods; specifically through the integration of causal inference modelling and agent-based modelling. An example domain of this challenge is modelling patterns; causes and consequences of obesity within our society.Mark is also interested in ‘algorithmic explainability’ and the development of ‘smart AI’; i.e. the use of causal inference methodology to understand the workings; operations and consequences of machine learning and artificial intelligence. ; ,Mark is seeking to understand complex relationships between individuals within their natural environment through the development and application of observational methods; specifically through the integration of causal inference modelling and agent-based modelling. An example domain of this challenge is modelling patterns; causes and consequences of obesity within our society.Mark is also interested in ‘algorithmic explainability’ and the development of ‘smart AI’; i.e. the use of causal inference methodology to understand the workings; operations and consequences of machine learning and artificial intelligence. ; ,['N/A'],Professor Mark Gilthorpe 
mark-graham,['N/A'], ; Mark Graham is an Associate Professor at the Oxford Internet Institute. Mark’s research focuses internet geographies; digital work; and economic development. He is currently leading a European Research Council Starting Grant to lead a team to study 'knowledge economies' in Sub-Saharan Africa over five years.; Mark studies the intersections of geography and big online datasets. Specifically; this strand of work examines two broad questions: (1) what can we learn about volumes and types of online participation from different parts of the world? (e.g. why do we see higher rates of participation from some people/places and not others?); (2) we can we learn about the representation of physical phenomena from digital datasets? (e.g. what can the geography of digital phenomena; such as tweets about floods; tell us about physical phenomena; such as floods? etc.). He also leads an Oxford-based ‘incubator’ that focuses on the intersections between big data and human development.; ,Mark studies the intersections of geography and big online datasets. Specifically; this strand of work examines two broad questions: (1) what can we learn about volumes and types of online participation from different parts of the world? (e.g. why do we see higher rates of participation from some people/places and not others?); (2) we can we learn about the representation of physical phenomena from digital datasets? (e.g. what can the geography of digital phenomena; such as tweets about floods; tell us about physical phenomena; such as floods? etc.). He also leads an Oxford-based ‘incubator’ that focuses on the intersections between big data and human development.; ,['N/A'],Professor Mark Graham 
mark-kelson,Research methods; Statistical methods & theory; Causality; Modelling (Statistical methods & theory); ,Mark studied Mathematics and Statistics at University College Cork; before completing an MSc in Statistics there also. He studied for his PhD in Medical Statistics at Cardiff University before spending a decade as a clinical trials statistician in the Centre for Trials Research. Mark’s current role is Senior Lecturer in Data Science in the Mathematics department of the University of Exeter. ; Mark is interested in using data science for health. He particularly focusses on physical activity and mental health. He finds accelerometry data interesting. In addition; he is interested in clinical trials and how data science can help make them more efficient. ; ,Mark is interested in using data science for health. He particularly focusses on physical activity and mental health. He finds accelerometry data interesting. In addition; he is interested in clinical trials and how data science can help make them more efficient. ; ,['N/A'],Dr Mark Kelson 
mark-mon-williams,Control theory; Neuroscience; Systems theory; Databases; Human computer interface; Reinforcement learning; Cognitive science; Data science of government & politics; Uncertainty quantification; Probability; ,Professor Mark Mon-Williams (MMW) holds a Chair in Cognitive Psychology at the University of Leeds; and is Professor of Psychology at the Bradford Institute of Health Research and Professor of Paediatric Vision at The Norwegian Centre for Vision. 25 years ago; MMW made fundamental contributions to our understanding of the sensorimotor impact of Virtual Reality (work that was headline news around the world).; He is Director of the Centre for Immersive Technologies at the University of Leeds. MMW is Director of the Centre of Applied Education Research (a partnership between the Universities of Leeds and Bradford together with the Department for Education; the Education Endowment Foundation; and the Bradford Local Authority) – a multidisciplinary Centre based at the Bradford Royal Infirmary. MMW leads a research group that use their fundamental scientific contributions to address applied issues within surgery; rehabilitation and childhood development and he has responsibility for ensuring societal impact arises from research conducted within the University of Leeds' Faculty of Medicine and Health.; MMW leads the NHS CLAHRC committee responsible for 'Identifying and Supporting Children with Difficulties'; and is an executive member of the Born in Bradford project (a longitudinal cohort study following the lifelong development of 13;500+ children). His research is funded by a number of organisations including the EPSRC; EEF; MRC and ESRC. MMW is a Founder Member of the Priestley Academy Trust (a multiple academy trust that includes the first school known to provide free meals to children); and sits on the Bradford Opportunity Area partnership board.; MMW’s Turing based research builds on a recently awarded Medical Research Council Mental Health Data Pathfinder grant (£1.5 Million) that is linking two large cohort datasets (the Avon Longitudinal Study of Parents and Children and Born in Bradford) in order to explore ways of decreasing the incidence of mental health problems. The goal is to create generative models that can better predict the childhood risk factors for mental health problems. The data analytic and artificial intelligence expertise within the Turing Institute will help ensure this project ‘revolutionises healthcare’. ; A key mental health challenge is the effective early identification of children at risk of a mental health problem; so that early intervention may prevent future disability. Linked to this is the challenge of identifying factors that protect some children from developing mental illness; despite experiencing extreme adversity (including adverse events that lead to public care). Better understanding of these issues could empower service providers (including schools; health and social care services) to support children better; and mitigate the risk of children developing mental health problems. Risk prediction is improved by greater availability of relevant information which can also help reduce bias associated with single measures. ; We are in the process of linking education; health and social care data; including data on receipt of public care; with study data to identify risk and protective factors. This work opens up the possibility of producing powerful predictive models that can improve our understanding of how underlying factors influence the probability of mental ill health.; ,MMW’s Turing based research builds on a recently awarded Medical Research Council Mental Health Data Pathfinder grant (£1.5 Million) that is linking two large cohort datasets (the Avon Longitudinal Study of Parents and Children and Born in Bradford) in order to explore ways of decreasing the incidence of mental health problems. The goal is to create generative models that can better predict the childhood risk factors for mental health problems. The data analytic and artificial intelligence expertise within the Turing Institute will help ensure this project ‘revolutionises healthcare’. ; A key mental health challenge is the effective early identification of children at risk of a mental health problem; so that early intervention may prevent future disability. Linked to this is the challenge of identifying factors that protect some children from developing mental illness; despite experiencing extreme adversity (including adverse events that lead to public care). Better understanding of these issues could empower service providers (including schools; health and social care services) to support children better; and mitigate the risk of children developing mental health problems. Risk prediction is improved by greater availability of relevant information which can also help reduce bias associated with single measures. ; We are in the process of linking education; health and social care data; including data on receipt of public care; with study data to identify risk and protective factors. This work opens up the possibility of producing powerful predictive models that can improve our understanding of how underlying factors influence the probability of mental ill health.; ,['N/A'],Professor Mark Mon-Williams 
mark-sandler,Human computer interface; Information retrieval; Deep learning; Social media; ,Mark Sandler is Professor of Signal Processing at Queen Mary University of London; and is also Director of the Centre for Digital Music. He gained a BSc (Hons) in Electronic Engineering from University of Essex in 1978; following that with a PhD; also from Essex in 1983 on digital audio power amplification.; His first academic post was as Lecturer in Telecommunications in the Department of Electronic Engineering at King's College London; under the headship of Professor Charles Turner; FREng. He was Director of Research; then gained a personal Chair in Signal Processing before becoming Head of Department. He left KCL in 2001 to take up a chair at Queen Mary; where he founded the Centre for Digital Music.; He was Head of School of Electronic Engineering and Computer Science from 2010-2014 and founding Director of the Centre for Doctoral Training in Media and Arts Technology from 2009 to 2016. He has won grants as PI with a total value in excess of £20M; has graduated around 40 PhD students and has published nearly 400 papers. As a Fellow of the Royal Academic of Engineering; he sits on its Research Committee and Membership Committee.; The goal of Mark's research at the Turing is to bring together the worlds of data science and music. To that end; an agreement to compute music-audio features on a massive scale is in place with Deezer; a French music streaming service. This will provide the raw; music-feature data with which to work. The intention is to compare the outcomes from the use of deep learning; statistical analysis and graph theoretic approaches; applied to music collection navigation.; ,The goal of Mark's research at the Turing is to bring together the worlds of data science and music. To that end; an agreement to compute music-audio features on a massive scale is in place with Deezer; a French music streaming service. This will provide the raw; music-feature data with which to work. The intention is to compare the outcomes from the use of deep learning; statistical analysis and graph theoretic approaches; applied to music collection navigation.; ,['N/A'],Professor Mark Sandler 
mark-steedman,['N/A'],Mark Steedman is Professor of Cognitive Science in the School of Informatics at the University of Edinburgh. Previously; he taught as Professor in the Department of Computer and Information Science at the University of Pennsylvania; which he joined as Associate Professor in 1988. His PhD in Artificial Intelligence is from the University of Edinburgh. He is a Fellow of the Association for the Advancement of Artificial Intelligence; the British Academy; the Royal Society of Edinburgh; the Association for Computational Linguistics; and the Cognitive Science Society; and a Member of the European Academy.; His research interests cover issues in computational linguistics; artificial intelligence; computer science and cognitive science; including syntax and semantics of natural language; wide-coverage parsing and open-domain question-answering; comprehension of natural language discourse by humans and by machine; grammar-based language modeling; natural language generation; and the semantics of intonation in spoken discourse. Much of his current NLP research is addressed to probabilistic parsing and robust semantics for question-answering using the CCG grammar formalism; including the acquisition of language from paired sentences and meanings by child and machine. He sometimes works with colleagues in computer animation using these theories to guide the graphical animation of speaking virtual or simulated autonomous human agents. Some of his research concerns the analysis of music by humans and machines.; Mark is a Turing Fellow through his supervision of Javad Hosseini; a doctoral student at the Turing.; ,His research interests cover issues in computational linguistics; artificial intelligence; computer science and cognitive science; including syntax and semantics of natural language; wide-coverage parsing and open-domain question-answering; comprehension of natural language discourse by humans and by machine; grammar-based language modeling; natural language generation; and the semantics of intonation in spoken discourse. Much of his current NLP research is addressed to probabilistic parsing and robust semantics for question-answering using the CCG grammar formalism; including the acquisition of language from paired sentences and meanings by child and machine. He sometimes works with colleagues in computer animation using these theories to guide the graphical animation of speaking virtual or simulated autonomous human agents. Some of his research concerns the analysis of music by humans and machines.; Mark is a Turing Fellow through his supervision of Javad Hosseini; a doctoral student at the Turing.; ,['N/A'],Professor Mark Steedman 
markus-brede,Dynamical systems & differential equations; Multi-agent reasoning; Control theory; Game theory; Mathematical modelling; Dynamic/static (Mathematical modelling); Graph theory; Social networks; Monte Carlo methods; Simulation; ,Markus Brede is an associate professor of computer science at the University of Southampton. After receiving a PhD in theoretical physics at the university of Leipzig (Germany) he has worked as a postdoctoral researcher and later research scientist at the Complex Systems Science group at Atmospheric Research of CSIRO (Australia) and then joined the Agents; Interaction; and Complexity group of ECS at the university of Southampton.; The main focus of his research is on theoretical work on the structure and dynamics of complex networks and interdisciplinary applications of network science and dynamical systems.  Examples of past work range from theoretical work on optimal network configurations subject to various constraints; synchronization of non-linear oscillators; evolutionary game theory; and network control in relation to spreading processes on networks. Previous applied work has spanned various disciplines; with recent work on; e.g.;  the game theory of climate change; coupling models of economic and climate dynamics; dynamical systems modeling of historical societies; the stability of ecological systems; banking regulation; network models of language learning; and the modeling of consensus formation in social systems.; ,The main focus of his research is on theoretical work on the structure and dynamics of complex networks and interdisciplinary applications of network science and dynamical systems.  Examples of past work range from theoretical work on optimal network configurations subject to various constraints; synchronization of non-linear oscillators; evolutionary game theory; and network control in relation to spreading processes on networks. Previous applied work has spanned various disciplines; with recent work on; e.g.;  the game theory of climate change; coupling models of economic and climate dynamics; dynamical systems modeling of historical societies; the stability of ecological systems; banking regulation; network models of language learning; and the modeling of consensus formation in social systems.; ,['N/A'],Dr Markus Brede 
martin-neil,Cognitive science; Uncertainty quantification; Causality; Simulation; Probability; ,"Martin Neil is a Professor in Computer Science and Statistics in Queen Mary; University of London. His research interests cover Bayesian modelling and risk quantification in diverse applied areas. At Queen Mary he teaches decision and risk analysis in the School for Electronic Engineering and Computer Science. Professor Neil is also a joint founder of Agena Ltd; who develop and distribute AgenaRisk; a software product for modelling risk and uncertainty using probabilistic modelling and AI methods. Professor Neil was a fellow at the Isaac Newton Institute for Mathematical Sciences; Cambridge University in 2016 on Probability and Statistics in Forensic Science.; Professor Neil experience in applying Bayesian methods to real problems has convinced him that intelligent risk assessment and decision analysis requires knowledge and data. Not just ""Big Data"". Typically; this involves analysing and predicting the probabilities of unknown events using Bayesian statistical methods including causal; probabilistic models (Bayesian networks). In addition to working on theoretical and algorithmic foundations; this work covers a wide range of application domains such as medical analytics; legal reasoning; embedded software; operational risk in finance; systems and design reliability (including software); project risk; commercial risk; decision support; cost benefit analysis; AI and personalization; machine learning; legal argumentation and cyber security.; Professor Neil has consulted or provided training to many organisations including General Electric; Milliman LLC; Mastercard; AIB bank; Philips; NATS; QinetiQ; DSTL (UK MOD); AON; Royal Bank of Canada; TNO (the Netherlands); BHP Billiton and ICRAF (The World Agroforestry Centre) and more. Professor Neil's book Risk Assessment and Decision Analysis with Bayesian Networks ; co-authored with Professor Norman Fenton; covers the majority of his work on Bayesian networks theory and applications. It was picked up some rather good reviews on Amazon.; ","Professor Neil experience in applying Bayesian methods to real problems has convinced him that intelligent risk assessment and decision analysis requires knowledge and data. Not just ""Big Data"". Typically; this involves analysing and predicting the probabilities of unknown events using Bayesian statistical methods including causal; probabilistic models (Bayesian networks). In addition to working on theoretical and algorithmic foundations; this work covers a wide range of application domains such as medical analytics; legal reasoning; embedded software; operational risk in finance; systems and design reliability (including software); project risk; commercial risk; decision support; cost benefit analysis; AI and personalization; machine learning; legal argumentation and cyber security.; Professor Neil has consulted or provided training to many organisations including General Electric; Milliman LLC; Mastercard; AIB bank; Philips; NATS; QinetiQ; DSTL (UK MOD); AON; Royal Bank of Canada; TNO (the Netherlands); BHP Billiton and ICRAF (The World Agroforestry Centre) and more. Professor Neil's book Risk Assessment and Decision Analysis with Bayesian Networks ; co-authored with Professor Norman Fenton; covers the majority of his work on Bayesian networks theory and applications. It was picked up some rather good reviews on Amazon.; ",['N/A'],Professor Martin Neil 
martin-weidner,Graph theory; Social networks; High dimensional inference; Non-parametric & semi-parametric methods; Asymptotic (Statistical methods & theory); Estimation theory; ,Martin is an Associate Professor in the Economics Department at UCL. He gained an Economics PhD from the University of Southern California in 2011; and a Physics PhD from the University of Hamburg in 2006.; Martin's primary research field is econometrics; with a special focus on the analysis of longitudinal data and social networks. His work is in particularly concerned with high-dimensional statistical problems that arise in the analysis of microeconomic data. His research on social networks combines methods from statistics and graph theory; and explores new connections between those two fields.; ,Martin's primary research field is econometrics; with a special focus on the analysis of longitudinal data and social networks. His work is in particularly concerned with high-dimensional statistical problems that arise in the analysis of microeconomic data. His research on social networks combines methods from statistics and graph theory; and explores new connections between those two fields.; ,['N/A'],Dr Martin Weidner 
marya-bazzi,['N/A'],Marya received her PhD in Applied Mathematics from Oxford University in 2016. Her thesis focused on clustering in time-dependent networks. Before joining the Turing she was a postdoctoral scholar at Oxford University and Head of Analytics at a London-based FinTech. Marya is particularly interested in developing research projects at the interface between academia and industry.  ; Marya is interested in developing models and algorithms to extract and/or recover information from large time-dependent data sets. She is equally interested in applying such tools to real-world problems in areas such as Sociology; Psychology; Biology; and Finance. Areas of particular interest are clustering; time-dependent networks; multilayer networks; stochastic block models (e.g.; used as generative models for interdependent “mesoscale” structure); matrix completion; graph sparsification; combinatorial optimisation and spectral optimisation. Areas she plans to further explore are statistical learning and natural language processing. Key to her approach is interdisciplinary research and close collaboration with industry.; ,Marya is interested in developing models and algorithms to extract and/or recover information from large time-dependent data sets. She is equally interested in applying such tools to real-world problems in areas such as Sociology; Psychology; Biology; and Finance. Areas of particular interest are clustering; time-dependent networks; multilayer networks; stochastic block models (e.g.; used as generative models for interdependent “mesoscale” structure); matrix completion; graph sparsification; combinatorial optimisation and spectral optimisation. Areas she plans to further explore are statistical learning and natural language processing. Key to her approach is interdisciplinary research and close collaboration with industry.; ,['N/A'],Dr Marya Bazzi 
massimo-poesio,Neural networks; Neuroscience; Deep learning; Natural language processing; Cognitive science; Data science of government & politics; Linguistics; Social media; Uncertainty quantification; ,Massimo Poesio is a Professor in Computational Linguistics in the School of Electronic Engineering and Computer Science; Queen Mary University of London. He is a cognitive scientist with a particular focus on Computational Linguistics; also known as Human Language Technology. His main research interests include computational models of anaphora resolution (coreference); the creation of large corpora of semantically annotated data (an area in which he pioneered the use of games-with-a-purpose with the development of Phrase Detectives; http://www.phrasedetectives.org); the study of conceptual knowledge using a combination of methods from human language technology and neuroscience; semantic interpretation of verbal and non-verbal communication in interaction; and the application of text analytics methods to real life problems.; Much of Professor Poesio's recent work is concerned with applications of text analytics to real world problems; in particular through the analysis of language uses in social media. Examples include his work on detecting deception in communication (e.g.; on detecting deceptive reviews online); his work on supporting human rights organisations (e.g.; by identifying and assessing reports of human rights violations in social media); and his work on analysing discussions in online forums to improve online communication.; Professor Poesio is the recipient of an Advanced Grant from the European Research Council to analyze disagreements in language interpretation; the project DALI (http://www.dali-ambiguity.org); ,Much of Professor Poesio's recent work is concerned with applications of text analytics to real world problems; in particular through the analysis of language uses in social media. Examples include his work on detecting deception in communication (e.g.; on detecting deceptive reviews online); his work on supporting human rights organisations (e.g.; by identifying and assessing reports of human rights violations in social media); and his work on analysing discussions in online forums to improve online communication.; ,Professor Poesio is the recipient of an Advanced Grant from the European Research Council to analyze disagreements in language interpretation; the project DALI (http://www.dali-ambiguity.org); ,Professor Massimo Poesio 
matt-kusner,['N/A'],Matt J. Kusner was a visiting researcher at Cornell University under the supervision of Kilian Q. Weinberger and received his Ph.D. in Machine Learning from Washington University in St. Louis. His work is in the areas of privacy; budgeted learning; model compression; and Bayesian optimization. He is from Iowa City; Iowa; USA and is married to the wonderful Sonia Rego.  ; Matt's research aims to address the disconnect between state-of-the-art machine learning models; and models that are often used to solve real-world problems. Frequently; in real-world settings the modeller is confronted with a trade-off between maximising an objective (e.g.; returning more accurate results) and minimising a budget (e.g.; producing predictions in under a millisecond). His research considers three specific types of budgets: time; space; and privacy. These feature in recommendation; face recognition; bankruptcy prediction; stock market modelling; and real-time machine translation. Directly addressing these real-world trade-offs at an optimisation level results in algorithms that are simultaneously practical and accurate.; ,Matt's research aims to address the disconnect between state-of-the-art machine learning models; and models that are often used to solve real-world problems. Frequently; in real-world settings the modeller is confronted with a trade-off between maximising an objective (e.g.; returning more accurate results) and minimising a budget (e.g.; producing predictions in under a millisecond). His research considers three specific types of budgets: time; space; and privacy. These feature in recommendation; face recognition; bankruptcy prediction; stock market modelling; and real-time machine translation. Directly addressing these real-world trade-offs at an optimisation level results in algorithms that are simultaneously practical and accurate.; ,['N/A'],Dr Matt Kusner 
matthew-brett,Neuroscience; Programming languages; Research methods; Statistical methods & theory; ,"Matthew is a lecturer at the University of Birmingham. He qualified as a doctor in 1990; and trained in general medicine and neurology before starting his research career in functional brain imaging and the Hammersmith Hospital and Oxford University. He moved to the MRC Cognition and Brain Sciences Unit in 1996 to study brain imaging of movement; but his interests gradually shifted to analysis methods of brain images; and teaching brain imaging methods to psychologists and neuroscientists.; From 2003 to 2005; he worked at the University of California; Berkeley; where he first came across the principles of reproducible science; this led him to implement the first reproducible paper on brain imaging analysis (Aston; Turkheimer & Brett 2006). While at Berkeley he started working with Jarrod Millman; at the Brain Imaging Center; on new libraries for brain imaging analysis in Python; later funded with an NIH grant. He became a core contributor to SciPy; and developed his own libraries for brain imaging analysis; including Nibabel; that is now the base layer for other Python brain imaging libraries.; He returned to Cambridge in 2005; but went back to Berkeley in 2008 to continue his work on scientific Python and brain imaging. While at Berkeley; Matthew started to concentrate on teaching of brain imaging analysis; statistics and reproducibile research; and the central role of code in teaching basic ideas in imaging and statistics. He returned to the UK to work at the University of Birmingham; where his main focus is on teaching data science to undergraduates in life sciences and across the University.; Matthew's primary interests are in two related areas:; The term ""data science"" carries little meaning without context and it has been understood in many ways. Initial reports from industry characterised data scientists as scientists who had become much more effective in data analysis by using code. The first response from academia has been to characterise data science as ""a superset of the fields of statistics and machine learning which adds some technology for ‘scaling up’ to ‘big data’"" (Donoho 2015).; Does this response from academia capture what is really important in data science? Donoho argues strongly that it does not. Our understanding of data science will have fundamental implications for what we study and what we teach. For example; Berkeley's hugely successful undergraduate course on data science puts great emphasis on understanding data through code; with relatively little time devoted to machine learning; and none to techniques for analysing big data.; Matthew is working on methods for teaching data science to first-year undergraduates of any background; using the Berkeley course as a starting point. The textbook for his 2018-19 course is https://matthew-brett.github.io/dsfe. He also works on arguments for a productive definition of data science; see https://matthew-brett.github.io/dsfe/chapters/01/what-is-data-science for a summary.; ","Matthew's primary interests are in two related areas:; The term ""data science"" carries little meaning without context and it has been understood in many ways. Initial reports from industry characterised data scientists as scientists who had become much more effective in data analysis by using code. The first response from academia has been to characterise data science as ""a superset of the fields of statistics and machine learning which adds some technology for ‘scaling up’ to ‘big data’"" (Donoho 2015).; Does this response from academia capture what is really important in data science? Donoho argues strongly that it does not. Our understanding of data science will have fundamental implications for what we study and what we teach. For example; Berkeley's hugely successful undergraduate course on data science puts great emphasis on understanding data through code; with relatively little time devoted to machine learning; and none to techniques for analysing big data.; Matthew is working on methods for teaching data science to first-year undergraduates of any background; using the Berkeley course as a starting point. The textbook for his 2018-19 course is https://matthew-brett.github.io/dsfe. He also works on arguments for a productive definition of data science; see https://matthew-brett.github.io/dsfe/chapters/01/what-is-data-science for a summary.; ",['N/A'],Dr Matthew Brett 
matthew-leeke,['N/A'],Dr Leeke is an Associate Professor and Director of Admissions and Recruitment within the Warwick University Department of Computer Science.  ;  ; His research addresses a variety of issues relating to the design; implementation and evaluation of dependable systems. He is particularly interested in approaches for the design and location of efficient error detection mechanisms; as well a privacy issues in emerging areas such as wireless sensor networks.; ,His research addresses a variety of issues relating to the design; implementation and evaluation of dependable systems. He is particularly interested in approaches for the design and location of efficient error detection mechanisms; as well a privacy issues in emerging areas such as wireless sensor networks.; ,['N/A'],Dr Matthew Leeke 
melanie-smallman,Data science of government & politics; Ethics; Research methods; Social media; Social psychology; ,Melanie Smallman is a lecturer in Science and Technology Studies at UCL and Co-Director of the UCL Hub for Responsible Research and Innovation. Melanie's research focuses on how responsibility and ethics is understood and enacted in research and how these issues affect public perceptions and social acceptance of technologies. Specifically; she is interested in the role of technologies in increasing inequality and how these wider social impacts can be taken account of within ethical frameworks.; Her research utilises large datasets and computational approaches to qualitative data analysis and she is interested in developing these techniques further. Previously; Melanie ran science policy communication consultancy Think-Lab and spent eight years as an adviser within the UK Government. She is a former Fellow in Science; Technology and Society at the Harvard Kennedy School for Government and has a PhD in Science and Technology Studies at UCL.; AI and data science offers huge opportunities; but also raises important issues around ethics; responsibility and social acceptability. There is also growing evidence that digital and data technologies are helping create growing economic inequalities. As a Fellow at The Alan Turing Institute and a member of the Data Ethics Group; Melanie Smallman's research will examine these issues; in particular looking at the role of AI and data science in growing inequalities and how these issues can be taken account of within ethical frameworks. At the same time; Melanie draws upon data science and computational approaches to qualitative data analysis for her research.; During her time at the Turing; Melanie also hopes to collaborate with data and computational scientists to develop computational and AI approaches to humanities and social research further; ,AI and data science offers huge opportunities; but also raises important issues around ethics; responsibility and social acceptability. There is also growing evidence that digital and data technologies are helping create growing economic inequalities. As a Fellow at The Alan Turing Institute and a member of the Data Ethics Group; Melanie Smallman's research will examine these issues; in particular looking at the role of AI and data science in growing inequalities and how these issues can be taken account of within ethical frameworks. At the same time; Melanie draws upon data science and computational approaches to qualitative data analysis for her research.; During her time at the Turing; Melanie also hopes to collaborate with data and computational scientists to develop computational and AI approaches to humanities and social research further; ,['N/A'],Dr Melanie Smallman 
melissa-terras,Data structures; Communications; Information retrieval; Natural language processing; Pattern recognition; Ethics; Linguistics; Research methods; Social media; ,Melissa Terras is the Professor of Digital Cultural Heritage at the University of Edinburgh's College of Arts; Humanities; and Social Sciences (CAHSS); leading digital aspects of research within CAHSS; and Director of Research in the new Edinburgh Futures Institute. Her research focuses on the use of computational techniques to enable research in the arts; humanities; and wider cultural heritage and information environment that would otherwise be impossible. With a background in Classical Art History and English Literature (MA; University of Glasgow); and Computing Science (MSc IT with distinction in Software and Systems; University of Glasgow); her doctorate (Engineering; University of Oxford) examined how to use image processing and machine learning to interpret and read deteriorated Ancient Roman texts.; She is an Honorary Professor of Digital Humanities in UCL Department of Information Studies; where she was employed from 2003-2017; Directing UCL Centre for Digital Humanities from 2013. Books include 'Image to Interpretation: An Intelligent System to Aid Historians in Reading the Vindolanda Texts' (2006; Oxford University Press) and and 'Defining Digital Humanities: A Reader' (Ashgate 2013) which has been translated into Russian and Chinese. She is a Trustee of the National Library of Scotland; serves on the Board of Curators of the University of Oxford Libraries; is a Fellow of the Chartered Institute of Library and Information Professionals; and Fellow of the British Computer Society. You can generally find her on Twitter @melissaterras.; A growing source of large scale data is emerging from mass-digitisation programs within the Gallery; Library; Archive and Museum (GLAM) sector. However; most GLAM institutions have neither the expertise nor the resources to allow effective mining of this content that moves beyond basic keyword searching of OCRd text provided by standard search interfaces.; Melissa's work will:; While there are a range of technical barriers (OCR correction; non-standardised data management) as well as interdisciplinary issues (access to large scale compute has been difficult for humanities researchers as there is too steep a learning curve); the establishment of a shared resource of historical texts from GLAM partners would enable humanities research at the Turing to be truly supercharged; laying the groundwork for the secure sharing and text mining of in-copyright GLAM material (such as journal or web archive content) and can be seen as exploratory research which assists large scale mining of content while engaging with the technical practices and legal frameworks of the GLAM sector.; ,A growing source of large scale data is emerging from mass-digitisation programs within the Gallery; Library; Archive and Museum (GLAM) sector. However; most GLAM institutions have neither the expertise nor the resources to allow effective mining of this content that moves beyond basic keyword searching of OCRd text provided by standard search interfaces.; Melissa's work will:; While there are a range of technical barriers (OCR correction; non-standardised data management) as well as interdisciplinary issues (access to large scale compute has been difficult for humanities researchers as there is too steep a learning curve); the establishment of a shared resource of historical texts from GLAM partners would enable humanities research at the Turing to be truly supercharged; laying the groundwork for the secure sharing and text mining of in-copyright GLAM material (such as journal or web archive content) and can be seen as exploratory research which assists large scale mining of content while engaging with the technical practices and legal frameworks of the GLAM sector.; ,['N/A'],Professor Melissa Terras 
michael-barnes,Complexity (Algorithms); Artificial intelligence; Computer systems & architectures; Research methods; Time series; ,Michael Barnes leads a bioinformatics team working across the many areas of bioscience; including genomics; drug discovery; stratified medicine; high performance computing; machine learning and artificial intelligence and health informatics with a unified objective to drive forward translation into the clinic. Michael is also an HDR-UK Investigator. He is a co-investigator and leads data integration on several MRC stratified medicine projects; including MRC PSORT (Psoriasis); MRC RA-Map (RA); MRC MATURA (RA) and MRC CLUSTER (Juvenile Idiopathic Arthritis). He serves on the MRC Methodology Research Panel; UKRI Genomic Expert Panel and also advises on a number of project boards; including the MRC-eMedLab HPC facility; the IMI etriks project and the F1000 faculty.; Michael leads research at the Turing focused on the application of novel machine learning and AI algorithms to precision medicine and healthcare. He is also interested in explainable AI.  ; ,Michael leads research at the Turing focused on the application of novel machine learning and AI algorithms to precision medicine and healthcare. He is also interested in explainable AI.  ; ,['N/A'],Dr Michael Barnes PhD
michael-benedikt,Databases; ,Michael Benedikt received his PhD in mathematical logic from the University Wisconsin. He then spent a decade in industrial research at Bell Laboratories; working on computational logic; data management; and telecommunications. Since 2007 he has been a professor in Oxford University's computer science department.; Professor Benedikt's research spans automated reasoning; data management; with a particular interest in the interaction between the two topics.; ,Professor Benedikt's research spans automated reasoning; data management; with a particular interest in the interaction between the two topics.; ,['N/A'],Professor Michael Benedikt 
michael-castelle,Multi-agent systems; Neural networks; Communications; Databases; Operating systems; Computer vision; Data science of government & politics; Ethics; Linguistics; Research methods; ,"Michael Castelle is an Assistant Professor in the Centre for Interdisciplinary Methodologies at the University of Warwick. He received a PhD from the University of Chicago Department of Sociology in 2017; and also holds an Sc.B. in Computer Science from Brown University. His work draws from the fields of science and technology studies; machine learning; sociological theory; the history and philosophy of computing; economic sociology; and linguistic anthropology.; Explosive growth in the contemporary field of deep learning has an undeniable relevance not just for computer vision and machine translation researchers and practitioners; but also for social scientists and historians who will need to contextualize these developments for a broader audience. Michael's research goals are to find the potential ""translations"" between DL techniques and social scientific and humanistic methods; as well as discovering the implications of sociological; educational; and aesthetic theory (such as the work of Bourdieu; Piaget; and Vygotsky) for DL techniques; including the novel (and intriguingly 'social') architectures of generative adversarial networks.; ","Explosive growth in the contemporary field of deep learning has an undeniable relevance not just for computer vision and machine translation researchers and practitioners; but also for social scientists and historians who will need to contextualize these developments for a broader audience. Michael's research goals are to find the potential ""translations"" between DL techniques and social scientific and humanistic methods; as well as discovering the implications of sociological; educational; and aesthetic theory (such as the work of Bourdieu; Piaget; and Vygotsky) for DL techniques; including the novel (and intriguingly 'social') architectures of generative adversarial networks.; ",['N/A'],Dr Michael Castelle 
michael-catt,Artificial intelligence; Machine learning; Mathematical modelling; ,Michael Catt joined Newcastle University from industry as a professor of practice in 2009. Michael acquired considerable experience in the development of  in-vitro diagnostic systems at Unipath (now Swiss Precision Diagnostics); contributing IP; research and technical development to leading consumer brands for women's health (Clearblue; Clearplan) as well as point of care; rapid diagnostic systems for clinical immunoassay and microbiology. One of the consumer products employed embedded self-learning algorithms developed by Michael and his team to improve the identification of the fertile phase over successive cycles of use and are still used in the product today.; Michael moved to Unilever Corporate Research to lead the 'Healthy Ageing' programme; contributing to well known brands and to spin-out technologies and integrated systems for web/mobile lifestyle intervention and wearable monitors for research and consumer use (e.g. genea). A core focus of the Healthy Ageing programme was preservation of metabolic health through the life-course via lifestyle behaviour.  Michael has continued to progress these interests; contributing to the National Innovation Centre for Ageing; the Newcastle University spin-out 'Changing Health' and to a number of other multinational; start-up and spin-out companies focused on the use of biomarkers in health management. ; The recent UK Industrial Strategy Healthy Ageing Grand Challenge identifies the need to reduce inequalities in health trajectories and improve healthy life expectancy. Michael's Turing research is aligned to the technical realisation of these goals; bridging exploration of biomarkers in large well characterised cohorts; such as ukbiobank; to explore the fusion of non-invasive and other accessible biomarkers to inform lifestyle intervention strategies; the associated design and realisation of cloud/mobile; wearable and self-monitoring delivery systems with consequent evaluation and translation into practice.  ; ,The recent UK Industrial Strategy Healthy Ageing Grand Challenge identifies the need to reduce inequalities in health trajectories and improve healthy life expectancy. Michael's Turing research is aligned to the technical realisation of these goals; bridging exploration of biomarkers in large well characterised cohorts; such as ukbiobank; to explore the fusion of non-invasive and other accessible biomarkers to inform lifestyle intervention strategies; the associated design and realisation of cloud/mobile; wearable and self-monitoring delivery systems with consequent evaluation and translation into practice.  ; ,['N/A'],Professor Michael Catt 
michael-farber,['N/A'],Michael Farber is a Professor of Mathematics at Queen Mary University of London; he is a Turing Fellow and Turing University Lead for Queen Mary. He obtained his PhD and DSc degrees in the former USSR. Prior to joining Queen Mary in 2014; Michael Farber had professorial appointments at the universities of Warwick; Durham and Tel Aviv. He is an editor of several mathematical journals; among them “Applied and Computational Topology”. In 2004 he received a Royal Society Wolfson Research Merit Award. He is currently the Director of the Institute of Applied Data Science at Queen Mary. ; Michael Farber has broad research interests and experience in various areas of applied and pure mathematics. He worked in the Institute of Cybernetics in the former USSR leading a department for optimisation and control of large radio telescopes. His results in pure mathematics include the solution of two problems of R. H. Fox about 2-dimensional knotted spheres in 4-dimensional space and a problem of S. P. Novikov about critical points of multi-valued functions on manifolds. Another achievement is the solution of the “zero-in-the-spectrum” conjecture of M. Gromov. Since 2003 Professor Farber has worked in applied algebraic topology; stochastic topology and topological robotics. ; ,Michael Farber has broad research interests and experience in various areas of applied and pure mathematics. He worked in the Institute of Cybernetics in the former USSR leading a department for optimisation and control of large radio telescopes. His results in pure mathematics include the solution of two problems of R. H. Fox about 2-dimensional knotted spheres in 4-dimensional space and a problem of S. P. Novikov about critical points of multi-valued functions on manifolds. Another achievement is the solution of the “zero-in-the-spectrum” conjecture of M. Gromov. Since 2003 Professor Farber has worked in applied algebraic topology; stochastic topology and topological robotics. ; ,['N/A'],Professor Michael Farber 
michael-inouye,Neural networks; Graph theory; Causality; High dimensional inference; ,Dr Michael Inouye is a Principal Researcher at the Department of Public Health and Primary Care at the University of Cambridge; Head of Systems Genomics at the Baker Heart and Diabetes Institute; and the Director of the Cambridge Baker Systems Genomics Initiative.; His research interests lie at the interface of machine learning; biology and healthcare. He has worked extensively on computational methods to predict disease using human genomic data and how so-called genomic risk scores are best applied in clinical practice. Furthermore; he has a long track record in the analysis of high-dimensional omics data to identify the causal biomolecular networks of disease.; ,His research interests lie at the interface of machine learning; biology and healthcare. He has worked extensively on computational methods to predict disease using human genomic data and how so-called genomic risk scores are best applied in clinical practice. Furthermore; he has a long track record in the analysis of high-dimensional omics data to identify the causal biomolecular networks of disease.; ,['N/A'],Dr Michael Inouye 
michael-rovatsos,Ethics; Data science of government & politics; Cognitive science; Research methods; Social media; Logic (Theoretical mathematics); Human computer interface; Game theory; Multi-agent reasoning; Robotics; Multi-agent systems; Reinforcement learning; ,Michael Rovatsos is Professor of Artificial Intelligence at the University of Edinburgh and Director of the Bayes Centre; the  University's innovation centre for Data Science and AI. He obtained his PhD in Informatics from the Technical University of Munich in 2004; after which he went straight into a full-time academic position at Edinburgh. He has a track record of over 90 publications in AI; and has been involved in externally funded projects worth over £17m; out of which he has personally held £2.5m as PI.; He recently led technical work in the EPSRC-funded UnBias project; developing fair resource allocation algorithms and conducting empirical research into users' perceptions of algorithmic fairness. This work toward developing AI-assisted methods for ethical self-regulation of online platforms continues in the follow-up project ReEnTrust. He is an Associate Editor of the Knowledge and Information Systems Journal; was recently Blue Sky Track Co-Chair of the AAMAS 2018 conference; and is Conference Coordinator for ACM's AI Special Interest Group.; His research interests are in Artificial Intelligence with a specific focus on multiagent systems; automated planning; and human-friendly and ethical algorithm design. His recent involvement in large; interdisciplinary projects has led to a major shift of his research agenda toward ethical AI; developing intelligent decision-making algorithms and platform architectures that support the moral values of their stakeholders. In this context; he is particularly interested in diversity-awareness; i.e. the ability of systems to deal with conflicting user preferences regarding the properties that algorithmic decisions made by these systems should exhibit.; ,His research interests are in Artificial Intelligence with a specific focus on multiagent systems; automated planning; and human-friendly and ethical algorithm design. His recent involvement in large; interdisciplinary projects has led to a major shift of his research agenda toward ethical AI; developing intelligent decision-making algorithms and platform architectures that support the moral values of their stakeholders. In this context; he is particularly interested in diversity-awareness; i.e. the ability of systems to deal with conflicting user preferences regarding the properties that algorithmic decisions made by these systems should exhibit.; ,['N/A'],Dr Michael Rovatsos 
michal-branicki,['N/A'],Michal works on the interface of applied probability; information theory and dynamical systems with applications to Bayesian data assimilation; Bayesian learning; stochastic control; and data-driven dimension reduction in stochastic systems. He is particularly interested in theoretical and computational aspects of probabilistic approaches to prediction and uncertainty quantification in dynamical systems; and methods for Bayesian time-dependent inverse problems in high-dimensions. These themes require a systematic integration of data-driven and model-based techniques for state estimation and classification problems from large sets of noisy data in order to maximise information flow from the available data to model dynamics.; Provably robust techniques for data-driven construction of predictive methods for state estimation and classification from large sets of noisy data sets embedded in high-dimensions. The overarching approach relies on the synergy between diffusion maps; topological data analysis and dimension reduction; here; topological tools naturally complement manifold learning techniques based on concepts from harmonic analysis and information geometry. Two focus areas are: (i) Ranking of discrete data sets with incomplete information in a ‘small & noisy data’ regime (applications to latent topic modelling); (ii) Consistent manifold recovery for topological data analysis; spectral clustering; and equation-free modelling for data assimilation.  ; ,Provably robust techniques for data-driven construction of predictive methods for state estimation and classification from large sets of noisy data sets embedded in high-dimensions. The overarching approach relies on the synergy between diffusion maps; topological data analysis and dimension reduction; here; topological tools naturally complement manifold learning techniques based on concepts from harmonic analysis and information geometry. Two focus areas are: (i) Ranking of discrete data sets with incomplete information in a ‘small & noisy data’ regime (applications to latent topic modelling); (ii) Consistent manifold recovery for topological data analysis; spectral clustering; and equation-free modelling for data assimilation.  ; ,['N/A'],Dr Michal Branicki 
michelle-morris,Databases; Pattern recognition; Research methods; Causality; Simulation; Time series; Probability; ,Michelle Morris is a University Academic Fellow in the School of Medicine at the University of Leeds; based in the Leeds Institute for Data Analytics. She is an interdisciplinary researcher with a background spanning: health informatics; geography; nutritional epidemiology and health economics. Her primary research interests are in spatial and social variations in diet; lifestyle and health and how new and emerging forms of data can be best utilised to understand these.; She leads a team focused on the use of new forms of 'big' and spatial data in health research; working closely with industry partners on food and activity data. Graduating with a neuroscience degree in 2002; she began a career in health informatics; working in a graduate position at EMIS; one of the UK's leading healthcare clinical system providers; where she gained international project management experience. She returned to study for a MSc; equipping her in statistics and epidemiology training (2009) and completed an interdisciplinary PhD investigating 'Spatial analysis of dietary cost patterns and implications for health' (2013); followed by postdoctoral positions in both nutritional epidemiology and consumer data research.; Currently; she directs the multidisciplinary ESRC Strategic Network for Obesity and has developed a diverse teaching portfolio; including spatial analytics and visualisation for health. With this unique career history she is well placed to achieve her vision to cross discipline boundaries bringing together people; data and methods to improve health through informatics - specifically combining consumer analytics with health informatics and using 'big data' to benefit patient outcomes.; Michelle's Turing related research will build upon and develop new approaches to using digital lifestyle data in health research. Digital lifestyle data might include supermarket food purchase transactions; as recorded using loyalty cards; or physical activity levels recorded through fitness trackers or wearable devices. Arguably the most important aspect of this work is understanding public opinion on the use of their information in this way. To do this; she will survey approximately 10;000 adults in the UK - for more information visit the survey at lida.leeds.ac.uk/research/lifeinfo/; As these types of digital lifestyle data were not originally collected for the purpose of health research it is important to understand if they are valuable and reliable to be used for this purpose. Therefore; Michelle proposes to validate these new data sources against the more traditional methods such as diet and activity diaries. Through collaborations with the Turing network it will be possible to apply new data science methods to these new data to generate new insights into lifestyle behaviours and related health outcomes.; ,Michelle's Turing related research will build upon and develop new approaches to using digital lifestyle data in health research. Digital lifestyle data might include supermarket food purchase transactions; as recorded using loyalty cards; or physical activity levels recorded through fitness trackers or wearable devices. Arguably the most important aspect of this work is understanding public opinion on the use of their information in this way. To do this; she will survey approximately 10;000 adults in the UK - for more information visit the survey at lida.leeds.ac.uk/research/lifeinfo/; As these types of digital lifestyle data were not originally collected for the purpose of health research it is important to understand if they are valuable and reliable to be used for this purpose. Therefore; Michelle proposes to validate these new data sources against the more traditional methods such as diet and activity diaries. Through collaborations with the Turing network it will be possible to apply new data science methods to these new data to generate new insights into lifestyle behaviours and related health outcomes.; ,['N/A'], Michelle Morris 
miguel-rodrigues,Neural networks; Deep learning; Uncertainty quantification; High dimensional inference; Estimation theory; ,Miguel Rodrigues is a Reader in the Department of Electronic and Electrical Engineering; University College London; London; UK. He was previously with the Department of Computer Science at the University of Porto; Portugal; rising through the ranks from Assistant to Associated Professor. He has also held various appointments with institutions worldwide including Princeton University; the University of Cambridge and Duke University. He obtained an undergraduate degree in Electrical and Computer Engineering from the Faculty of Engineering of the University of Porto; Portugal and the Ph.D. degree in Electronic and Electrical Engineering from University College London.; Miguel's research lies in the general area of mathematics of information; information theory; and information processing.; He will work on a variety of topics at the Turing including: (1) information-theoretic foundations of privacy (2) information-theoretic foundations of security and (3) information-theoretic foundations of deep learning algorithms.; ,Miguel's research lies in the general area of mathematics of information; information theory; and information processing.; He will work on a variety of topics at the Turing including: (1) information-theoretic foundations of privacy (2) information-theoretic foundations of security and (3) information-theoretic foundations of deep learning algorithms.; ,['N/A'], Miguel Rodrigues 
mihaela-van-der-schaar,Machine learning; Uncertainty quantification; ,Professor van der Schaar is John Humphrey Plummer Professor of Machine Learning; Artificial Intelligence and Medicine at the University of Cambridge and a Turing Fellow at The Alan Turing Institute in London; where she leads the effort on data science and machine learning for personalised medicine. She is an IEEE Fellow (2009). She has received the Oon Prize on Preventative Medicine from the University of Cambridge (2018).  She has also been the recipient of an NSF Career Award; 3 IBM Faculty Awards; the IBM Exploratory Stream Analytics Innovation Award; the Philips Make a Difference Award and several best paper awards; including the IEEE Darlington Award. She holds 35 granted USA patents.; The current emphasis of her research is on machine learning with applications to medicine; finance and education. She has also worked on data science; network science; game theory; signal processing; communications; and multimedia. Prior to her academic career; she was a Senior Researcher at Philips Research in the Netherlands and USA; from this work she holds 33 patents.; ,The current emphasis of her research is on machine learning with applications to medicine; finance and education. She has also worked on data science; network science; game theory; signal processing; communications; and multimedia. Prior to her academic career; she was a Senior Researcher at Philips Research in the Netherlands and USA; from this work she holds 33 patents.; ,['N/A'],Professor Mihaela van der Schaar 
mihai-cucuringu,['N/A'],Mihai received his PhD in Applied and Computational Mathematics (PACM) at Princeton University in 2012; supervised by Amit Singer. His thesis was on the low-rank matrix completion problem and several distance geometry problems with applications to sensor network localization and three-dimensional structuring of molecules. During 2013-2016 he was a CAM Assistant Adjunct Professor in Computational Applied Mathematics at UCLA. During Fall 2014; he was a Research Fellow at the Simons Institute for Theory of Computing at UC Berkeley; in the program Algorithmic Spectral Graph Theory.; Mihai's research interests concern the development and mathematical analysis of algorithms for large networks; certain inverse problems on graphs; and big data analysis; with applications to various problems in engineering; machine learning; finance; and biology. Particular areas of interest are spectral and SDP-relaxation algorithms and applications; the group synchronisation problem; ranking from noisy pairwise comparisons; lead-lag relationships in multivariate time series; clustering; core-periphery structure in networks; multiplex networks; dimensionality reduction and diffusion maps (with an eye towards heterogeneous data and nonlinear time series); spectral algorithms for analysis of signed graphs and correlation networks. The above problems share an important feature: they can all be solved by exploiting the spectrum of their corresponding graph Laplacian.; ,Mihai's research interests concern the development and mathematical analysis of algorithms for large networks; certain inverse problems on graphs; and big data analysis; with applications to various problems in engineering; machine learning; finance; and biology. Particular areas of interest are spectral and SDP-relaxation algorithms and applications; the group synchronisation problem; ranking from noisy pairwise comparisons; lead-lag relationships in multivariate time series; clustering; core-periphery structure in networks; multiplex networks; dimensionality reduction and diffusion maps (with an eye towards heterogeneous data and nonlinear time series); spectral algorithms for analysis of signed graphs and correlation networks. The above problems share an important feature: they can all be solved by exploiting the spectrum of their corresponding graph Laplacian.; ,['N/A'],Dr Mihai Cucuringu 
mike-davies,Algorithms; Information theory (Applied mathematics); Machine learning; High dimensional inference; ,Mike E. Davies holds the Jeffrey Collins Chair in Signal and Image Processing at  the University of Edinburgh; where he also leads the Edinburgh Compressed Sensing Research Group; and was Head of the Institute for Digital Communications (IDCOM) 2013-2016. He received an M.A. in engineering from Cambridge University in 1989 where he was awarded a Foundation Scholarship (1987); and a Ph.D. degree in nonlinear dynamics from University College London (UCL) in 1993.; He was awarded a Royal Society University Research Fellowship in 1993 and was a Texas Instruments Distinguished Visiting Professor at Rice University in 2012. He leads the UK University Defence Research Collaboration (UDRC) programme on signal processing for defence in collaboration with the U.K. Defence Science and Technology Laboratory (DSTL) and is a recipient of a European Research Council (ERC) advanced grant on Computational Sensing.; His research focuses on sparse representations; compressed sensing; and the exploitation of low dimensional models in computation sensing and signal processing. He has explored the application of these ideas to various advanced medical imaging and RF based sensing applications; and has an active interest in the related topics of machine learning; high-dimensional statistics and information theory.; Mike Davies was elected a Fellow of IEEE (2015); Royal Academy of Engineering (2017) and Royal Society of Edinburgh (2018) for his contributions to sparse signal processing and compressed sensing ; ,His research focuses on sparse representations; compressed sensing; and the exploitation of low dimensional models in computation sensing and signal processing. He has explored the application of these ideas to various advanced medical imaging and RF based sensing applications; and has an active interest in the related topics of machine learning; high-dimensional statistics and information theory.; ,Mike Davies was elected a Fellow of IEEE (2015); Royal Academy of Engineering (2017) and Royal Society of Edinburgh (2018) for his contributions to sparse signal processing and compressed sensing ; ,Professor Mike Davies FIEEE; FREng; CEng
mike-wald,Artificial intelligence; Communications; Human computer interface; Applications (Machine learning); Computer vision; Deep learning; Natural language processing; Speech recognition; Privacy & trust; Ethics; ,Professor Mike Wald leads research into accessible technologies in the Web and Internet Science Group; ECS; University of Southampton. He has had many years of experience working with disabled users in research; development; support and provision of knowledge in the areas of disability; assistive technology and digital accessibility.; He was a founder member of the International Liberated Learning Consortium that included leading universities and organisations investigating how speech recognition captioning and transcription could be enhanced. Over 20;000 people in more than 150 countries have enrolled on his online inclusion and digital accessibility courses.; Professor Wald's research seeks to understand the design and deployment of AI to benefit all members of society; including traditionally underserved communities. The global assistive technology market is set to exceed $26 billion by 2024; but in many low and middle-income countries there is limited access to these technologies. However; the growing use of AI in mobile; free and open source applications and knowledge sharing of expertise could help to achieve the World Health Organisation’s goal for Universal Health Coverage which “can be advanced inclusively only if people are able to access quality assistive products when and where they need them”.; The impact that AI will have on education and employment in the future is undeniable; but AI research could be a force for good for the more than a billion people who live with some form of disability as long as they are not marginalised. The impact of this work could become part of the World Health Organisation’s drive to address the “unmet need of assistive products [that] is crucial to achieve the Sustainable Development Goals; to provide Universal Health Coverage; and to implement the UN Convention on the Rights of Persons with Disabilities”; ,Professor Wald's research seeks to understand the design and deployment of AI to benefit all members of society; including traditionally underserved communities. The global assistive technology market is set to exceed $26 billion by 2024; but in many low and middle-income countries there is limited access to these technologies. However; the growing use of AI in mobile; free and open source applications and knowledge sharing of expertise could help to achieve the World Health Organisation’s goal for Universal Health Coverage which “can be advanced inclusively only if people are able to access quality assistive products when and where they need them”.; The impact that AI will have on education and employment in the future is undeniable; but AI research could be a force for good for the more than a billion people who live with some form of disability as long as they are not marginalised. The impact of this work could become part of the World Health Organisation’s drive to address the “unmet need of assistive products [that] is crucial to achieve the Sustainable Development Goals; to provide Universal Health Coverage; and to implement the UN Convention on the Rights of Persons with Disabilities”; ,['N/A'],Professor Mike Wald 
mingli-chen,['N/A'],['N/A'],['N/A'],['N/A'],Dr Mingli Chen 
mirco-musolesi,Multi-agent systems; Artificial intelligence; Human computer interface; Computing networks; Applications (Machine learning); Reinforcement learning; Social networks; Social media; Social psychology; ,Mirco Musolesi is a Reader in Data Science at the Department of Geography at UCL; where he leads the Intelligent Social Systems Lab. He received a PhD in Computer Science from UCL and a Master in Electronic Engineering from the University of Bologna. Before joining UCL; he held research and teaching positions at Dartmouth College; Cambridge; St Andrews and Birmingham.; The focus of the work of Mirco's lab is on the design of next-generation intelligent systems based on computational models of human behaviour and social dynamics. Current areas of interest include: AI and machine learning for mobile and ubiquitous systems; reinforcement learning and multi-agent systems; computational and mathematical models of human behaviour and social systems (using mobile data; sensor and Internet of Things data; social media and other user-generated content; etc.) and their applications; and AI and machine learning techniques applied to personal data (in particular data from mobile devices and social media); including their privacy implications.; ,The focus of the work of Mirco's lab is on the design of next-generation intelligent systems based on computational models of human behaviour and social dynamics. Current areas of interest include: AI and machine learning for mobile and ubiquitous systems; reinforcement learning and multi-agent systems; computational and mathematical models of human behaviour and social systems (using mobile data; sensor and Internet of Things data; social media and other user-generated content; etc.) and their applications; and AI and machine learning techniques applied to personal data (in particular data from mobile devices and social media); including their privacy implications.; ,['N/A'],Dr Mirco Musolesi 
murray-pollock,Data science of government & politics; Differential privacy; Probability; Simulation; Monte Carlo methods; High dimensional inference; Causality; Uncertainty quantification; Stochastic optimisation; Parallel computing; ,Murray Pollock is an Assistant Professor of Statistics at the University of Warwick. Prior to this he was a Research Fellow on 'Intractable Likelihood: New Challenges from Modern Applications' (a project held jointly between Bristol; Lancaster; Oxford and Warwick universities); and completed his Ph.D. at Warwick (under the supervision of Adam Johansen and Gareth Roberts). Before entering academia he worked as an actuary in the US (Philadelphia) and Spain (Madrid).; Murray's current research is primarily in computational statistics and data science; and is broadly concerned with addressing practical constraints that arise in modern inferential application settings (for instance algorithmic scalability with data size; working under data privacy constraints; and methodological design in parallel and distributed computing environments). Research interests include cryptography; machine learning; Monte Carlo methodology; perfect simulation; risk modelling and stochastic differential equations.; ,Murray's current research is primarily in computational statistics and data science; and is broadly concerned with addressing practical constraints that arise in modern inferential application settings (for instance algorithmic scalability with data size; working under data privacy constraints; and methodological design in parallel and distributed computing environments). Research interests include cryptography; machine learning; Monte Carlo methodology; perfect simulation; risk modelling and stochastic differential equations.; ,['N/A'],Dr Murray Pollock 
nadia-papamichail,Operations research; Visualisation (Computer systems & architectures); Natural language processing; Ethics; Management science; Research methods; ,Nadia Papamichail is a Senior Lecturer in Information and Decision Systems at the University of Manchester. She holds a PhD in Computer Science from the University of Manchester and an MSc in Information Systems from the University of Leeds.; Nadia has a track record of inter-disciplinary research funded by the EPSRC and Innovate UK. She works on the interface of decision analysis; behaviour and support. She is the theme lead of Decision Analytics and Behaviour at the Decision and Cognitive Sciences Research Centre and an academic director of the Law and Technology Initiative (LaTi). She is the elected Chair of DASIG; a network of practitioners and academics which runs under the auspices of the Operational Research Society and seeks to promote the discipline of decision analysis in the UK and abroad.; Nadia’s Turing related work focuses on developing tools with explainable and interpretable outputs. She is particularly interested in decision analytic tools that combine inputs from decision makers; stakeholders and models. Even though explanatory capabilities do not improve the quality or accuracy of a system’s results; they add transparency into the algorithmic process; improve traceability and assure users that the reasoning behind the system is robust.; ,Nadia’s Turing related work focuses on developing tools with explainable and interpretable outputs. She is particularly interested in decision analytic tools that combine inputs from decision makers; stakeholders and models. Even though explanatory capabilities do not improve the quality or accuracy of a system’s results; they add transparency into the algorithmic process; improve traceability and assure users that the reasoning behind the system is robust.; ,['N/A'],Dr Nadia Papamichail 
nasir-rajpoot,['N/A'],Nasir Rajpoot is Professor in Computer Science at Warwick; where he started his academic career as an Assistant Professor in 2001; and the Royal Society Wolfson Research Merit Award holder since Sep 2017. He holds an Honorary Scientist position and serves as the Academic Lead for Digital Pathology Centre of Excellence at the University Hospitals Coventry & Warwickshire NHS Trust. Rajpoot is the founding Head of Tissue Image Analytics (TIA) lab at Warwick since 2012. The focus of current research in his lab is on developing novel computational pathology algorithms for improved cancer diagnostics and better stratification of cancer patients.  ; Modern day slide scanners are capable of generating large microscopic resolution images of conventional tissue slides; spurring a revolution in the practice of cellular pathology as a discipline. This development comes at a time when computing capacity and machine learning technologies are peaking; offering a remarkable opportunity to reveal complex cellular patterns in a data-driven manner. Rajpoot’s research capitalises on this opportunity to seek answers to questions like: Can we develop novel efficient image based measures of the ‘state of play’ of complex diseases such as cancer? And can we use such measures to further our understanding of cancer and predict the progression and outcome of cancer?; ,Modern day slide scanners are capable of generating large microscopic resolution images of conventional tissue slides; spurring a revolution in the practice of cellular pathology as a discipline. This development comes at a time when computing capacity and machine learning technologies are peaking; offering a remarkable opportunity to reveal complex cellular patterns in a data-driven manner. Rajpoot’s research capitalises on this opportunity to seek answers to questions like: Can we develop novel efficient image based measures of the ‘state of play’ of complex diseases such as cancer? And can we use such measures to further our understanding of cancer and predict the progression and outcome of cancer?; ,['N/A'],Professor Nasir Rajpoot 
natalia-bochkina,['N/A'],Natalia Bochkina joined the University of Edinburgh as a Lecturer in Statistics in 2007. In 2003-2007 she was a postdoc at the Biostatistics group at the Imperial College London working on the collaborative project building a biological atlas of insulin resistance. In 2002-2003 she was a statistician in biopharmaceutical company Oxford GlycoSciences (Ltd); completing her PhD in 2002 at the University of Bristol.  ; Natalia is interested in understanding theoretical properties of decision-making under uncertainty which arises in statistics and in machine learning motivated by practical applications. Currently her main areas of research are study of the rate of convergence and local approximation of posterior distribution (Bernstein-von Mises theorem) in Bayesian nonparametric; semi-parametric and high dimensional models which can be misspecified; non-regular or ill-posed; including inverse problems.; In additional to theory; she is interested in statistical modelling of high throughput genomic data; including the current project on estimating gene interaction networks using graphical models. An exciting emerging area is taking into the account computational complexity when studying efficiency of the decision-making. One of the tools is calibration of faster approximate Bayesian models using techniques for misspecified models to achieve best possible efficiency.; ,Natalia is interested in understanding theoretical properties of decision-making under uncertainty which arises in statistics and in machine learning motivated by practical applications. Currently her main areas of research are study of the rate of convergence and local approximation of posterior distribution (Bernstein-von Mises theorem) in Bayesian nonparametric; semi-parametric and high dimensional models which can be misspecified; non-regular or ill-posed; including inverse problems.; In additional to theory; she is interested in statistical modelling of high throughput genomic data; including the current project on estimating gene interaction networks using graphical models. An exciting emerging area is taking into the account computational complexity when studying efficiency of the decision-making. One of the tools is calibration of faster approximate Bayesian models using techniques for misspecified models to achieve best possible efficiency.; ,['N/A'],Dr Natalia Bochkina 
nathan-griffiths,Multi-agent systems; Multi-agent reasoning; Neural networks; Neural & evolutionary computing; Reinforcement learning; Ethics; ,Nathan Griffiths is a Professor in Computer Science and a Royal Society Industry Fellow (2015-2019). He is principal investigator for JASPR: Justified Assessment of Service Provider Reputation (£274K; EPSRC EP/M012662/1); TASCC: The Cooperative Car (£2M; EPSRC EP/N012380/1 and Jaguar Land Rover); and several smaller projects supporting PhD students investigating reputation; machine learning and data science (supported by Jaguar Land Rover; TRL; and EPSRC). The JASPR project is developing technologies for including circumstance-specific information in reputation assessments. TASCC: The Cooperative Car is developing a platform to manage; process and learn from interconnected vehicle data.; His primary research areas are multi-agent systems; trust and reputation; social network analysis; and machine learning. Currently; his research is focused around supporting cooperation through reputation and provenance; machine learning for intelligent vehicle applications including mining telemetry and mobility data; conventions and norm emergence; and influence manipulation in social networks. His work on the fellowship focuses on (i) the emergence and manipulation of conventions and norms; and (ii) modelling and predicting individual pattern-of-life (including mobility and behaviour); in both cases considering issues of fairness and transparency.; ,His primary research areas are multi-agent systems; trust and reputation; social network analysis; and machine learning. Currently; his research is focused around supporting cooperation through reputation and provenance; machine learning for intelligent vehicle applications including mining telemetry and mobility data; conventions and norm emergence; and influence manipulation in social networks. His work on the fellowship focuses on (i) the emergence and manipulation of conventions and norms; and (ii) modelling and predicting individual pattern-of-life (including mobility and behaviour); in both cases considering issues of fairness and transparency.; ,['N/A'],Professor Nathan Griffiths 
nathanael-fijalkow,['N/A'],"Nathanaël Fijalkow obtained his PhD in 2015 in Computer Science; entitled ""Counting and Randomising in Automata Theory""; jointly awarded by Paris 7 and Warsaw University under the supervision of Thomas Colcombet and Mikołaj Bojańczyk. He joined the University of Oxford in 2016 as a Research Assistant in the Verification group. His research interests are in Logics; Automata Theory; Games; Stochastic Systems; Formal Methods; Verification and Complexity.  ; Nathanaël's Research Fellowship at The Alan Turing Institute will allow him occasion to broaden his perspectives; he plan to work on Streaming Algorithms; Robustness for Stochastic Systems; Probabilistic Databases and Machine Learning. His ambition is to provide theoretical foundations for these questions; and to demonstrate their values through practical applications.; ",Nathanaël's Research Fellowship at The Alan Turing Institute will allow him occasion to broaden his perspectives; he plan to work on Streaming Algorithms; Robustness for Stochastic Systems; Probabilistic Databases and Machine Learning. His ambition is to provide theoretical foundations for these questions; and to demonstrate their values through practical applications.; ,['N/A'],Dr Nathanaël Fijalkow 
neave-oclery,Data science of government & politics; Research methods; Control theory; Robotics; Dynamical systems & differential equations; Multi-agent systems; Mathematical physics; Graph theory; ,Originally from Dublin; Neave O'Clery is currently Associate Professor in Applied Urban Science at UCL. She was previously a Senior Research Fellow at the Mathematical Institute at the University of Oxford; and prior that a Fulbright Scholar and Postdoctoral Research Fellow at the Center for International Development at the Harvard Kennedy School following her PhD (mathematics) at Imperial College. Neave is also founder and Editor in Chief of Angle; an online journal focusing on the intersection of policy; politics and science since 2009. Her current research focuses on data and network modelling of economic development and urban systems.; Globally; a consensus has emerged that emphasises the role of cities; rather than countries; as key engines of economic growth. A new project; funded by the Economic Data Science Program at the Alan Turing Institute; seeks to deploy a novel data modelling approach to shed light on some key economic policy issues facing UK cities; taking a skills-based perspective on industrial diversification and resilience. We will use this model to investigate the future diversification potential of individual cities; and their capacity to respond - in terms of cross-sectoral labour mobility - to challenges such as Brexit and automation. Working alongside policy-makers from inception; we will contribute to the evidence base for industrial and skills policy-making at a local and national level; ,Globally; a consensus has emerged that emphasises the role of cities; rather than countries; as key engines of economic growth. A new project; funded by the Economic Data Science Program at the Alan Turing Institute; seeks to deploy a novel data modelling approach to shed light on some key economic policy issues facing UK cities; taking a skills-based perspective on industrial diversification and resilience. We will use this model to investigate the future diversification potential of individual cities; and their capacity to respond - in terms of cross-sectoral labour mobility - to challenges such as Brexit and automation. Working alongside policy-makers from inception; we will contribute to the evidence base for industrial and skills policy-making at a local and national level; ,['N/A'],Dr Neave O'Clery 
neil-stewart,Social data science; Social psychology; ,Neil Stewart studied Natural Sciences and Experimental Psychology at Cambridge before moving to Warwick as a Psychology PhD student in 1997. His work as a Postdoc; Lecturer and Reader in Psychology was in perceptual decision making and categorisation. More recently as a Professor of Psychology he has worked on topics in behavioural and economic science. In 2017 he joined Warwick Business School as a Professor of Behavioural Science. ; He works in the field of behavioural and economic science; and applies this research to problems in the real world. Right now he is working on consumer decision making using credit card transaction data; on criminal and other bad behaviour using crime and incident records; and on a mathematical model of consumer decision making called decision by sampling. He uses a mixture of laboratory experiments; field experiments; and data science techniques applied to large data sets.; ,He works in the field of behavioural and economic science; and applies this research to problems in the real world. Right now he is working on consumer decision making using credit card transaction data; on criminal and other bad behaviour using crime and incident records; and on a mathematical model of consumer decision making called decision by sampling. He uses a mixture of laboratory experiments; field experiments; and data science techniques applied to large data sets.; ,['N/A'],Professor Neil Stewart 
neil-walton,Stochastic (Mathematical modelling); Probability; ,Neil is head of probability and statistics and a reader at the University of Manchester's School of Mathematics.; ,['N/A'],['N/A'],Dr Neil Walton 
neil-white,Applied mathematics; Machine learning; ,Professor Neil White was Head of the School of Electronics and Computer Science from 2011 to 2015 and is currently Co-Director of the ECS Centre for Health Technologies. He obtained a PhD from the University of Southampton in 1988 for a thesis describing the piezoresistive effect in thick-film resistors. A paper based on this work was awarded the 1989 Educational Prize from the International Society for Hybrid Microelectronics (ISHM). Neil was appointed as a Lecturer within the School in 1990 and promoted to Senior Lecturer in 1999; Reader in 2000 and was awarded a personal Chair in 2002. He is co-author of the book Intelligent Sensor Systems; which was first published by the Institute of Physics Publishing in 1994. He is also co-author of the book MEMS: Mechanical Sensors; published by Artech House.; He lectures on digital electronics; electronic measurement techniques and advanced instrumentation and sensors. He is a Chartered Engineer; Fellow of the IET; Senior Member of the IEEE; Fellow of the IoP and a Chartered Physicist. He is a member of the Peer Review College for the EPSRC and is on the Editorial Board of the international journals Sensor Review and Journal of Materials Science: Materials in Electronics. He is also a Series Editor for the Integrated Microsystems series for Artech House. He has published over 200 scientific papers in the area of sensors and instrumentation systems and holds 10 patents. He is a former Director and co-founder of the University spin-out company Perpetuum Ltd.; which specialises in vibration energy harvesting. He was the recipient of the 2009 Callendar silver Medal; awarded by the Institute of Measurement and Control for his 'outstanding contribution to the art of instruments or measurement'.; Neil's research interests include thick-film sensors; intelligent instrumentation; MEMS; self-powered microsensors and sensor networks.; ,Neil's research interests include thick-film sensors; intelligent instrumentation; MEMS; self-powered microsensors and sensor networks.; ,['N/A'],Professor Neil White 
niccolo-tempini,Data structures; Ethics; Data science of government & politics; Research methods; Social media; Databases; Knowledge representation; Systems theory; Applications (Machine learning); ,Niccolò Tempini is Senior Lecturer in Data Studies at the University of Exeter; Department of Sociology; Philosophy and Anthropology; and a Turing Fellow at The Alan Turing Institute. He is an interdisciplinary social scientist interested in questions of information; data; technology; organisation; value and knowledge. He researches big data research and digital infrastructures; investigating the specific knowledge production economies; organisation forms and data management innovations that these projects engender with a focus in their social and epistemic consequences.; He studies the practices of data scientists; software developers; researchers and non-professionalised experts; to understand how different forms of knowledge and value intersect with each other when different actors come to grips with new methods and new forms of data; information technology and organisation. His research has been published in international journals across science and technology studies; information systems; sociology and philosophy. More information at www.tempini.info.; For a few years; Niccolò has been investigating the ways in which people and institutions use data and information technology to know something about the social and personal life of a population or an individual; and what organisational trade-offs and social consequences each innovative approach presents. Issues related to the value; management; control; governance and security of data and Internet networks are today at the centre of some of the most current debates in the public sphere. Niccolò has been studying; among others; various issues connected to the collection; governance and reuse of data such as change in work roles and problem domain; the organisation of interdisciplinary research projects; the shifting boundaries of formal organisations and institutions; and the associated issues of trust; accountability and security; and changes in the value; usability and meaningfulness of data as they move across situations. His future research will focus on the social and epistemic conditions in machine learning development for health care and research; and in the design of 'soft' methodologies for ML technology design and development. Cutting edge data innovations succeed when they sit at the crossroads of complex convergences of heterogeneous requirements (interdisciplinary; communicational; methodological; social; ethical). In this research Niccolò will aim to observe and explain how machine learning is introduced in contexts of health care and research; and the gap between development and adoption. He will aim to develop a “methodological toolkit” that can help to factor social and contextual conditions in machine learning technology development.; ,For a few years; Niccolò has been investigating the ways in which people and institutions use data and information technology to know something about the social and personal life of a population or an individual; and what organisational trade-offs and social consequences each innovative approach presents. Issues related to the value; management; control; governance and security of data and Internet networks are today at the centre of some of the most current debates in the public sphere. Niccolò has been studying; among others; various issues connected to the collection; governance and reuse of data such as change in work roles and problem domain; the organisation of interdisciplinary research projects; the shifting boundaries of formal organisations and institutions; and the associated issues of trust; accountability and security; and changes in the value; usability and meaningfulness of data as they move across situations. His future research will focus on the social and epistemic conditions in machine learning development for health care and research; and in the design of 'soft' methodologies for ML technology design and development. Cutting edge data innovations succeed when they sit at the crossroads of complex convergences of heterogeneous requirements (interdisciplinary; communicational; methodological; social; ethical). In this research Niccolò will aim to observe and explain how machine learning is introduced in contexts of health care and research; and the gap between development and adoption. He will aim to develop a “methodological toolkit” that can help to factor social and contextual conditions in machine learning technology development.; ,['N/A'],Dr Niccolò Tempini 
nicholas-fuggle,Unsupervised learning; Time series; ,Nicholas Fuggle is a Dunhill Medical Trust research fellow and rheumatology regsitrar at the MRC Lifecourse Epidemiology Unit (LEU); University of Southampton. After completing his medical degree and intercalated BSc at Imperial College and a clinical research fellowship at St George’s University London he was awarded an NIHR academic clinical fellowship in rheumatology at the MRC LEU.;  ;  ;  ; His current research centers on the epidemiology of musculoskeletal diseases within the Hertfordshire Cohort (a group of community-dwelling older adults) and focuses on the epigenetic determinants of accelerated ageing and using imaging processing to analyse musculoskeletal imaging modalities.; World Congress of Osteoporosis; Young Investigator Award (Krakow; 2018); National Osteoporosis Society; New Investigator Award (Birmingham; 2018); World Congress of Osteoporosis; Young Investigator Award (Florence; 2017); Bone Research Society; Young Investigator Award (Bristol; 2017); World Congress of Dermatology; Scholar (Vancouver; 2015); ,His current research centers on the epidemiology of musculoskeletal diseases within the Hertfordshire Cohort (a group of community-dwelling older adults) and focuses on the epigenetic determinants of accelerated ageing and using imaging processing to analyse musculoskeletal imaging modalities.; ,World Congress of Osteoporosis; Young Investigator Award (Krakow; 2018); National Osteoporosis Society; New Investigator Award (Birmingham; 2018); World Congress of Osteoporosis; Young Investigator Award (Florence; 2017); Bone Research Society; Young Investigator Award (Bristol; 2017); World Congress of Dermatology; Scholar (Vancouver; 2015); ,Dr Nicholas Fuggle 
nicholas-loman,Data structures; Neural networks; Communications; Databases; Parallel computing; Information retrieval; Real time computing; Deep learning; Pattern recognition; Graph theory; Hardware optimisation (FPGA/GPU); Software framework development; Ethics; Research methods; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Time series; Probability; ,Nick is Professor of Microbial Genomics and Bioinformatics at the Institute of Microbiology and Infection (School of Biosciences) at the University of Birmingham. He graduated with a degree in Pathology (Infection & Immunity) from Imperial College in 2001; and in Medicine from Queen Mary's School of Medicine and Dentistry in 2004. After working as a junior doctor; he subsequently gained a PhD in Comparative Bacterial Genomics at the University of Birmingham in 2012. Nick's research focuses on the development of cutting-edge sequencing technologies and novel bioinformatics methods to help enable new applications in public health and clinical microbiology. He work focus on outbreak response; pathogen surveillance and infectious disease diagnostics including rapid detection of antimicrobial resistance.; Sequencing data; e.g. from portable nanopore sequencing devices offer the potential for a low-cost; rapid diagnostic tests for virus and bacterial diseases. At the Turing; Nick will focus on analysis of very large (> petabyte) genome sequencing datasets from microbial (including viruses and bacteria) populations and communities. This information will help understand how pathogens evolve; and how genome information can predict important characteristics of microbes; including antibiotic resistance and virulence. Working with colleagues from the Turing he will explore new machine learning techniques to help make such inferences in order to help reduce the spread of antibiotic resistance and to help in the control and treatment of infectious diseases.; Nick is interested in new technologies and approaches to the investigation of outbreaks and epidemics. In 2011 he kick-started a consortium that analysed a large European outbreak of E. coli originating from Germany through an open analysis of genomic data conducted online and real-time. In 2015 he established real-time genomic surveillance of the Ebola epidemic by establishing (with Public Health England and the European Mobile Labs) nanopore sequencing technology into Guinea. This outbreak response provided real-time open datasets which in turn generated vital epidemiological evidence of how Ebola virus evolved and spread; including revealing new insights into Ebola virus persistence in survivors.; ,Sequencing data; e.g. from portable nanopore sequencing devices offer the potential for a low-cost; rapid diagnostic tests for virus and bacterial diseases. At the Turing; Nick will focus on analysis of very large (> petabyte) genome sequencing datasets from microbial (including viruses and bacteria) populations and communities. This information will help understand how pathogens evolve; and how genome information can predict important characteristics of microbes; including antibiotic resistance and virulence. Working with colleagues from the Turing he will explore new machine learning techniques to help make such inferences in order to help reduce the spread of antibiotic resistance and to help in the control and treatment of infectious diseases.; ,Nick is interested in new technologies and approaches to the investigation of outbreaks and epidemics. In 2011 he kick-started a consortium that analysed a large European outbreak of E. coli originating from Germany through an open analysis of genomic data conducted online and real-time. In 2015 he established real-time genomic surveillance of the Ebola epidemic by establishing (with Public Health England and the European Mobile Labs) nanopore sequencing technology into Guinea. This outbreak response provided real-time open datasets which in turn generated vital epidemiological evidence of how Ebola virus evolved and spread; including revealing new insights into Ebola virus persistence in survivors.; ,Professor Nicholas Loman 
nick-higham,Numerical analysis; Parallel computing; Nonlinear programming; Algebra; ,Nick Higham is Royal Society Research Professor and Richardson Professor of Applied Mathematics in the School of Mathematics at the University of Manchester. He served as President of the Society for Industrial and Applied Mathematics (SIAM); 2018-2019. Much of his research is concerned with the accuracy and stability of numerical algorithms; and the second edition of his monograph on this topic was published by SIAM in 2002.; His other books include Functions of Matrices: Theory and Computation (SIAM; 2008); the first ever research monograph on matrix functions; and the 1000-page The Princeton Companion to Applied Mathematics (2015); of which he was editor.; Higham has contributed software to LAPACK and the NAG library; and has written numerous M-files included in MATLAB. His algorithms are also included in Julia; SciPy; Mathematica and other packages.; He is a Fellow of the Royal Society; a SIAM Fellow; and a Member of Academia Europaea. Honours include a 1999 Junior Whitehead Prize and the 2008 Fröhlich Prize; both from the London Mathematical Society; and he held a Royal Society-Wolfson Research Merit Award (2003--2008). He blogs about applied mathematics at http://nickhigham.wordpress.com; Higham develops fundamental theory and algorithms that enable reliable and efficient solution of practically-relevant problems. He is working on developing a new generation of numerical linear algebra algorithms that exploit current and future computers. The algorithms will be fast and will be accompanied by rigorous error analysis to guarantee their reliability. The target problems are linear equations; linear least squares problems; eigenvalue problems; the singular value decomposition and matrix function evaluation. These are the innermost kernels in many scientific and engineering applications—in particular; in data science and in machine learning—so it is essential that they are fast; accurate; and reliable. ; A particular aspect of this work is the exploitation of low precision arithmetic; which is now available in hardware and is increasingly being used in machine learning and scientific computing more generally. The use of low precision raises questions about the reliability of computations (provably getting the right results).; ,Higham develops fundamental theory and algorithms that enable reliable and efficient solution of practically-relevant problems. He is working on developing a new generation of numerical linear algebra algorithms that exploit current and future computers. The algorithms will be fast and will be accompanied by rigorous error analysis to guarantee their reliability. The target problems are linear equations; linear least squares problems; eigenvalue problems; the singular value decomposition and matrix function evaluation. These are the innermost kernels in many scientific and engineering applications—in particular; in data science and in machine learning—so it is essential that they are fast; accurate; and reliable. ; A particular aspect of this work is the exploitation of low precision arithmetic; which is now available in hardware and is increasingly being used in machine learning and scientific computing more generally. The use of low precision raises questions about the reliability of computations (provably getting the right results).; ,['N/A'],Professor Nick Higham 
nick-holliman,Data structures; Robotics; Parallel computing; Human computer interface; Stochastic optimisation; Cognitive science; ,Nick Holliman is Professor of Visualization in the School of Computing at Newcastle University. At Newcastle he is a member of the Digital Institute and is also part of the team working to establish the new National Innovation Centre for Data. He is a graduate in Computing with Electronics from the Departments of Computer Science and Applied Physics at Durham University; his PhD from Leeds University investigated novel parallel computing architectures for high performance graphics and was sponsored by IBM UK.; He first worked at startup Lightwork Design Ltd where he researched and developed the first commercial implementation of the radiosity lighting simulation method. Moving to Sharp Laboratories of Europe in Oxford as Principal Researcher he led a team researching software methods for auto-stereoscopic (glasses-free) 3D displays including graphics algorithms and real time head detection and eye tracking. Sharp commercialised these results in laptops; desktops and mobile phones; the most commercially successful outcome being Nintendo’s 3DS game system.; Returning to the academy in 2001 he formed and led the Innovative Computing research group at Durham University; subsequently moving to build the Interactive Media group at York University where he was founding programme director for the BSc in Interactive Media. Both continue as successful groups today. In 2015 he moved to Newcastle University to join the Digital Institute and focus on the research challenges of visualisation for big data; in 2018 he became head of the Scalable Research Group in the School of Computing. His research has always involved multi-disciplinary or multi-perspective collaborations.; His research aims to find new ways to create data visualisations that directly address challenges in the human understanding of big data and AI. Starting off in electronic form in a computer; a display device converts information optically to light; this electromagnetic information is detected by the eye and the information finally exists as electro-chemical nerve impulses in the brain. How should we optimise this process so that we can quickly and accurately understand new ideas; and with these ideas take decisions about the world?; His recent research at Newcastle has focused on exploiting the power of cloud super-computing to create more effective visualisations of urban analytics data. Cloud super-computing allows three orders of magnitude more computing power to be applied to data visualisation problems than would be possible on even the most powerful desktop computer. This is allowing the exploration of visual approaches to representing data that have not been see before; one example is the TeraScope project that is producing a visualisation from city scale to room scale in a single image of one trillion pixels.; As a Turing Fellow he is planning to follow two directions of research: First to address the challenge of visualising uncertainty; how might we represent values and our confidence in values in immediately understandable ways. Second to address the challenge of how to automate visualisation; big data is too big for a human designer to confidently represent visually; how can we automate this process using stochastic optimisation techniques and perhaps also machine learning.; As well as patents and academic papers his research outputs have included a number of award winning short stereoscopic 3D films on the role of dark matter in the creation of the universe in collaboration with Professor Carlos Frenk and the Institute for Computational Cosmology at Durham University. He is a member of the Association for Computing Machinery; The IEEE Computer Society; the Royal Statistical Society and the Society for Imaging Science and Technology.; ,His research aims to find new ways to create data visualisations that directly address challenges in the human understanding of big data and AI. Starting off in electronic form in a computer; a display device converts information optically to light; this electromagnetic information is detected by the eye and the information finally exists as electro-chemical nerve impulses in the brain. How should we optimise this process so that we can quickly and accurately understand new ideas; and with these ideas take decisions about the world?; His recent research at Newcastle has focused on exploiting the power of cloud super-computing to create more effective visualisations of urban analytics data. Cloud super-computing allows three orders of magnitude more computing power to be applied to data visualisation problems than would be possible on even the most powerful desktop computer. This is allowing the exploration of visual approaches to representing data that have not been see before; one example is the TeraScope project that is producing a visualisation from city scale to room scale in a single image of one trillion pixels.; As a Turing Fellow he is planning to follow two directions of research: First to address the challenge of visualising uncertainty; how might we represent values and our confidence in values in immediately understandable ways. Second to address the challenge of how to automate visualisation; big data is too big for a human designer to confidently represent visually; how can we automate this process using stochastic optimisation techniques and perhaps also machine learning.; ,As well as patents and academic papers his research outputs have included a number of award winning short stereoscopic 3D films on the role of dark matter in the creation of the universe in collaboration with Professor Carlos Frenk and the Institute for Computational Cosmology at Durham University. He is a member of the Association for Computing Machinery; The IEEE Computer Society; the Royal Statistical Society and the Society for Imaging Science and Technology.; ,Professor Nick Holliman 
nick-malleson,Probabilistic programming; Simulation; Monte Carlo methods; Uncertainty quantification; Multi-agent systems; ,Dr Malleson is Professor of Spatial Science at the Centre for Spatial Analysis and Policy at the School of Geography; University of Leeds; UK. He has a PhD in Geography and undergraduate degrees in Computer Science (BSc) and Multidiciplinary Informatics (MSc). Most of his research focuses on the development of computer models that can help to understand social phenomena. He has particular interests in simulations of crime patterns; and in models that can be used to describe the flows of people around cities. More recently; he has become in interested in how 'big data'; agent-based modelling; and smart cities initiatives can be used to reduce the impacts of problems like pollution or crime.; The aim of this project is to develop methods that can be used to better understand uncertainty in individual-level models. In particular; it will explore and extend the state-of-the-art in two related areas: ensemble modelling and emulators for use in individual-level models. Ensembles are groups of models that are initialised with slight variations in their starting conditions (either in parameter values or input data) and then executed simultaneously. The distributions of outputs from ensembles are indicative of the range of possible outcomes that a simulation might produce; and the uncertainty associated with particular outcomes.; Although widely used in the physical sciences; the use of ensembles in individual-level modelling (beyond simple parameter sweeps) is limited. An improved understanding about how ensembles can be applied; how their outputs can be interpreted; and how they can inform our understanding of where uncertainty arises; in the context of agent-based modelling; will be extremely timely and valuable. The project will also investigate emulators; and their applicability to individual-level models.; The motivation behind the use of emulators is that individual-based simulations are usually extremely computationally expensive. An emulator is a simple model that mimics the behaviour of a more complex model. A good emulator could be used in an ensemble; for example; where the overall aim is to explore the distribution of model outcomes; not analyse the results of a particular model configuration in detail. The project will; therefore; explore the use of emulators as a means of simulating the behaviour of more complex agent-based models.; Dr Malleson was recently awarded the Gill Memorial Award For outstanding early career research in agent-based social geography from the Royal Geographical Society. He has also recently secured €1.5M from the European Research Council (Starting Grant) for a project to explore the use of data assimilation methods in agent-based urban models.; ,The aim of this project is to develop methods that can be used to better understand uncertainty in individual-level models. In particular; it will explore and extend the state-of-the-art in two related areas: ensemble modelling and emulators for use in individual-level models. Ensembles are groups of models that are initialised with slight variations in their starting conditions (either in parameter values or input data) and then executed simultaneously. The distributions of outputs from ensembles are indicative of the range of possible outcomes that a simulation might produce; and the uncertainty associated with particular outcomes.; Although widely used in the physical sciences; the use of ensembles in individual-level modelling (beyond simple parameter sweeps) is limited. An improved understanding about how ensembles can be applied; how their outputs can be interpreted; and how they can inform our understanding of where uncertainty arises; in the context of agent-based modelling; will be extremely timely and valuable. The project will also investigate emulators; and their applicability to individual-level models.; The motivation behind the use of emulators is that individual-based simulations are usually extremely computationally expensive. An emulator is a simple model that mimics the behaviour of a more complex model. A good emulator could be used in an ensemble; for example; where the overall aim is to explore the distribution of model outcomes; not analyse the results of a particular model configuration in detail. The project will; therefore; explore the use of emulators as a means of simulating the behaviour of more complex agent-based models.; ,Dr Malleson was recently awarded the Gill Memorial Award For outstanding early career research in agent-based social geography from the Royal Geographical Society. He has also recently secured €1.5M from the European Research Council (Starting Grant) for a project to explore the use of data assimilation methods in agent-based urban models.; ,Professor Nick Malleson 
nick-polydorides,Compression (Algorithms); Numerical (Algorithms); Dynamical systems & differential equations; Mathematical physics; Numerical analysis; Real time computing; Stochastic (Mathematical modelling); Uncertainty quantification; Simulation; ,Nick Polydorides is a Reader and Head of the Institute for Digital Communications; at the School of Engineering of the University of Edinburgh University. Previously he was an assistant Professor at the Cyprus Institute for Energy and Environment research and held postdoctoral positions at the School of Mathematics of the University of Manchester and the Lab for Information & Decision Systems at MIT. His background is in computational modelling and inverse problems as these apply to engineering and physical systems. As of 2019 he serves in EPSRC’s strategic advisory teams for Information & Communication Technologies and Mathematical Sciences.; These days he is mostly interested in real-time simulation of complex systems in the context of digital twins. In particular he is looking into randomised numerical algebra (sketching) algorithms for model order reduction in model prediction and parameter estimation in high-dimensional engineering models. Such technology builds up on linear algebra and numerical methods for partial differential equations to provide much needed solutions to a variety of applications in security; additive manufacturing and energy.; ,These days he is mostly interested in real-time simulation of complex systems in the context of digital twins. In particular he is looking into randomised numerical algebra (sketching) algorithms for model order reduction in model prediction and parameter estimation in high-dimensional engineering models. Such technology builds up on linear algebra and numerical methods for partial differential equations to provide much needed solutions to a variety of applications in security; additive manufacturing and energy.; ,['N/A'],Dr Nick Polydorides 
nick-whiteley,Numerical (Algorithms); Dynamical systems & differential equations; Control theory; Neuroscience; Nonlinear dynamics; Parallel computing; Applications (Machine learning); Reinforcement learning; Unsupervised learning; Dynamic/static (Mathematical modelling); Stochastic (Mathematical modelling); Convex programming; Stochastic optimisation; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Time series; Modelling (Statistical methods & theory); Probability; ,Dr Whiteley is a Associate Professor in Statistics and a member of the Institute for Statistical Science in the School of Mathematics; University of Bristol. He was awarded an MEng in Engineering Science from the University of Oxford in 2003 and a PhD in Computational Statistics and Statistical Signal Processing from the University of Cambridge in 2009. At the University of Bristol he was a Brunel Fellow in Statistics from 2008 to 2010 and Lecturer in Statistics from 2010 to 2017.; Dr Whiteley's research lies at the interface between computational statistics; data science and machine learning. He seeks to understand and develop statistical models for complex; often high-dimensional stochastic systems; and algorithms which go along with them to help uncover patterns; detect changes; assess evidence and make predictions from data. Topics of particular interest include: convex optimisation; inference and forecasting for time-series and streaming data; Monte Carlo algorithms; nonlinear filtering; partially observed stochastic processes; and probabilistic foundations of machine learning. He works with scientists and engineers developing statistical solutions to problems from finance; neuroscience; telecommunications and beyond.; ,Dr Whiteley's research lies at the interface between computational statistics; data science and machine learning. He seeks to understand and develop statistical models for complex; often high-dimensional stochastic systems; and algorithms which go along with them to help uncover patterns; detect changes; assess evidence and make predictions from data. Topics of particular interest include: convex optimisation; inference and forecasting for time-series and streaming data; Monte Carlo algorithms; nonlinear filtering; partially observed stochastic processes; and probabilistic foundations of machine learning. He works with scientists and engineers developing statistical solutions to problems from finance; neuroscience; telecommunications and beyond.; ,['N/A'],Dr Nick Whiteley 
nick-wright,Robotics; Multi-agent reasoning; Control theory; Neural networks; Real time computing; Computer vision; Deep learning; Pattern recognition; Reinforcement learning; ,Nick Wright is an Engineering Professor at Newcastle University where he undertakes research in the use of digital technology applied to unusual environments. He has worked extensively with projects aimed at the marine; aerospace; space and manufacturing sectors in collaboration with many major companies. He has particular interests in the use of embedded AI and data science deployed in circumstances where cloud services are not available. Prior to joining Newcastle University; he undertook PhD studies at Edinburgh University; Nick Wright is developing AI systems for use in unusual environments where cloud services are not easily accessed - this includes both factories and extreme environment (such as the deep ocean; the artic or space). Current projects include the use of real time image analysis in manufacturing for inspection of products; cetacean species identification; deep ocean surveying and low-cost environmental monitoring.; ,Nick Wright is developing AI systems for use in unusual environments where cloud services are not easily accessed - this includes both factories and extreme environment (such as the deep ocean; the artic or space). Current projects include the use of real time image analysis in manufacturing for inspection of products; cetacean species identification; deep ocean surveying and low-cost environmental monitoring.; ,['N/A'],Professor Nick Wright 
nicolas-pugeault,Robotics; Neural networks; Computer vision; Deep learning; Pattern recognition; Reinforcement learning; Cognitive science; ,Dr Nicolas Pugeault is a Lecturer in Computer Vision and Machine Learning at the University of Exeter. Dr Pugeault obtained his PhD from the University of Goettingen (Germany). Previously; he was at the University of Edinburgh; University of Southern Denmark and University of Surrey.; Dr Pugeault's research focuses aspects of machine learning; computer vision and intelligent robotics. More specifically; he is interested on how machines can 'learn to see' from experience; using machine learning algorithms to analyse and interpret image and video data. This research has been applied to problems as varied as developmental robotics; navigation; autonomous driving; sign language recognition amongst others.; ,Dr Pugeault's research focuses aspects of machine learning; computer vision and intelligent robotics. More specifically; he is interested on how machines can 'learn to see' from experience; using machine learning algorithms to analyse and interpret image and video data. This research has been applied to problems as varied as developmental robotics; navigation; autonomous driving; sign language recognition amongst others.; ,['N/A'],Dr Nicolas Pugeault 
niels-peek,Artificial intelligence; Human computer interface; Machine learning; Applications (Machine learning); Unsupervised learning; Cognitive science; Statistical methods & theory; ,"Niels Peek is Professor of Health Informatics and Strategic Research Domain Director for Digital Health at the University of Manchester. He has a background in Computer Science and Artificial Intelligence; and his research focuses on data-driven methods for health research; healthcare quality improvement; and computerised decision support. He leads the Greater Manchester Connected Health City; which is part of the £20M ""Health North"" investment to establish a learning health system in the North of England. Professor Peek has co-authored more than 150 peer-reviewed scientific publications. From 2013 to 2017 he was the President of the Society for Artificial Intelligence in Medicine.; He is a member of the editorial boards of the Journal of the American Medical Informatics Association and the Artificial Intelligence in Medicine journal. In April 2017; he organised the Informatics for Health 2017 conference in Manchester which was attended by more than 800 people from 30 countries. He also co-chaired the Scientific Programme Committee of MEDINFO-2017; the 16th World Congress on Health and Biomedical Informatics; which was held in Hangzhou; China; in August 2017. In 2018 he was elected to become a fellow of the American Collecege of Medical Informaticians and a fellow of the Alan Turing Institute.; Health data; for example from electronic health records or other sources such as wearables and the Internet; are subject to complex data generation processes; which are not addressed with existing methods for risk prediction. Professor Peek's research for the Turing focuses on two key issues. First; routine health data is subject to ‘informative presence’ whereby presence of a particular observation (e.g. a blood test) is informative independent of the actual result of the test. For example; it gives information about an individual’s tendency to engage with the healthcare system; and/or information about a clinician’s prior beliefs about a patient’s condition that drives them to run particular tests. This is challenging to model because it essentially corresponds to a ‘missing not at random’ scenario. Second; existing risk prediction models model prognostic risk only – and do not allow consideration of ‘what-if’ scenarios.; potential outcomes (causal) framework would allow us to infer risk under different intervention scenarios – both at individual patient level and at policy level. Initial work has explored the use of marginal structural models to infer potential outcomes for patients at risk of cardiovascular events – where the intervention is a statin prescription. Currently more advanced models that allow ‘what-if’ prediction modelling using messy observational data are being developed.; ",Health data; for example from electronic health records or other sources such as wearables and the Internet; are subject to complex data generation processes; which are not addressed with existing methods for risk prediction. Professor Peek's research for the Turing focuses on two key issues. First; routine health data is subject to ‘informative presence’ whereby presence of a particular observation (e.g. a blood test) is informative independent of the actual result of the test. For example; it gives information about an individual’s tendency to engage with the healthcare system; and/or information about a clinician’s prior beliefs about a patient’s condition that drives them to run particular tests. This is challenging to model because it essentially corresponds to a ‘missing not at random’ scenario. Second; existing risk prediction models model prognostic risk only – and do not allow consideration of ‘what-if’ scenarios.; potential outcomes (causal) framework would allow us to infer risk under different intervention scenarios – both at individual patient level and at policy level. Initial work has explored the use of marginal structural models to infer potential outcomes for patients at risk of cardiovascular events – where the intervention is a statin prescription. Currently more advanced models that allow ‘what-if’ prediction modelling using messy observational data are being developed.; ,,Professor Niels Peek PhD FACMI
nigel-collier,['N/A'],Nigel is a Director of Research in Computational Linguistics at the University of Cambridge. Before this he was a Marie Curie Research Fellow at the European Bioinformatics Institute and an Associate Professor at the National Institute of Informatics in Japan. He received his PhD in 1996 from The University of Manchester Institute of Science and Technology (now The University of Manchester) for his research into how a Hopfield network (a type of recurrent artificial neural network) could be used to perform machine translation.; The current focus of Nigel’s research is on machine learning for Natural Language Processing (NLP) with application to health. Health data includes many types of large-scale unstructured sources including informal patient self reports in the social media; news reports; electronic patient records and the scientific literature. I am particularly interested in techniques for grounding text to domain ontologies to enable better machine understanding and knowledge discovery. At the same time I am also interested in forming bridges to other disciplines; particularly around the social and ethical dimensions of new types of health text data.; Nigel is a Turing Fellow through his supervision of Alexander Mansbridge; a doctoral student at the Turing.; ,The current focus of Nigel’s research is on machine learning for Natural Language Processing (NLP) with application to health. Health data includes many types of large-scale unstructured sources including informal patient self reports in the social media; news reports; electronic patient records and the scientific literature. I am particularly interested in techniques for grounding text to domain ontologies to enable better machine understanding and knowledge discovery. At the same time I am also interested in forming bridges to other disciplines; particularly around the social and ethical dimensions of new types of health text data.; Nigel is a Turing Fellow through his supervision of Alexander Mansbridge; a doctoral student at the Turing.; ,['N/A'],Dr Nigel Collier 
nik-lomax,Real time computing; Research methods; Monte Carlo methods; Simulation; Time series; ,Nik Lomax is a University Academic Fellow in Data Analytics for Population Research at the University of Leeds; where he gained his PhD in Population Geography. Nik is a co-Investigator of the ESRC funded Consumer Data Research Centre where he leads the methods workstream.; Nik is developing innovative methods for estimating and projecting people; households and their demographic characteristics. He is addressing new ways to integrate information on human behaviour which are lacking from current demographic models and will focus on the dynamic transitions which occur in everyday life; for example decisions to move or stay in a location; form new households; or have children. His research will also assess the impact of external forces on these decisions; from the local (e.g. housing conditions) to the global (e.g. economic outlook); thus producing new scenarios which can be used in a wide range of applied settings.; Core projects will be developed using dynamic microsimulation techniques and wider cross-discipline methodological advances will be developed; such as the use of machine learning to train simulation parameters; the integration of Big Datasets containing information on human behaviour and extension to agent-based approaches for scenario simulation and dynamic modelling. The communication and visualisation of results; including the quantification of uncertainty in scenarios; is integral to achieving impact from the work.; ,Nik is developing innovative methods for estimating and projecting people; households and their demographic characteristics. He is addressing new ways to integrate information on human behaviour which are lacking from current demographic models and will focus on the dynamic transitions which occur in everyday life; for example decisions to move or stay in a location; form new households; or have children. His research will also assess the impact of external forces on these decisions; from the local (e.g. housing conditions) to the global (e.g. economic outlook); thus producing new scenarios which can be used in a wide range of applied settings.; Core projects will be developed using dynamic microsimulation techniques and wider cross-discipline methodological advances will be developed; such as the use of machine learning to train simulation parameters; the integration of Big Datasets containing information on human behaviour and extension to agent-based approaches for scenario simulation and dynamic modelling. The communication and visualisation of results; including the quantification of uncertainty in scenarios; is integral to achieving impact from the work.; ,['N/A'],Dr Nik Lomax 
nikolaos-fountoulakis,Graph theory; Probability; Combinatorics; Geometry & topology; ,Nikolaos Fountoulakis is a Reader in probabilistic combinatorics at the University of Birmingham. He gained his DPhil in Mathematics at the University of Oxford in 2003. He then held several research positions at McGill University; the University of Birmingham and the Max Planck Institute for Informatics. He returned to the University of Birmingham as a Lecturer in 2011. His research has been funded by the European Commission (through the Marie Curie scheme) as well as by the EPSRC.; His current research programme has to do with the development of the mathematical theory of evolving discrete topological spaces and their large-scale characteristics. These mechanisms are viewed as models of the evolution of networks in which relations are high-dimensional; not merely between any two nodes but are realised among groups of nodes. A key aspect of the project involves the combinatorial characteristics of these networks and the emerging geometry. In particular; he will be aiming to determine those conditions on the growth mechanism which ensure the emergence of scale-free characteristics as well as its small-world properties (two ubiquitous features in many instances of large-scale networks).; Research funding (present and past) 1.. EPSRC Standard Grant (PI) (2017-20) 2. EPSRC First Grant (PI) (2013-15) 3. Marie Curie Career Integration Grant (FP7) (PI) (2011-15) 5. Marie Curie Research Fellowship (Intra-European Fellowships scheme - FP7) (PI) (2010-11); ,His current research programme has to do with the development of the mathematical theory of evolving discrete topological spaces and their large-scale characteristics. These mechanisms are viewed as models of the evolution of networks in which relations are high-dimensional; not merely between any two nodes but are realised among groups of nodes. A key aspect of the project involves the combinatorial characteristics of these networks and the emerging geometry. In particular; he will be aiming to determine those conditions on the growth mechanism which ensure the emergence of scale-free characteristics as well as its small-world properties (two ubiquitous features in many instances of large-scale networks).; ,Research funding (present and past) 1.. EPSRC Standard Grant (PI) (2017-20) 2. EPSRC First Grant (PI) (2013-15) 3. Marie Curie Career Integration Grant (FP7) (PI) (2011-15) 5. Marie Curie Research Fellowship (Intra-European Fellowships scheme - FP7) (PI) (2010-11); ,Dr Nikolaos Fountoulakis 
norman-fenton,Systems theory; Graph theory; Cognitive science; Data science of government & politics; Uncertainty quantification; Causality; Probability; Combinatorics; ,"Norman Fenton is Professor of Risk Information Management at Queen Mary London University (since 2000) and is also a Director of Agena; a company that specializes in risk management for critical systems (since 1998). His previous academic posts were at City University (Professor in the Centre for Software Reliability; 1989-2000); South Bank (Director of Centre for Systems and Software Engineering; 1984-89); Oxford University Maths Institute and Wolfson College Oxford (Post Doctoral Research Fellow; 1982-84) and University College Dublin (Research Fellow; 1981-82). He was a Simons Fellow at the Isaac Newton Institute for Mathematical Sciences; Cambridge (2016); and was a visiting researcher at GMD; Germany (1988). Norman is an Affiliated Professor to the University of Haifa; Israel (since 2006); a Chartered Engineer; a Chartered Mathematician and is a Fellow of the British Computer Society. Norman's PhD was in Mathematics from Sheffield University.; Norman's current research focuses on critical decision-making and; in particular; on quantifying uncertainty. This typically involves analysing and predicting the probabilities of unknown events using causal; probabilistic models (Bayesian networks). This type of reasoning enables improved assessment by taking account of both statistical data where available and also expert judgement. This approach can provide more powerful insights and better decision making than is possible from purely data-driven solutions. Hence the approach can be summarised as 'smart data rather than big data'.; Applications include law and forensics (Norman has been an expert witness in major criminal and civil cases); medicine; security; software reliability; transport safety and reliability; finance; and football prediction. Norman also has a special interest in raising public awareness of the importance of probability theory and Bayesian reasoning in everyday life (including how to present such reasoning in simple lay terms) and he maintains a blog focusing on probability and the law.; Full listing of Norman's publications including downloads;  ; ; Norman has published 7 books and over 250 refereed articles; and has provided consulting to many major companies world-wide. His book ""Risk Assessment and Decision Analysis using Bayesian Networks"" with Martin Neil (first edition 2012; second edition 2018) was the first to bring Bayesian networks to a general audience. His book ""Software Metrics: A Rigorous and Practical Approach"" (now in its third edition) has 5863 citations.; Norman has been PI in grants totalling over £10million. He currently leads an EPRSC Digital Health Technologies Project (PAMBAYESIAN) and a Leverhulme Trust grant (CAUSAL-DYNAMICS). In 2014 Norman was awarded a prestigious European Research Council Advanced Grant (BAYES-KNOWLEDGE). In March 2015 Norman presented the award-winning BBC documentary Climate Change by Numbers. Since June 2011 he has led an international consortium (Bayes and the Law) of statisticians; lawyers and forensic scientists working to improve the use of statistics in court. In 2016 he led a prestigious six-month programme on Probability and Statistics in Forensic Science at the Isaac Newton Institute for Mathematical Sciences; University of Cambridge. He was appointed as a Fellow of The Alan Turing Institute in 2018.; ",Norman's current research focuses on critical decision-making and; in particular; on quantifying uncertainty. This typically involves analysing and predicting the probabilities of unknown events using causal; probabilistic models (Bayesian networks). This type of reasoning enables improved assessment by taking account of both statistical data where available and also expert judgement. This approach can provide more powerful insights and better decision making than is possible from purely data-driven solutions. Hence the approach can be summarised as 'smart data rather than big data'.; Applications include law and forensics (Norman has been an expert witness in major criminal and civil cases); medicine; security; software reliability; transport safety and reliability; finance; and football prediction. Norman also has a special interest in raising public awareness of the importance of probability theory and Bayesian reasoning in everyday life (including how to present such reasoning in simple lay terms) and he maintains a blog focusing on probability and the law.; Full listing of Norman's publications including downloads;  ; ; ,"Norman has published 7 books and over 250 refereed articles; and has provided consulting to many major companies world-wide. His book ""Risk Assessment and Decision Analysis using Bayesian Networks"" with Martin Neil (first edition 2012; second edition 2018) was the first to bring Bayesian networks to a general audience. His book ""Software Metrics: A Rigorous and Practical Approach"" (now in its third edition) has 5863 citations.; Norman has been PI in grants totalling over £10million. He currently leads an EPRSC Digital Health Technologies Project (PAMBAYESIAN) and a Leverhulme Trust grant (CAUSAL-DYNAMICS). In 2014 Norman was awarded a prestigious European Research Council Advanced Grant (BAYES-KNOWLEDGE). In March 2015 Norman presented the award-winning BBC documentary Climate Change by Numbers. Since June 2011 he has led an international consortium (Bayes and the Law) of statisticians; lawyers and forensic scientists working to improve the use of statistics in court. In 2016 he led a prestigious six-month programme on Probability and Statistics in Forensic Science at the Isaac Newton Institute for Mathematical Sciences; University of Cambridge. He was appointed as a Fellow of The Alan Turing Institute in 2018.; ",Professor Norman Fenton CEng; CMath; FBCS; MIET
oliver-davis,Research methods; Social media; Software framework development; Visualisation (Programming languages); Time series; Simulation; Modelling (Statistical methods & theory); High dimensional inference; Causality; Human computer interface; Visualisation (Computer systems & architectures); Natural language processing; Applications (Machine learning); ,Oliver Davis is Associate Professor and Turing Fellow at Bristol Medical School and the MRC Integrative Epidemiology Unit at the University of Bristol; where he co-directs the Dynamic Genetics Lab with Professor Claire Haworth. Oliver read Natural Sciences at Cambridge; studied for a PhD in social; genetic and developmental psychiatry at King’s College London (KCL) and held a Sir Henry Wellcome Research Fellowship; visiting the Wellcome Trust Centre for Human Genetics in Oxford and the European Bioinformatics Institute near Cambridge. He held academic positions at KCL and UCL before moving to Bristol in 2015.; Mental health and wellbeing are influenced by a complex interplay of social; genomic and developmental factors. Understanding these patterns and how we can intervene to promote good mental health is one of the greatest and most important challenges of our time.; Oliver’s research in mental health data science and digital epidemiology uses emerging technologies to collect; analyse and visualize data from large population samples around the world; exploring the dynamic relationship between social and genomic factors and wellbeing over the life course. Recent research from the lab includes the spACE project which is mapping geographical variation in social and genetic influences on complex traits; and the EMBERS project which is collecting millions of social media interactions from thousands of participants in epidemiological cohorts to track dynamic social and genetic influences on wellbeing in emerging adulthood.; ,Mental health and wellbeing are influenced by a complex interplay of social; genomic and developmental factors. Understanding these patterns and how we can intervene to promote good mental health is one of the greatest and most important challenges of our time.; Oliver’s research in mental health data science and digital epidemiology uses emerging technologies to collect; analyse and visualize data from large population samples around the world; exploring the dynamic relationship between social and genomic factors and wellbeing over the life course. Recent research from the lab includes the spACE project which is mapping geographical variation in social and genetic influences on complex traits; and the EMBERS project which is collecting millions of social media interactions from thousands of participants in epidemiological cohorts to track dynamic social and genetic influences on wellbeing in emerging adulthood.; ,['N/A'],Dr Oliver Davis 
omar-guerrero,Social data science; Data science of government & politics; Social networks; Simulation; Non-parametric & semi-parametric methods; Evolution & adaptation; Game theory; Multi-agent reasoning; Multi-agent systems; Stochastic (Mathematical modelling); Unsupervised learning; ,Omar Guerrero has a PhD in Computational Social Science. Previously; he was a research fellow at the Saïd Business School (Oxford) and an Oxford Martin Fellow at the Institute of New Economic Thinking. His research interests lie in the intersection of Economics; Complexity Science and Data Science.; Omar’s research focuses on various socioeconomic topics such as labour and firm dynamics; detecting vote-trading behaviour in legislatures; modelling how governments determine policy priorities; and understanding how inequality emerges from institutional arrangements. The overarching theme of his agenda is using computation to bridge the gap between data science and social modelling. For this; he employs methods from fields such as agent computing and network science. More generally; he is interested in advancing CSS by developing methods and models that can inform public policy.; ,Omar’s research focuses on various socioeconomic topics such as labour and firm dynamics; detecting vote-trading behaviour in legislatures; modelling how governments determine policy priorities; and understanding how inequality emerges from institutional arrangements. The overarching theme of his agenda is using computation to bridge the gap between data science and social modelling. For this; he employs methods from fields such as agent computing and network science. More generally; he is interested in advancing CSS by developing methods and models that can inform public policy.; ,['N/A'],Dr Omar A Guerrero 
pamela-ugwudike,Algorithms; Data structures; Numerical (Algorithms); Artificial intelligence; Databases; Human computer interface; Machine learning; Mathematical modelling; Data science of government & politics; Ethics; Modelling (Statistical methods & theory); ,Pamela Ugwudike is an Associate Professor of Criminology and the Director of Research at the Department of Sociology; Social Policy and Criminology; University of Southampton. Prior to joining Southampton; she was at Swansea University where she completed her Masters Degree in Criminology (with Distinction) and her PhD. She held several academic posts at Swansea University over a period of ten years. First; she was a Research Officer; then a Lecturer in Criminology; and subsequently a Senior Lecturer in Criminology; in the College of Law and Criminology. She also led the Swansea Service Evaluation Team (SSET) which was an interdisciplinary team of researchers and one of the College’s criminal justice research teams from 2013-2017. Pamela is also Fellow of the Higher Education Academy; UK and a member of the Howard League for Penal Reform’ Research Advisory Group. She was a member of the British Society of Criminology’s Executive Committee (2015-2018).; Pamela’s research interests include studying interactions between digital technology and criminal justice. She is currently working on the following projects: digital predictive technologies and conduits of algorithmic bias in policing and penal services; digital governance and regulatory systems for law enforcement agencies; and applications of digital technology in youth justice contexts. ; At the Turing; Pamela will work on an interdisciplinary project with experts in data science and mathematical sciences to explore the implications of operationalising predictive artificial intelligence (AI) technologies in the criminal justice system. Her project will focus on the proactive or preventative policing phase when data-driven predictive algorithms are used for crime prevention purposes. There is growing interest in exploring the benefits and potential harms of predictive algorithms in criminal justice systems; but the extant empirical literature focuses mainly on policing and sentencing contexts in the US and other jurisdictions. Limited independent research exists on the nature and impact of the predictive algorithms employed by police services in the UK. The project seeks to address this gap in knowledge. Its main objective is to identify possible conduits of bias and corrective measures.;  ; ,Pamela’s research interests include studying interactions between digital technology and criminal justice. She is currently working on the following projects: digital predictive technologies and conduits of algorithmic bias in policing and penal services; digital governance and regulatory systems for law enforcement agencies; and applications of digital technology in youth justice contexts. ; At the Turing; Pamela will work on an interdisciplinary project with experts in data science and mathematical sciences to explore the implications of operationalising predictive artificial intelligence (AI) technologies in the criminal justice system. Her project will focus on the proactive or preventative policing phase when data-driven predictive algorithms are used for crime prevention purposes. There is growing interest in exploring the benefits and potential harms of predictive algorithms in criminal justice systems; but the extant empirical literature focuses mainly on policing and sentencing contexts in the US and other jurisdictions. Limited independent research exists on the nature and impact of the predictive algorithms employed by police services in the UK. The project seeks to address this gap in knowledge. Its main objective is to identify possible conduits of bias and corrective measures.;  ; ,['N/A'],Dr Pamela Ugwudike 
panos-panagiotopoulos,Data science of government & politics; Social networks; Research methods; Social media; ,Dr Panos Panagiotopoulos is a Senior Lecturer in Information Management with the School of Business and Management at Queen Mary University of London. He completed an MEng degree in electrical and computer engineering at the National University of Athens and a PhD in information systems and computing at Brunel University London where he received the Vice Chancellor’s Prize for Doctoral Research.Dr Panagiotopoulos is a member of the editorial board for Government Information Quarterly and New Technology; Work and Employment. He has extensive experience as a track chair and associate editor in several national and international conferences such as the British Academy of Management and the European; American and International Conference on Information Systems. Within his interests in digital applications in the public sector; Dr Panagiotopoulos has collaborated with central and local government organisations; including the Food Standards Agency; the Department for Business; Energy and Industrial Strategy (BEIS) and the Department for the Environment; Food and Rural Affairs (DEFRA). ; Dr Panagiotopoulos specialises in digital government and social media research. His previous work has focused on different aspects of digital engagement; networks of practice; social media analytics; crowdsourcing and information flows around regulatory decisions.In current work; he is interested to explore how new forms of data; data science and data analytics are reshaping public policy and public management. In particular; this relates to the potential of emerging technologies in regulation and the new roles for regulators in this context.Within the Turing; Dr Panagiotopoulos' interests align with the social data science area and the government innovation programme.; ,Dr Panagiotopoulos specialises in digital government and social media research. His previous work has focused on different aspects of digital engagement; networks of practice; social media analytics; crowdsourcing and information flows around regulatory decisions.In current work; he is interested to explore how new forms of data; data science and data analytics are reshaping public policy and public management. In particular; this relates to the potential of emerging technologies in regulation and the new roles for regulators in this context.Within the Turing; Dr Panagiotopoulos' interests align with the social data science area and the government innovation programme.; ,['N/A'],Dr Panos Panagiotopoulos 
paolo-missier,Databases; Parallel computing; Applications (Machine learning); Deep learning; ,Paolo Missier is a Reader in Large-Scale Information Management with the School of Computing at Newcastle University. Paolo's background is broadly in data and information management; with a specific long-term research interest in data provenance; where amongst other research contributions; he was instrumental to the specification of the PROV data model for provenance; through the W3C.; Closer to Big Data management and Data Science and Engineering; more recently Paolo studied the problem of optimising re-computation of resource-intensive data analytics processes when their outcomes are invalidated through changes in the processes' inputs and dependencies (ReComp: http://recomp.org.uk; 2016-2019; EPSRC funding).; More can be read about current and past projects on Paolo's personal page.  Paolo's 20+ years career trajectory started with an industry applied research position at Bell Communication Research (1994-2001); before obtaining his PhD in Computer Science at University of Manchester in 2007; leading to his current academic career; first at Manchester (2008-2010) and currently at Newcastle; since 2011.; At the Turing; Paolo is leading the [email protected] project on using Data Science in the healthcare domain; specifically to help realise a vision of predictive; personalised and participatory medicine. The project aims to study the predictive power of signals extracted from free-living wearable devices; in combination with personal genetic features; with specific focus on age-related diseases.; The [email protected] project will address some of the key data science challenges associated with the development of new preventive; predictive and personalised models that define the data-driven future of healthcare. It will contribute to the Turing's 'Revolutionise healthcare' challenge; with specific focus on (i) enabling early and precise detection; diagnosis; and treatment of illnesses; and (ii) predicting or preventing diseases for those at highest risk.; Chronic and metabolic diseases are a leading cause of death and disability. Current data acquisition technology provides the means to fully characterise an entire population of individuals in terms of a broad diversity of quantitative datasets; ranging from periodic but low-rate multi-omics data (genomics; proteomics; metabolomics; and more) to continuous and high rate self-monitoring data from wearable sensors. Translating this wealth of population-scale big data into models that can predict early disease onset at the granularity of the single individual entails multiple data science challenges. These include integrating across and understanding the correlations amongst diverse signals; engineering multi-scale features from monitoring time series data; exploring machine learning approaches to building effective predictive models; as well as data engineering challenges; such as distributed architectures for scalable data processing.; The UK BioBank will provide an initial testbed for experimentation; consisting of a cohort of individuals who are longitudinally characterised in terms of genotyping; phenotyping; and free-living monitoring from wearable devices. Additional testbeds will be added for independent testing and validation. This project is a collaboration between the School of Computing and the Institute of Genetic Medicine at Newcastle University; the National Innovation Centre for Ageing; and the NIHR Innovation Observatory; both based in Newcastle; with funding from the Turing.; ,At the Turing; Paolo is leading the [email protected] project on using Data Science in the healthcare domain; specifically to help realise a vision of predictive; personalised and participatory medicine. The project aims to study the predictive power of signals extracted from free-living wearable devices; in combination with personal genetic features; with specific focus on age-related diseases.; The [email protected] project will address some of the key data science challenges associated with the development of new preventive; predictive and personalised models that define the data-driven future of healthcare. It will contribute to the Turing's 'Revolutionise healthcare' challenge; with specific focus on (i) enabling early and precise detection; diagnosis; and treatment of illnesses; and (ii) predicting or preventing diseases for those at highest risk.; Chronic and metabolic diseases are a leading cause of death and disability. Current data acquisition technology provides the means to fully characterise an entire population of individuals in terms of a broad diversity of quantitative datasets; ranging from periodic but low-rate multi-omics data (genomics; proteomics; metabolomics; and more) to continuous and high rate self-monitoring data from wearable sensors. Translating this wealth of population-scale big data into models that can predict early disease onset at the granularity of the single individual entails multiple data science challenges. These include integrating across and understanding the correlations amongst diverse signals; engineering multi-scale features from monitoring time series data; exploring machine learning approaches to building effective predictive models; as well as data engineering challenges; such as distributed architectures for scalable data processing.; The UK BioBank will provide an initial testbed for experimentation; consisting of a cohort of individuals who are longitudinally characterised in terms of genotyping; phenotyping; and free-living monitoring from wearable devices. Additional testbeds will be added for independent testing and validation. This project is a collaboration between the School of Computing and the Institute of Genetic Medicine at Newcastle University; the National Innovation Centre for Ageing; and the NIHR Innovation Observatory; both based in Newcastle; with funding from the Turing.; ,['N/A'],Dr Paolo Missier 
paolo-turrini,Multi-agent systems; Operations research; Multi-agent reasoning; Game theory; Reinforcement learning; Graph theory; Cognitive science; Data science of government & politics; Ethics; Social media; Social psychology; Uncertainty quantification; Monte Carlo methods; Combinatorics; ,Paolo Turrini is a lecturer at the Department of Computer Science; University of Warwick. He gained his PhD in Computer Science at the University of Utrecht; working on modal logics for game theory. He won a COFUND Marie Curie fellowship from the University of Luxembourg; where he worked on logical and game theoretic models of trust in coalition formation. He also won an Intra European Marie Curie Fellowship; moving to Imperial College London; to work on models of distributed negotiations. At Imperial he was a recipient of a Junior Research Fellowship. His research is in artificial intelligence; in particular; models of strategic behaviour. He publishes in venues such as IJCAI; AAAI; AAMAS; JAAMAS.; In distributed artificial intelligence; where autonomous entities freely interact to pursue potentially conflicting objectives; regulating their decision-making is a critical concern. Automated trading; traffic control; resource allocation; are all scenarios in which the decisions of self-interested agents; even when individually rational; might lead to outcomes that are detrimental for society. Paolo's research; sits at the crossroads of computer science and game theory; and aims to construct algorithms for the regulation of complex strategic interaction; spelling out the theoretical and computational guarantees under which individually rational decisions lead to socially optimal outcomes. Agents are modelled as autonomous entities acting in a dynamic and unknown environment; which they can learn and reason about; and which is typically inhabited by other agents pursuing potentially conflicting objectives.; The research methodology builds on rigorous game-theoretic modelling and asks algorithmic questions. Instances of such questions (extremely relevant to top publication venues in AI; algorithmic game theory and multiagent systems) are: - can we devise a protocol for collective decision-making that cannot be manipulated? Think for instance of the problem of biased reviews in online social networks. - can we prevent external attackers from learning the structure of a social network? Think of a service provider interested in gathering information on potential customers. - can we deploy these programs and augment the existing systems? Think of the changes that current recommender systems need to undergo in order to be trustworthy for the users. Typical techniques are equilibrium analysis; complexity analysis and validation against human data.; ,In distributed artificial intelligence; where autonomous entities freely interact to pursue potentially conflicting objectives; regulating their decision-making is a critical concern. Automated trading; traffic control; resource allocation; are all scenarios in which the decisions of self-interested agents; even when individually rational; might lead to outcomes that are detrimental for society. Paolo's research; sits at the crossroads of computer science and game theory; and aims to construct algorithms for the regulation of complex strategic interaction; spelling out the theoretical and computational guarantees under which individually rational decisions lead to socially optimal outcomes. Agents are modelled as autonomous entities acting in a dynamic and unknown environment; which they can learn and reason about; and which is typically inhabited by other agents pursuing potentially conflicting objectives.; The research methodology builds on rigorous game-theoretic modelling and asks algorithmic questions. Instances of such questions (extremely relevant to top publication venues in AI; algorithmic game theory and multiagent systems) are: - can we devise a protocol for collective decision-making that cannot be manipulated? Think for instance of the problem of biased reviews in online social networks. - can we prevent external attackers from learning the structure of a social network? Think of a service provider interested in gathering information on potential customers. - can we deploy these programs and augment the existing systems? Think of the changes that current recommender systems need to undergo in order to be trustworthy for the users. Typical techniques are equilibrium analysis; complexity analysis and validation against human data.; ,['N/A'],Dr Paolo Turrini 
patrick-healey,Human computer interface; Natural language processing; Cognitive science; Ethics; Linguistics; Social psychology; ,Healey is Professor of Human Interaction and Head of the Cognitive Science Research Group in the School of Electronic Engineering; Queen Mary; University of London. He recently served as Senior Researcher in Residence at the Digital Catapult and as 2016 International Visiting Chair in Empirical Foundations of Linguistics at Sorbonne Cite / Paris 7. Healey's research focuses on understanding the mechanisms of human-human interaction and the development of technologies that can enable richer; more effective forms of communication.; Healey is pursuing three projects with the Turing Institute: 1. To develop new forms of adaptive learning that gives machines the ability to adapt their communication in real-time to different people and different situations; just as people can. 2. To analyse the ways in which people use cues such as hand shape; gaze and pressure sensitivity to hand things to each other. This will help inform the design of collaborative robots. 3. To explore the development of a personal data question register as a non-technical solution to the problems of transparency and accountability in the use of personal data.; ,Healey is pursuing three projects with the Turing Institute: 1. To develop new forms of adaptive learning that gives machines the ability to adapt their communication in real-time to different people and different situations; just as people can. 2. To analyse the ways in which people use cues such as hand shape; gaze and pressure sensitivity to hand things to each other. This will help inform the design of collaborative robots. 3. To explore the development of a personal data question register as a non-technical solution to the problems of transparency and accountability in the use of personal data.; ,['N/A'],Professor Patrick Healey 
patrick-rebeschini,Complexity (Algorithms); Information theory (Applied mathematics); Multi-agent systems; Operations research; Control theory; Neural networks; Communications; Deep learning; Reinforcement learning; Supervised learning; Graph theory; Stochastic (Mathematical modelling); Convex programming; Stochastic optimisation; High dimensional inference; Monte Carlo methods; Non-parametric & semi-parametric methods; Information theory (Statistical methods & theory); Probability; ,Patrick Rebeschini is an Associate Professor in the Department of Statistics at the University of Oxford; and a Tutorial Fellow at University College; Oxford. Prior to joining Oxford; Patrick was at Yale University where he has been an Associate Research Scientist in the Electrical Engineering Department; a Lecturer in the Computer Science Department; and a Postdoctoral Associate at the Yale Institute for Network Science. He holds a PhD in Operations Research and Financial Engineering from Princeton University.; Patrick's research interests are in probability in high dimension; optimisation; and machine learning. He is interested in the investigation of fundamental principles to perform scalable inference and learning in high-dimensional models; and in the design and analysis of distributed algorithms.; ,Patrick's research interests are in probability in high dimension; optimisation; and machine learning. He is interested in the investigation of fundamental principles to perform scalable inference and learning in high-dimensional models; and in the design and analysis of distributed algorithms.; ,['N/A'],Professor Patrick Rebeschini 
patrick-rubin-delanchy,Data structures; Communications; Parallel computing; Visualisation (Computer systems & architectures); Applications (Machine learning); Natural language processing; Semi-supervised learning; Supervised learning; Unsupervised learning; Graph theory; Stochastic (Mathematical modelling); Visualisation (Programming languages); Social networks; Social media; Uncertainty quantification; High dimensional inference; Non-parametric & semi-parametric methods; Asymptotic (Statistical methods & theory); Modelling (Statistical methods & theory); Probability; Geometry & topology; ,Patrick Rubin-Delanchy is a lecturer in Statistics at the University of Bristol. Prior to this lectureship; he was a Heilbronn Research fellow at the University of Oxford. He obtained his PhD from Imperial College London in 2008.; In this research; several mathematical and computational techniques (spectral embedding; topological data analysis; computational geometry) will be combined to enable practical analysis of complex connectivity data. The basic idea is to convert these complex objects into simpler structures; typically point clouds; and use geometric and topological information in these point clouds to make inference about the original object. The main envisaged application area is cyber-security; where complex connectivity data is ubiquitous and often very poorly exploited.; ,In this research; several mathematical and computational techniques (spectral embedding; topological data analysis; computational geometry) will be combined to enable practical analysis of complex connectivity data. The basic idea is to convert these complex objects into simpler structures; typically point clouds; and use geometric and topological information in these point clouds to make inference about the original object. The main envisaged application area is cyber-security; where complex connectivity data is ubiquitous and often very poorly exploited.; ,['N/A'],Dr Patrick Rubin-Delanchy 
paul-dodds,Multi-agent systems; Numerical analysis; Operations research; Graph theory; Stochastic optimisation; Data science of government & politics; Monte Carlo methods; Simulation; Time series; ,Dr Paul Dodds is Associate Professor in Energy Systems at University College London. He specialises in whole-economy energy system modelling and recently led the development of a new model; UK TIMES; which was the principal tool underpinning the UK Government's Clean Growth Strategy. He also leads UCL's academic work on European energy system modelling. His research explores how different technologies might contribute to energy system transitions in the future. He leads or manages major projects on energy storage; hydrogen and bioenergy; which all cut across engineering; environment and policy issues; and is a member of the UK Energy Research Centre; the UK CCS Research Centre and the Hydrogen and Fuel Cell Supergen Hub.; His past work has included an exploratory study into using hydrogen for heating; which has recently received much traction in the UK; and formalising 'model archaeology' as a method to design and understand the evolution of energy models. He works closely with the BEIS economics and engineering teams and with stakeholders from across industry; representing BEIS at the IEA ETSAP and IEA Hydrogen Technology Collaboration Programmes.; The UK energy system is being transformed by investments in low-carbon technologies to meet greenhouse gas emission targets. Energy system models focus on emissions and minimising the total energy supply cost. Yet given the vital underlying role of energy in the economy; security of supply is of paramount importance; and fuel poverty is a key issue for the UK Government. The potential for energy to contribute to a UK industrial renaissance is reflected by the focus on energy in the UK industrial strategy.; Paul's primary research interest at The Alan Turing Institute is to understand how data science can enable security of supply; fuel poverty and industrial opportunities to be much better understood within energy system models. Much of the extensive data required by energy system models are currently estimated. New sources of data are becoming available; for example from smart meters; which could greatly improve the representation of energy service demands in the short term and also fuel poverty in the longer term.; Paul is examining the extent to which smart meter data could be used to improve existing energy system models and build new higher resolution models that are more focused on the demand side. With the introduction of variable renewable generation into the energy system; both energy supply and demand will be sensitive to weather and to longer-term climatic changes. Paul is examining whether new UK climate projections from the Met Office could be used to improve energy scenario modelling using advanced data science methods.; ,The UK energy system is being transformed by investments in low-carbon technologies to meet greenhouse gas emission targets. Energy system models focus on emissions and minimising the total energy supply cost. Yet given the vital underlying role of energy in the economy; security of supply is of paramount importance; and fuel poverty is a key issue for the UK Government. The potential for energy to contribute to a UK industrial renaissance is reflected by the focus on energy in the UK industrial strategy.; Paul's primary research interest at The Alan Turing Institute is to understand how data science can enable security of supply; fuel poverty and industrial opportunities to be much better understood within energy system models. Much of the extensive data required by energy system models are currently estimated. New sources of data are becoming available; for example from smart meters; which could greatly improve the representation of energy service demands in the short term and also fuel poverty in the longer term.; Paul is examining the extent to which smart meter data could be used to improve existing energy system models and build new higher resolution models that are more focused on the demand side. With the introduction of variable renewable generation into the energy system; both energy supply and demand will be sensitive to weather and to longer-term climatic changes. Paul is examining whether new UK climate projections from the Met Office could be used to improve energy scenario modelling using advanced data science methods.; ,['N/A'],Dr Paul Dodds 
paul-jenkins,Deep learning; Natural language processing; Linguistics; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Time series; Probability; ,Paul Jenkins is an Associate Professor of Data Science at the University of Warwick; with a joint appointment in the Departments of Computer Science and Statistics. He obtained an MMath followed by a DPhil in statistics in 2008 from the University of Oxford. From 2008 to 2012 he was a postdoctoral researcher in the Computer Science Division at the University of California; Berkeley; before joining the University of Warwick in 2012 as a Harrison Early Career Assistant Professor.; Paul's research covers computational statistics; machine learning; Bayesian nonparametric statistics; and inference from stochastic processes; with particular application to population genetics and other evolutionary models. At The Alan Turing Institute he is focusing on developing methodology to understand word meaning evolution from large; unstructured corpora; and tumour evolution from time-series multi-omics data in cancer.; ,Paul's research covers computational statistics; machine learning; Bayesian nonparametric statistics; and inference from stochastic processes; with particular application to population genetics and other evolutionary models. At The Alan Turing Institute he is focusing on developing methodology to understand word meaning evolution from large; unstructured corpora; and tumour evolution from time-series multi-omics data in cancer.; ,['N/A'], Paul Jenkins 
paul-schofield,Knowledge representation; Machine learning; Deep learning; ,Dr Schofield is Reader in Biomedical Informatics at the University of Cambridge. He gained his first degree in Biochemistry in 1981 at the University of Oxford and then went on to complete a DPhil on the integration of chromatin assembly and histone gene expression in 1984. He moved to his post in Cambridge at the then Department of Anatomy – now Physiology Development and Neuroscience – in 1990 and is a fellow of Robinson College. He holds an adjunct Professorship at the Jackson Laboratory in the USA.; Dr Schofield's research interests centre on formal descriptions of human and model organism disease; with a particular interest in using phenotype and disease ontologies for the discovery of  human disease genes and their causative variants. He has ongoing interests in ontology axiomatisation; biomedical data integration and applications in machine learning and neural networks for personalised medicine.; The research he is conducting at the Turing involves extraction of patients' genome sequence and clinical data from electronic health records; and its integration with the huge volume of human genotype/phenotype information and data from fundamental experimental sciences; scattered internationally across multiple public databases. The data will be linked into a very large network; or knowledge graph; which may then be exploited using machine learning approaches to discover; for example; links between patient groups not previously suspected (patient stratification); insights into genetic overlaps between seemingly unrelated diseases and traits; and new predictors of prognosis; clinical events and responsiveness to therapy.; ,Dr Schofield's research interests centre on formal descriptions of human and model organism disease; with a particular interest in using phenotype and disease ontologies for the discovery of  human disease genes and their causative variants. He has ongoing interests in ontology axiomatisation; biomedical data integration and applications in machine learning and neural networks for personalised medicine.; The research he is conducting at the Turing involves extraction of patients' genome sequence and clinical data from electronic health records; and its integration with the huge volume of human genotype/phenotype information and data from fundamental experimental sciences; scattered internationally across multiple public databases. The data will be linked into a very large network; or knowledge graph; which may then be exploited using machine learning approaches to discover; for example; links between patient groups not previously suspected (patient stratification); insights into genetic overlaps between seemingly unrelated diseases and traits; and new predictors of prognosis; clinical events and responsiveness to therapy.; ,['N/A'],Dr Paul Schofield 
paul-watson-0,['N/A'],Paul Watson is Professor of Computer Science and Director of the Digital Institute at Newcastle University. He is PI of the EPSRC Centre for Doctoral Training in Cloud Computing for Big Data as well as the £30M National Innovation Centre for Data. He previously directed the £12M RCUK-funded Digital Economy Hub on Social Inclusion through the Digital Economy which focussed on using advanced computing technologies to transform the lives of older people and those with disabilities. He graduated in 1983 with a BSc in Computer Engineering from Manchester University; followed by a PhD on parallel computing in 1986. In the 80s; as a Lecturer at Manchester University; he was a designer of the Alvey Flagship and Esprit EDS systems. From 1990-5 he worked for ICL as a system designer of the Goldrush MegaServer parallel database server; which was released as a product in 1994.; In August 1995 he moved to Newcastle University; where he has led a range of research projects. His research interest is in scalable information management with a current focus on Data Analytics and IoT. He sits on the board of Dynamo North East; an industry-led organisation created to grow the IT economy of the region. He is also a member of the Department for Transport Science Advisory Council. Professor Watson is a Fellow of the Royal Academy of Engineering; a Fellow of the British Computer Society; a Chartered Engineer and a member of the UK Computing Research Committee. He received the 2014 Jim Gray eScience Award.; Paul's research focuses on designing; building and evaluating the next generation of scalable data analytics systems. He uses real-world applications – mainly drawn from healthcare - to inspire and evaluate our research.  Current work includes:; - the automatic partitioning; deployment and scaling of distributed IoT systems based on declarative descriptions of functional and non-functional properties. We apply these tools and techniques to systems that typically span wearables; mobile phones; and clouds; ensuring that they meet performance; security; bandwidth and energy requirements.; - the e-Science Central end-to-end data analytics platform that supports the ingestion; storage; sharing; discovery and analysis of data. This underpins a range of projects and labs; in both universities and industry; ,Paul's research focuses on designing; building and evaluating the next generation of scalable data analytics systems. He uses real-world applications – mainly drawn from healthcare - to inspire and evaluate our research.  Current work includes:; - the automatic partitioning; deployment and scaling of distributed IoT systems based on declarative descriptions of functional and non-functional properties. We apply these tools and techniques to systems that typically span wearables; mobile phones; and clouds; ensuring that they meet performance; security; bandwidth and energy requirements.; - the e-Science Central end-to-end data analytics platform that supports the ingestion; storage; sharing; discovery and analysis of data. This underpins a range of projects and labs; in both universities and industry; ,['N/A'],Professor Paul Watson 
paul-wilcox,Complexity (Algorithms); Data structures; Databases; Applications (Machine learning); Stochastic (Mathematical modelling); Software framework development; Uncertainty quantification; Time series; Probability; ,Paul D. Wilcox was born in Nottingham in 1971. He received an MEng degree in Engineering Science from the University of Oxford in 1994 and a PhD from Imperial College London in 1998. He remained in the Non-Destructive Testing (NDT) research group at Imperial College as a Research Associate until 2002; working on the development of guided wave array transducers for large area inspection. Since 2002; he has been with the Department of Mechanical Engineering at the University of Bristol where his current title is Professor of Dynamics. He held an EPSRC Advanced Research Fellowship in Quantitative Structural Health Monitoring from 2007 to 2012 and was Head of the Mechanical Engineering Department from 2015 to 2018. His research interests include array transducers; embedded sensors; ultrasonic particle manipulation; long-range guided wave inspection; structural health monitoring; elastodynamic scattering; data analysis and signal processing. In 2015 he was a co-founder of Inductosense Ltd.; a spin-out company which is commercialising inductively-coupled embedded ultrasonic sensors.; This pilot project will apply data science to exploit the full potential of quantitative Non-Destructive Evaluation (NDE) measurements of engineering assets; ranging from power stations to aeroplanes. Currently; the majority of NDE data is irreversibly condensed after a measurement is made; sometimes to as little as a binary pass/fail classification. In doing so; a wealth of useful information that could impact on the future safety and economic utility of an asset is lost forever. However; as the fourth industrial revolution unfolds; the enabling infrastructure to store raw NDE data from every inspection is becoming available in the form of scalable; cloud-based; asset-management platforms.; Permanent storage of raw NDE measurement data will become the norm; as it future-proofs historic measurements against new processing techniques as they become available. From a data science perspective this also presents a massive opportunity to extract much more information about the integrity of an engineering asset. NDE data from multiple measurement modalities can be integrated and analysed collectively in the context of historic measurements and data streams from other condition-monitoring modalities.; The specific objectives of the pilot project are to:;  ; ,This pilot project will apply data science to exploit the full potential of quantitative Non-Destructive Evaluation (NDE) measurements of engineering assets; ranging from power stations to aeroplanes. Currently; the majority of NDE data is irreversibly condensed after a measurement is made; sometimes to as little as a binary pass/fail classification. In doing so; a wealth of useful information that could impact on the future safety and economic utility of an asset is lost forever. However; as the fourth industrial revolution unfolds; the enabling infrastructure to store raw NDE data from every inspection is becoming available in the form of scalable; cloud-based; asset-management platforms.; Permanent storage of raw NDE measurement data will become the norm; as it future-proofs historic measurements against new processing techniques as they become available. From a data science perspective this also presents a massive opportunity to extract much more information about the integrity of an engineering asset. NDE data from multiple measurement modalities can be integrated and analysed collectively in the context of historic measurements and data streams from other condition-monitoring modalities.; The specific objectives of the pilot project are to:;  ; ,['N/A'],Professor Paul Wilcox 
pawan-kumar,['N/A'],M. Pawan Kumar is an Associate Professor of Information Engineering at the University of Oxford and a fellow of Lady Margaret Hall. He is a principal researcher in the OVAL group; which focuses on the design and analysis of discrete and continuous optimization algorithms for the problems encountered in computer vision and machine learning.;  ; At the Turing; he plans to explore various problems that are central to scaling up machine learning for visual data. Examples include efficient and principled optimization algorithms for deep neural networks; learning from large-scale weakly supervised data such as freely downloadable images or videos on the Internet that have sparse and often noisy tags; compressing neural networks for memory and speed efficiency; and fast combinatorial algorithms and relaxation-based approaches for structured prediction.; ,At the Turing; he plans to explore various problems that are central to scaling up machine learning for visual data. Examples include efficient and principled optimization algorithms for deep neural networks; learning from large-scale weakly supervised data such as freely downloadable images or videos on the Internet that have sparse and often noisy tags; compressing neural networks for memory and speed efficiency; and fast combinatorial algorithms and relaxation-based approaches for structured prediction.; ,['N/A'],Professor Pawan Kumar 
pedro-rodriguez-cutillas,Databases; ,Pedro R. Cutillas is a biochemist with expertise in the development of computational methods for the analysis of complex biological data. He graduated with a PhD in 2004 from University College London. His studies used advanced proteomics methods in a project that investigated kidney physiology. He then completed postdoctoral training at the Ludwig Institute for Cancer Research (UCL branch). In 2007; He obtained a lectureship at Bart's School of Medicine (Centre for Cell Signalling). After a period in the MRC Clinical Sciences Centre (2012-2013); where he was Head of the Mass Spectrometry and Proteomics; he joined the Barts Cancer Institute at Bart's School of Medicine in 2013 as Reader in Cell Signalling and Proteomics.; Dr Cutillas uses machine learning and other computational methods to investigate how the biochemistry of cancer cells affects their responses to treatment. He focuses on a group of drugs named kinase inhibitors; which target biochemical pathways involved in cell communication. It is now well established that essentially all tumours over-activate a group of enzymes named kinases; whose function in normal cells is to regulate metabolism; cell division and cell identity; tumour cells dysregulate this biochemical machinery leading to uncontrolled proliferation and invasion. There are over 500 different kinase genes in humans; forming an intricate network of biochemical reactions. However; the precise nature of kinase dysregulation varies among patients; making it challenging to identify the kinase inhibitor drug that may be most effective to treat a given patient.; Dr Cutillas uses advanced computational methods to interrogate large scale proteomic and biochemical data obtained from tumour biopsies. By associating different markers of kinase network circuitry with the phenotype of cancer cells; Dr Cutillas is providing insights into the fundamental properties of cancer biochemistry and he is also is developing predictive algorithms for personalised cancer medicine.; ,Dr Cutillas uses machine learning and other computational methods to investigate how the biochemistry of cancer cells affects their responses to treatment. He focuses on a group of drugs named kinase inhibitors; which target biochemical pathways involved in cell communication. It is now well established that essentially all tumours over-activate a group of enzymes named kinases; whose function in normal cells is to regulate metabolism; cell division and cell identity; tumour cells dysregulate this biochemical machinery leading to uncontrolled proliferation and invasion. There are over 500 different kinase genes in humans; forming an intricate network of biochemical reactions. However; the precise nature of kinase dysregulation varies among patients; making it challenging to identify the kinase inhibitor drug that may be most effective to treat a given patient.; Dr Cutillas uses advanced computational methods to interrogate large scale proteomic and biochemical data obtained from tumour biopsies. By associating different markers of kinase network circuitry with the phenotype of cancer cells; Dr Cutillas is providing insights into the fundamental properties of cancer biochemistry and he is also is developing predictive algorithms for personalised cancer medicine.; ,['N/A'],Dr Pedro Rodriguez Cutillas 
peter-boyle,['N/A'],Peter received his PhD in Particle Physics from the University of Edinburgh and has been a reader in the Theoretical Particle Physics research group at Edinburgh since 2004. Peter is a member of the RBC-UKQCD collaboration; and collaborates with Columbia University; Southampton; Brookhaven National Laboratory; RIKEN Brookhaven Research Center; University of Virginia; University of Connecticut.;  ; Low energy Quantum chromodynamics using computer simulation; and particularly matrix elements of hadrons that are required to constrain fundamental parameters of the standard model and search for physics beyond the standard model. Peter believes his publications represent the best constraints on Vus and BK which enter the famous unitarity triangle. Subtopics: Kaon matrix elements - BK; Kl3 ; K->pi pi. Non-perturbative renormalisation of Lattice operators. Chiral lagrangian. Hadron spectrum and decay constants. Electromagnetic effects.; ,Low energy Quantum chromodynamics using computer simulation; and particularly matrix elements of hadrons that are required to constrain fundamental parameters of the standard model and search for physics beyond the standard model. Peter believes his publications represent the best constraints on Vus and BK which enter the famous unitarity triangle. Subtopics: Kaon matrix elements - BK; Kl3 ; K->pi pi. Non-perturbative renormalisation of Lattice operators. Chiral lagrangian. Hadron spectrum and decay constants. Electromagnetic effects.; ,['N/A'],Professor Peter Boyle 
peter-challenor,Complexity (Algorithms); Numerical (Algorithms); Dynamical systems & differential equations; Numerical analysis; Deterministic (Mathematical modelling); Stochastic (Mathematical modelling); Stochastic optimisation; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Non-parametric & semi-parametric methods; Simulation; Modelling (Statistical methods & theory); ,Peter Challenor is a professor in the statistical sciences group of the Department of Mathematics of the University of Exeter. Before joining Exeter he spent many years working for NERC at the National Oceanography Centre in Southampton.; In order to understand the world we use models. These may be derived from the laws of physics and involve the use of partial differential equations or they may be empirical and based on the analysis of data. Such models are increasingly used in aid in decision making in the real world. Such models are never perfect and this needs to be taken into account when they are used in decision making. Professor Peter Challenor's research is concerned with the development of methods to quantify the uncertainty in such circumstances. Applications include models of the environment; climate; engineering and healthcare.; ,In order to understand the world we use models. These may be derived from the laws of physics and involve the use of partial differential equations or they may be empirical and based on the analysis of data. Such models are increasingly used in aid in decision making in the real world. Such models are never perfect and this needs to be taken into account when they are used in decision making. Professor Peter Challenor's research is concerned with the development of methods to quantify the uncertainty in such circumstances. Applications include models of the environment; climate; engineering and healthcare.; ,['N/A'],Professor Peter Challenor 
peter-flach,Robotics; Pattern formation; Pattern recognition; Supervised learning; Ethics; Causality; Logic (Theoretical mathematics); ,Peter Flach is Professor of Artificial Intelligence at the University of Bristol. His main research is in the area of mining highly structured data and the evaluation and improvement of machine learning models using ROC analysis. He has also published on the logic and philosophy of machine learning; and on the combination of logic and probability. He is author of Simply Logical: Intelligent Reasoning by Example (John Wiley; 1994) and Machine Learning: the Art and Science of Algorithms that Make Sense of Data (Cambridge University Press; 2012).Professor Flach is the Editor-in-Chief of the Machine Learning journal; one of the two top journals in the field that has been published for over 25 years by Kluwer and now Springer-Nature. He was Programme Co-Chair of the 1999 International Conference on Inductive Logic Programming; the 2001 European Conference on Machine Learning; the 2009 ACM Conference on Knowledge Discovery and Data Mining; and the 2012 European Conference on Machine Learning and Knowledge Discovery in Databases in Bristol. He is a founding board member of the European Association for Data Science (EuADS.org). ; The fundamental question of measurement theory is: how to assign meaningful numbers to objects that are not themselves numbers. Issues of measurement are of particular importance in inductive sciences including data science and artificial intelligence; for example when one assesses the capability of models and learning algorithms to generalise beyond the observed data.; Nevertheless; measurement concepts are underdeveloped in data science and AI; in at least the following senses: (i) a wide-spread under-appreciation of the importance and effects of measurement scales; and (ii) the fact that in most cases the quantity of interest is latent; i.e. not directly observable. This Pilot Project will seek to advance understanding of capabilities and skills of models and algorithms in data science and AI; and how to measure those capabilities and skills. Just as psychometrics has developed tools to model the skills of a human learner and develop standardised (SAT) tests; there is a need for similar tools to model the skills of learning machines and for standardised benchmarks which will allow skill assessment with only a few well-chosen test sets.; ,The fundamental question of measurement theory is: how to assign meaningful numbers to objects that are not themselves numbers. Issues of measurement are of particular importance in inductive sciences including data science and artificial intelligence; for example when one assesses the capability of models and learning algorithms to generalise beyond the observed data.; Nevertheless; measurement concepts are underdeveloped in data science and AI; in at least the following senses: (i) a wide-spread under-appreciation of the importance and effects of measurement scales; and (ii) the fact that in most cases the quantity of interest is latent; i.e. not directly observable. This Pilot Project will seek to advance understanding of capabilities and skills of models and algorithms in data science and AI; and how to measure those capabilities and skills. Just as psychometrics has developed tools to model the skills of a human learner and develop standardised (SAT) tests; there is a need for similar tools to model the skills of learning machines and for standardised benchmarks which will allow skill assessment with only a few well-chosen test sets.; ,['N/A'],Professor Peter Flach 
peter-richtarik,['N/A'],Peter Richtarik is a Reader in the School of Mathematics at the University of Edinburgh; and is the Head of a Big Data Optimization Lab. He received his PhD from Cornell University in 2007; and currently holds an EPSRC Early Career Fellowship in Mathematical Sciences.  ;  ; Peter's main research focus is the development of new optimisation algorithms and theory. In particular; much of his recent work is in the emerging field of big data optimisation; with applications in machine learning in general and empirical risk minimisation in particular. For big data optimisation problems; traditional methods are no longer suitable; and hence there is need to develop new algorithmic paradigms. An important role in this respect is played by randomised algorithms of various flavors; including randomised coordinate descent; stochastic gradient descent; randomised subspace descent and randomised quasi-Newton methods. Parallel and distributed variants are of particular importance.; ,Peter's main research focus is the development of new optimisation algorithms and theory. In particular; much of his recent work is in the emerging field of big data optimisation; with applications in machine learning in general and empirical risk minimisation in particular. For big data optimisation problems; traditional methods are no longer suitable; and hence there is need to develop new algorithmic paradigms. An important role in this respect is played by randomised algorithms of various flavors; including randomised coordinate descent; stochastic gradient descent; randomised subspace descent and randomised quasi-Newton methods. Parallel and distributed variants are of particular importance.; ,['N/A'],Dr Peter Richtarik 
peter-smith,['N/A'],Peter Smith is Professor of Social Statistics within Social Statistics & Demography at the University of Southampton. He has worked at the University for over 25 years. He obtained a First Class BSc in Mathematics in 1986 from Lancaster University; and returned there to complete a PhD in Statistics in 1990; having obtained an MSc in Probability and Statistics with Distinction in 1987 from the University of Sheffield.; His publications include articles in the Journal of the Royal Statistical Society; Series A; B and C; Biometrika and the Journal of the American Statistical Association. Peter was awarded the Royal Statistical Society Guy Medal in Bronze in 1999 and was Joint Editor of Series C of their Journal from 2013 to 2016.; Peter has research interests in developing new statistical methodology; including methods for handling non-response and for modelling longitudinal data; and applying sophisticated statistical methods to problems in demography; medicine and health sciences.; ,Peter has research interests in developing new statistical methodology; including methods for handling non-response and for modelling longitudinal data; and applying sophisticated statistical methods to problems in demography; medicine and health sciences.; ,['N/A'],Professor Peter Smith 
peter-tennant,Complexity (Algorithms); Research methods; Causality; Graph theory; ,Peter is a translational data scientist with a primary focus on understanding; adapting; and applying 'causal inference' methods to improve health and social science research. Peter is based at the Leeds Institute for Data Analytics; University of Leeds; where he teaches on their MSc in Health Data Analytics and Summer School in Causal Inference with Observational Data. He is also the University of Leeds representative on the UK Reproducibility Network.; Peter's Turing Fellowship focuses on translating and adapting graphical methods for causal inference from their theoretical origins into the complex setting of applied health and social science research. This involves: 1) Developing training and guidance on the application and reporting of causal diagrams and directed acyclic graphs; 2) Adapting and developing notation to improve detection and understanding of common analytical errors; particularly arising from the analysis of deterministic variables and 3) Demonstrating the utility and insights of 'causal inference' for resolving areas of debate and confusion in applied health and social science research.; ,Peter's Turing Fellowship focuses on translating and adapting graphical methods for causal inference from their theoretical origins into the complex setting of applied health and social science research. This involves: 1) Developing training and guidance on the application and reporting of causal diagrams and directed acyclic graphs; 2) Adapting and developing notation to improve detection and understanding of common analytical errors; particularly arising from the analysis of deterministic variables and 3) Demonstrating the utility and insights of 'causal inference' for resolving areas of debate and confusion in applied health and social science research.; ,['N/A'], Peter Tennant 
peter-triantafillou,['N/A'],Peter Triantafillou is Professor of Data Systems and Head of the Data Sciences Division at the Department of Computer Science at the University of Warwick; Fellow of the Alan Turing Institute ; member of the Advisory Board of the Urban Big Data Research Centre (a national infrastructure for urban data services and analytics); and Honorary Senior Research Fellow at the School of Computing Science at the University of Glasgow. Prior to that; he held the Data Systems Chair at the School of Computing Science at the University of Glasgow.; Peter has also held professorial positions at Simon Fraser University (1991-1995); the Technical University of Crete (1995-2002); the University of Patras (2002-2013); and visiting professorships at the Max-Planck Institute for Informatics (in 2004-2005 and in 2012-2013). Peter received his Ph.D. in computer science from the University of Waterloo and was the Department of Computer Science and the Faculty of Mathematics nominee for the Gold Medal for outstanding achievements at the Doctoral level.;  ; Peter's research efforts in general aim to push the state of the art in our understanding of how:; At The Alan Turing Institute; Peter's research will specifically focus on the intersection and cross-fertilisation of machine learning and data systems: How to employ ML models and approaches in order to improve the efficiency; scalability and accuracy of data analytics systems; and; conversely; how to use advanced data management algorithms and structures in order to improve the performance of time-consuming and resource-hungry machine learning tasks.; Peter has published extensively in top journals and conferences in the above areas; has served in the Technical Program Committees of more than 120 international conferences; and has been the PC Chair or Vice-chair in several prestigious conferences. Peter has received the best paper awards at the ACM SIGIR Conference (on Information Retrieval) in 2016 and at the ACM CIKM Conference (on Information and Knowledge Management) in 2006 and is a co-designer of several innovative systems (such as the MINERVA decentralized search engine and the eXO decentralized social networking system).; ,Peter's research efforts in general aim to push the state of the art in our understanding of how:; At The Alan Turing Institute; Peter's research will specifically focus on the intersection and cross-fertilisation of machine learning and data systems: How to employ ML models and approaches in order to improve the efficiency; scalability and accuracy of data analytics systems; and; conversely; how to use advanced data management algorithms and structures in order to improve the performance of time-consuming and resource-hungry machine learning tasks.; ,Peter has published extensively in top journals and conferences in the above areas; has served in the Technical Program Committees of more than 120 international conferences; and has been the PC Chair or Vice-chair in several prestigious conferences. Peter has received the best paper awards at the ACM SIGIR Conference (on Information Retrieval) in 2016 and at the ACM CIKM Conference (on Information and Knowledge Management) in 2006 and is a co-designer of several innovative systems (such as the MINERVA decentralized search engine and the eXO decentralized social networking system).; ,Professor Peter Triantafillou 
petra-vertes,Control theory; Neural networks; Neuroscience; Graph theory; Cognitive science; Time series; ,Petra Vertes is an MQ fellow at the University of Cambridge. She gained her MSci in theoretical physics and a PhD in artificial neural networks from the University of Cambridge. She then moved to the Brain Mapping Unit in the Department of Psychiatry for her postdoctoral work. In 2014 she was awarded an MRC fellowship in bioinformatics; followed by an MQ fellowship in 2018. Petra is also one of the co-founders and organisers of the Cambridge Networks Network (CNN) - a forum of over 450 academics across different disciplines who share an interest in network science (http://www.cnn.group.cam.ac.uk/).; Petra's research applies tools from physics; engineering and data science to fundamental problems in neuroscience and mental health. In particular; she is interested in understanding the biological basis for complex neuropsychiatric disorders such as schizophrenia and depression; and use this understanding for patient benefit. Petra has worked extensively with human brain networks; where nodes represent large-scale anatomical brain regions and links represent structural or functional connections derived from neuroimaging data. These methods have proven important in describing human brain organisation and how it changes with age; cognitive demand and disease.; However; developing principled prognostics and interventions will depend upon our understanding of how microscopic biological mechanisms shape macroscopic brain networks in health and in disease. For this reason; Petra's recent work has focused on integrating datasets across multiple scales; from large-scale neuroimaging data down to the cellular; molecular and genetic level. Her goal is to understand which genes and biological processes lead to specific abnormalities in human brain networks observed in various psychiatric disorders. She is also interested in developing peripheral biomarkers (measurable via blood tests or simple sensors) for early detection and personalized treatment in psychiatry.; Petra works with a wide variety of multivariate data; from neuroimaging to microarray; DNA; protein interaction networks; flow cytometry and data from wearable devices. Alongside her work on human brain networks; Petra also studies simpler organisms; such as C. elegans; which provide a test-bed for methodological innovations as well as insights into generalisable aspects of brain organisation; brain development; network dysfunction and repair.; ,Petra's research applies tools from physics; engineering and data science to fundamental problems in neuroscience and mental health. In particular; she is interested in understanding the biological basis for complex neuropsychiatric disorders such as schizophrenia and depression; and use this understanding for patient benefit. Petra has worked extensively with human brain networks; where nodes represent large-scale anatomical brain regions and links represent structural or functional connections derived from neuroimaging data. These methods have proven important in describing human brain organisation and how it changes with age; cognitive demand and disease.; However; developing principled prognostics and interventions will depend upon our understanding of how microscopic biological mechanisms shape macroscopic brain networks in health and in disease. For this reason; Petra's recent work has focused on integrating datasets across multiple scales; from large-scale neuroimaging data down to the cellular; molecular and genetic level. Her goal is to understand which genes and biological processes lead to specific abnormalities in human brain networks observed in various psychiatric disorders. She is also interested in developing peripheral biomarkers (measurable via blood tests or simple sensors) for early detection and personalized treatment in psychiatry.; Petra works with a wide variety of multivariate data; from neuroimaging to microarray; DNA; protein interaction networks; flow cytometry and data from wearable devices. Alongside her work on human brain networks; Petra also studies simpler organisms; such as C. elegans; which provide a test-bed for methodological innovations as well as insights into generalisable aspects of brain organisation; brain development; network dysfunction and repair.; ,['N/A'], Petra Vertes 
petros-dellaportas,['N/A'],Petros Dellaportas is a Professor of Statistical Science at the University College London. Previously he held academic positions in Athens University of Economics and Business; Greece.  He has published in Annals of Statistics; Biometrika; JRSSB; Statistical Science; Journal of Business and Economics Statistics; etc.  He is currently associate editor of Biometrika and Electronic journal of Statistics.; His research interests include Bayesian theory and applications; computational statistics; financial modelling. In particular he currently works in problems of multivariate stochastic volatility; Gaussian processes; Bayesian mixture hierarchical models; modelling of high-frequency financial data; modelling of sports data.; ,His research interests include Bayesian theory and applications; computational statistics; financial modelling. In particular he currently works in problems of multivariate stochastic volatility; Gaussian processes; Bayesian mixture hierarchical models; modelling of high-frequency financial data; modelling of sports data.; ,['N/A'],Professor Petros Dellaportas 
philip-dawid,Statistical methods & theory; Probability; Information theory (Statistical methods & theory); Causality; Uncertainty quantification; ,Philip Dawid is Emeritus Professor of Statistics of the University of Cambridge; and a Fellow of Darwin College; Cambridge. He is a leading proponent of Bayesian statistics.; Dawid has made fundamental contributions to both the philosophical underpinnings and the practical applications of statistics. His theory of conditional independence is a keystone of modern statistical theory and methods; and he has demonstrated its usefulness in a host of applications; including computation in probabilistic expert systems; causal inference; and forensic identification. Dawid was lecturer in statistics at University College London from 1969 to 1978. He was subsequently Professor of Statistics at City University; London until 1981; when he returned to UCL as a reader; becoming Pearson Professor of Statistics there in 1982. He moved to the University of Cambridge where he was appointed Professor of Statistics in 2007; retiring in 2013.; ,['N/A'],['N/A'],Professor Philip Dawid FRS
philip-greulich,Applied mathematics; Dynamical systems & differential equations; Mathematical modelling; Deterministic (Mathematical modelling); Stochastic (Mathematical modelling); Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; ,Philip has been Lecturer in Applied Mathematics at the University of Southampton since 2016; and since then has been working in mathematical modelling of stem cell dynamics and using statistical inference of cell lineage data to determine the rules of stem cell fate.; Philip received his PhD in Theoretical Physics in 2010 from the University of the Saarland; Saarbruecken; Germany; with a project in mathematical/computational modelling of intra-cellular transport. He went on to work as Postdoctoral researcher at the University of Edinburgh 2010-2012 further expanding his expertise in mathematical modelling of various biomedical problems (intracellular transport; evolution of antibiotic resistance; antibiotic mode of action). From 2013-2016 Philip was postdoctoral researcher at the Cavendish Laboratory; University of Cambridge; during which Philip started modelling of stem cell dynamics on cell lineage data from mouse oesophagus under Notch-inhibition and in oesophageal tumours.; Philip is working with cell lineage data coming from tracing the progeny of stem cells in adult mouse tissues (mammary gland; oesophagus; brain); and with corresponding high-dimensional single-cell transcriptomics data; which characterises cells via their RNA content. Philip using mathematical/computational modelling and statistical (Bayesian) inference to determine the ways how stem cells divide and differentiate; and how this affects the composition of tissue; and the risk of developing cancer. In that work; Philip works with experimental collaborators from Southampton; Cambridge; and Turin; Italy; who do cell lineage tracing experiments with mice that provide the data which; together with mathematical modelling/statistical inference; helps to acquire deep insights about how stem cells build tissues.; Philip has hold a fellowship from the German Academic Exchange Service from 2010-2012; a German Research Society Fellowship from 2013-2014; and an Associated Fellowship of Clare Hall College; Cambridge.; ,Philip is working with cell lineage data coming from tracing the progeny of stem cells in adult mouse tissues (mammary gland; oesophagus; brain); and with corresponding high-dimensional single-cell transcriptomics data; which characterises cells via their RNA content. Philip using mathematical/computational modelling and statistical (Bayesian) inference to determine the ways how stem cells divide and differentiate; and how this affects the composition of tissue; and the risk of developing cancer. In that work; Philip works with experimental collaborators from Southampton; Cambridge; and Turin; Italy; who do cell lineage tracing experiments with mice that provide the data which; together with mathematical modelling/statistical inference; helps to acquire deep insights about how stem cells build tissues.; ,Philip has hold a fellowship from the German Academic Exchange Service from 2010-2012; a German Research Society Fellowship from 2013-2014; and an Associated Fellowship of Clare Hall College; Cambridge.; ,Dr Philip Greulich 
philip-hamann,Data structures; Numerical (Algorithms); Information theory (Applied mathematics); Multi-agent systems; Numerical analysis; Operations research; Robotics; Neural networks; Pattern formation; Systems theory; Communications; Databases; Human computer interface; Computing networks; Neural & evolutionary computing; Operating systems; Real time computing; Visualisation (Computer systems & architectures); Applications (Machine learning); Computer vision; Deep learning; Natural language processing; Pattern recognition; Speech recognition; Unsupervised learning; Differential privacy; Identity management; Verification; Ethics; Social networks; Research methods; Social media; Uncertainty quantification; Causality; Monte Carlo methods; Non-parametric & semi-parametric methods; Simulation; Time series; Information theory (Statistical methods & theory); Modelling (Statistical methods & theory); Probability; ,Dr Hamann is an academic rheumatologist and is currently an NIHR Clinical Lecturer in Rheumatology at the University of Bristol and an NHS Clinical Entrepreneur Fellow. He completed his PhD in musculoskeletal epidemiology at the University of Bath. Using data from over 14;000 patients; from the British Society for Rheumatology Biologics Register he investigated associations between clinical factors (age; gender; BMI etc.) and long-term response to anti-tumour necrosis factor (a high-cost drug used to treat rheumatoid arthritis) and used latent-class mixed models to map trajectories of response to this class of drugs. His research interests centre on epidemiology and statistical modelling in large datasets to develop predictive algorithms to improve care for patients with rheumatoid arthritis. He is also passionate about the use of technology to deliver patient-focused care and the use of real-world and real-time data to make sure patients receive the right treatment at the right time. Dr Hamann conceptualised and developed an award-winning smartphone app and cloud-based software in collaboration with industry partners which allows patients to securely record and report rheumatoid arthritis disease activity using validated patient-reported outcomes which is in currently clinical use in the NHS.; Rheumatoid arthritis (RA) is the most common autoimmune inflammatory arthritis affecting 0.8% of the UK population. The impact of the disease is estimated to cost the UK £4.8 billion/annum; with estimated costs to the NHS of £560 million/annum. RA is a chronic disease requiring a range of involvement from community to secondary care that varies over time. The problem with current clinical management of RA is that it is based on infrequent prescheduled hospital appointments; with no monitoring of symptoms between. Thus; when a person with RA is seen they may be in remission and with no change to their treatment or may have experienced a flare-up; during which they did not receive timely treatment or support they needed. Self-management; patient engagement and empowerment; and better signposting to resources improves health outcomes. Many resources are available; online and within care settings; and people who identify specific goals and empowered to achieve them report better outcomes. Validated questionnaires measure an individual's engagement and disease severity; and can be completed remotely on a regular basis to develop a record of disease activity. Dr Hamann's research aims to understand how RA disease activity; physical activity and patient motivation changes over time; and how this might allow healthcare interventions (physio therapy; appointments etc.) to be targeted to improve outcomes for people with RA. He also hopes that by making sure that people with RA see the right member of the healthcare team at the right time; better and more efficient healthcare can be provided.; ,Rheumatoid arthritis (RA) is the most common autoimmune inflammatory arthritis affecting 0.8% of the UK population. The impact of the disease is estimated to cost the UK £4.8 billion/annum; with estimated costs to the NHS of £560 million/annum. RA is a chronic disease requiring a range of involvement from community to secondary care that varies over time. The problem with current clinical management of RA is that it is based on infrequent prescheduled hospital appointments; with no monitoring of symptoms between. Thus; when a person with RA is seen they may be in remission and with no change to their treatment or may have experienced a flare-up; during which they did not receive timely treatment or support they needed. Self-management; patient engagement and empowerment; and better signposting to resources improves health outcomes. Many resources are available; online and within care settings; and people who identify specific goals and empowered to achieve them report better outcomes. Validated questionnaires measure an individual's engagement and disease severity; and can be completed remotely on a regular basis to develop a record of disease activity. Dr Hamann's research aims to understand how RA disease activity; physical activity and patient motivation changes over time; and how this might allow healthcare interventions (physio therapy; appointments etc.) to be targeted to improve outcomes for people with RA. He also hopes that by making sure that people with RA see the right member of the healthcare team at the right time; better and more efficient healthcare can be provided.; ,['N/A'],Dr Philip Hamann 
phillip-stanley-marbell,['N/A'],Phillip is an Assistant Professor (UL) in the Department of Engineering at the University of Cambridge. Prior to joining Cambridge; he was a researcher at MIT; from 2014 to 2017. He received his Ph.D. from CMU in 2007; was a postdoc at TU Eindhoven until 2008; and then a permanent Research Staff Member at IBM Research—Zurich. In 2012 he joined Apple where he led the development of a new system component now used across all iOS and macOS platforms. Prior to completing his Ph.D.; he held positions at Bell-Labs Research; Lucent; Philips; and NEC Research Labs.; His research interests are in computing system hardware; architectures; and algorithms that interact with the physical world and which harness an understanding of the physical world and the flexibility of sensing systems for improved energy-efficiency and performance; and for reduced measurement uncertainty. In collaboration with colleagues at the Alan Turing Institute; Phillip is exploring new research challenges on sensor-rich materials; and on computing hardware and embedded sensing architectures which facilitate tracking measurement uncertainty throughout the sensing and computation process.; ,His research interests are in computing system hardware; architectures; and algorithms that interact with the physical world and which harness an understanding of the physical world and the flexibility of sensing systems for improved energy-efficiency and performance; and for reduced measurement uncertainty. In collaboration with colleagues at the Alan Turing Institute; Phillip is exploring new research challenges on sensor-rich materials; and on computing hardware and embedded sensing architectures which facilitate tracking measurement uncertainty throughout the sensing and computation process.; ,['N/A'],Dr Phillip Stanley-Marbell 
phyllis-illari,['N/A'],Phyllis Illari is Associate Professor of History and Philosophy of Science at UCL. She is primarily research active in the philosophy of science; with particular expertise in causality; mechanisms and information and data. She recently completed a major AHRC project 'Evaluating Evidence in Medicine'.With Federica Russo; Dr Illari is Editor-in-Chief of the European Journal for Philosophy of Science; and is also associate editor of Philosophy & Technology. Finally; she has served on the Data Ethics Group of the Alan Turing Institute since 2016.; Dr Illari serves on the Data Ethics Group of The Alan Turing Institute; which consults on many urgent initiatives improving data ethics practices nationally.; ,Dr Illari serves on the Data Ethics Group of The Alan Turing Institute; which consults on many urgent initiatives improving data ethics practices nationally.; ,['N/A'],Dr Phyllis Kirstin Illari 
prabhakar-rajan,Neural networks; Databases; Information retrieval; Operating systems; Deep learning; Simulation; ,"Dr Prabhakar Rajan has been a Group Leader at Barts Cancer Institute; Queen Mary University of London; and a Consultant Urological and Robotic Surgeon at University College London Hospitals and Barts Health NHS Trusts since 2015. He completed his PhD in prostate cancer biology at Newcastle University in 2008; and undertook post-doctoral research at the Cancer Research UK Beatson Institute; Glasgow between 2008 and 2015. Dr Rajan studied physiology and medicine at University of Cambridge; and trained in surgery; urology; and robotics in Edinburgh; Glasgow; and Stockholm; Sweden; respectively. ; Dr Rajan is interested in the utilization of ""real world"" data to understand the determinants of clinical outcomes following prostate cancer treatments. For example; he recently studied the impact of medical comorbidities on the survival of men with prostate cancer using a large population-based registry of over 100;000 men (Rajan et al; 2017 JCO). As part of his clinical role; he is developing interests in computational modelling using patient data captured via electronic health records and data systems. His ultimate goal is to utilise data science to improve the lives of patients with prostate cancer.; ","Dr Rajan is interested in the utilization of ""real world"" data to understand the determinants of clinical outcomes following prostate cancer treatments. For example; he recently studied the impact of medical comorbidities on the survival of men with prostate cancer using a large population-based registry of over 100;000 men (Rajan et al; 2017 JCO). As part of his clinical role; he is developing interests in computational modelling using patient data captured via electronic health records and data systems. His ultimate goal is to utilise data science to improve the lives of patients with prostate cancer.; ",['N/A'],Dr Prabhakar Rajan 
pramod-bhatotia,['N/A'],Pramod Bhatotia is a Senior Lecturer in the School of Informatics at the University of Edinburgh; and a Faculty Fellow at the Alan Turing Institute. Before moving to the UK; Pramod was a Research Group Leader at TU Dresden; where he led the Parallel and Distributed Systems group. Pramod graduated with a PhD (2015) from the Systems group at the Max Planck Institute for Software Systems (MPI-SWS). During his PhD studies; he interned/collaborated with Microsoft Research; IBM Research; Yahoo! Research; and Bell Labs. Prior to joining MPI-SWS; Pramod was a member of technical staff in the HPC group at IBM Research.; ,['N/A'],['N/A'],Dr Pramod Bhatotia 
primoz-skraba,Data structures; Neural networks; Nonlinear dynamics; Deep learning; High dimensional inference; Geometry & topology; ,Primoz Skraba is a Senior Lecturer in applied and computational topology. His research is broadly related to data analysis with an emphasis on topological data analysis. Generally; the problems he considers span both theory and applications. On the theory side; areas of interest include stability and approximation of algebraic invariants; stochastic topology (the topology of random spaces); and algorithmic research.; On the applications side; he focuses on combining topological ideas with machine learning; optimisation; and other statistical tools. Other applications areas of interest include visualisation and geometry processing. He received a PhD in Electrical Engineering from Stanford University in 2009 and has held positions at INRIA in France and the Jozef Stefan Institute; the University of Primorska; and the University of Nova Gorica in Slovenia; before joining Queen Mary University of London in 2018.; Data analysis is about understanding relationships – relationships between variables or different parts of data sets. Topology is the branch of mathematics which studies global; qualitative properties of spaces based on local properties. Topological data analysis (TDA) applies techniques from this field to data analysis. Dr Skraba's research is on combining machine learning techniques which allow a computer to outperform human capability in tasks such as classification of images or playing strategic games (e.g. chess; go; etc.) with techniques from topology. This can provide a new generation of techniques and applications beyond what is possible today.; ,Data analysis is about understanding relationships – relationships between variables or different parts of data sets. Topology is the branch of mathematics which studies global; qualitative properties of spaces based on local properties. Topological data analysis (TDA) applies techniques from this field to data analysis. Dr Skraba's research is on combining machine learning techniques which allow a computer to outperform human capability in tasks such as classification of images or playing strategic games (e.g. chess; go; etc.) with techniques from topology. This can provide a new generation of techniques and applications beyond what is possible today.; ,['N/A'],Dr Primoz Skraba 
quentin-berthet,['N/A'],Quentin Berthet is a Lecturer in the Statslab; in the DPMMS at Cambridge; and a fellow of St John’s College; since 2015. He is a former student of the Ecole Polytechnique; received a Ph.D. from Princeton University in 2014; and was a CMI postdoctoral fellow at Caltech.;  ; Dr Berthet's research is focused on the theoretical understanding of statistical procedures. He studies fundamental problems related to modern datasets such as computational constraints; heterogeneity of the data or presence of errors. He is interested in taking into account societal concerns in the analysis of data; by considering ethical aspects (privacy; security of data); as well as ways to guide decision making in a data-driven way. He uses tools from Statistics and Probability Theory; as well as from Optimisation; Information Theory and Computer Science.; ,Dr Berthet's research is focused on the theoretical understanding of statistical procedures. He studies fundamental problems related to modern datasets such as computational constraints; heterogeneity of the data or presence of errors. He is interested in taking into account societal concerns in the analysis of data; by considering ethical aspects (privacy; security of data); as well as ways to guide decision making in a data-driven way. He uses tools from Statistics and Probability Theory; as well as from Optimisation; Information Theory and Computer Science.; ,['N/A'],Dr Quentin Berthet 
rachel-franklin,Data science of government & politics; Research methods; ,"Rachel Franklin recently joined Newcastle University as professor of geographical analysis in the University's initiative in Spatial Analytics and Modeling ([email protected]) and in the Centre for Urban and Regional Development Studies (CURDS). Prior to Newcastle; she was the associate director of Brown University's initiative in spatial structures in the social sciences (S4); in the U.S. She is trained as a quantitative human geographer and her research focus is in spatial demography and the interplay between spatial analytics and demographic change; in particular quantifying patterns; sources and impacts of spatial inequality.; A challenging aspect of urban analytics; in particular smart city technologies and implementation; is the potential to create or reinforce socio-economic and demographic spatial inequalities within urban areas in terms of types of data collected; spatial coverage; and uncertainty. Franklin's Turing project focuses on the identification of coverage gaps and the investigation of how such gaps are associated with locations of vulnerable populations. Project aims are to generate measures of sensor ""deserts"" and populations at risk for use with sensor data products; to increase understanding of links between dynamics of population movements and sensor coverage; and to facilitate the development of smart city 'best practices' that maximise the value of urban analytics data for policy making and assessment.; ","A challenging aspect of urban analytics; in particular smart city technologies and implementation; is the potential to create or reinforce socio-economic and demographic spatial inequalities within urban areas in terms of types of data collected; spatial coverage; and uncertainty. Franklin's Turing project focuses on the identification of coverage gaps and the investigation of how such gaps are associated with locations of vulnerable populations. Project aims are to generate measures of sensor ""deserts"" and populations at risk for use with sensor data products; to increase understanding of links between dynamics of population movements and sensor coverage; and to facilitate the development of smart city 'best practices' that maximise the value of urban analytics data for policy making and assessment.; ",['N/A'],Professor Rachel Franklin 
rajen-shah,Machine learning; Statistical methods & theory; High dimensional inference; ,Dr Rajen Dinesh Shah is a Lecturer in Statistics at the Statistical Laboratory; University of Cambridge; where he previously completed my PhD at Trinity College under the supervision of Professor Richard Samworth. He is a member of the Research Section committee of the Royal Statistical Society.; Dr Shah is broadly interested in the analysis of large-scale data and his work falls under the areas of high-dimensional statistics; machine learning and computational statistics. He primarily works on developing algorithms for learning from large-scale data; and understanding their theoretical properties. He is particularly interested in creating methods that are able to perform well under constraints on computation and memory. Recently he has worked on variable selection and uncertainty quantification in high-dimensional settings; hashing methods for compressing large-scale data; and randomised algorithms for detecting interactions.; ,['N/A'],['N/A'],Dr Rajen Shah 
ralph-schroeder,Identity management; Data science of government & politics; Ethics; Social media; Causality; ,Ralph Schroeder is Professor in Social Science of the Internet at the Oxford Internet Institute. He is also the director of its MSc programme in Social Science of the Internet. Before coming to Oxford University; he was Professor in the School of Technology Management and Economics at Chalmers University in Gothenburg (Sweden). His publications include Social Theory after the Internet: Media; Technology and Globalization (UCL Press; 2018) Knowledge Machines: Digital Transformations of the Sciences and Humanities (MIT Press; 2015; co-authored with Eric T. Meyer); 'An Age of Limits: Social Theory for the Twenty-First Century' (Palgrave Macmillan; 2013); 'Being There Together: Social Interaction in Virtual Environments' (Oxford University Press; 2010) and 'Rethinking Science; Technology and Social Change' (Stanford University Press; 2007). His current research interests include digital media and right-wing populism; and the social Implications of big data.; Schroeder will be working on two areas.; 1 - Challenges for computational science of new sources of data in relation to statistical techniques; The vast bulk of large-scale data in the social sciences consists of digital media; including Twitter; Facebook; web browsing data; Wikipedia; smartphone location data and the like. Questions about access to these data; their representativeness and replicability and related issues have been much discussed. What has not been discussed to date are the ways in which these digital media data affect certain statistical techniques employed. In certain respects; these new sources affect statistical validity in ways that are different from traditional and commonly used sources of data; such as undertaking multiple regression on large-scale survey responses or national censuses or purposefully designed experiments.; 2 - Lessons from the application of personal data identifier schemes to the populations of world's two largest countries; India and China. This project compares two efforts that are arguably the single biggest current applications of capturing and deploying the analysis of personal data anywhere; Aadhaar in India and the social credit system in China. These two efforts are comparable because the aim of both is to create national databases which can be used to promote social development. There are also major differences; apart from the obvious difference between an authoritarian regime and an imperfect democracy. The implementations of both systems will have profound implications for other countries.; ,Schroeder will be working on two areas.; 1 - Challenges for computational science of new sources of data in relation to statistical techniques; The vast bulk of large-scale data in the social sciences consists of digital media; including Twitter; Facebook; web browsing data; Wikipedia; smartphone location data and the like. Questions about access to these data; their representativeness and replicability and related issues have been much discussed. What has not been discussed to date are the ways in which these digital media data affect certain statistical techniques employed. In certain respects; these new sources affect statistical validity in ways that are different from traditional and commonly used sources of data; such as undertaking multiple regression on large-scale survey responses or national censuses or purposefully designed experiments.; 2 - Lessons from the application of personal data identifier schemes to the populations of world's two largest countries; India and China. This project compares two efforts that are arguably the single biggest current applications of capturing and deploying the analysis of personal data anywhere; Aadhaar in India and the social credit system in China. These two efforts are comparable because the aim of both is to create national databases which can be used to promote social development. There are also major differences; apart from the obvious difference between an authoritarian regime and an imperfect democracy. The implementations of both systems will have profound implications for other countries.; ,['N/A'],Professor Ralph Schroeder 
ramji-venkataramanan,Communications; Convex programming; Nonlinear programming; Stochastic optimisation; High dimensional inference; Estimation theory; ,Ramji Venkataramanan is a University Lecturer in Information Engineering at the University of Cambridge. He is also a Fellow and Director of Studies at Trinity Hall. He received his PhD in electrical engineering (systems) from the University of Michigan; Ann Arbor in 2008; and his undergraduate degree from the Indian Institute of Technology; Madras in 2002. Before joining Cambridge University in 2013; he held post-doctoral positions at Stanford University and Yale University.; Ramji's research interests are broadly in the areas of information theory; statistical learning; and inference. A recent focus of his work has been on designing efficient message passing algorithms for high-dimensional statistical problems such as matrix factorisation; compressed sensing and phase retrieval. He hopes to collaborate with Turing researchers in areas such as computational biology and social science to explore new applications of these algorithms. He is also interested in understanding the fundamental limits of machine learning algorithms. For example; questions such as: How fast can the accuracy of any algorithm improve as the size of the training data grows?; ,Ramji's research interests are broadly in the areas of information theory; statistical learning; and inference. A recent focus of his work has been on designing efficient message passing algorithms for high-dimensional statistical problems such as matrix factorisation; compressed sensing and phase retrieval. He hopes to collaborate with Turing researchers in areas such as computational biology and social science to explore new applications of these algorithms. He is also interested in understanding the fundamental limits of machine learning algorithms. For example; questions such as: How fast can the accuracy of any algorithm improve as the size of the training data grows?; ,['N/A'], Ramji Venkataramanan 
raphael-hauser,['N/A'],Raphael Hauser studied Mathematics and Theoretical Physics at the EPFL and ETH in Lausanne and Zurich; Switzerland; followed by a PhD in Operations Research at Cornell University in Ithaca; USA. After a postdoc at Cambridge; Raphael joined the faculty at the University of Oxford; where he is currently an Associate Professor in Numerical Mathematics at the Oxford Mathematical Institute and the Tanaka Fellow in Applied Mathematics at Pembroke College. He is a member of the Numerical Analysis Group; the Mathematical and Computational Finance Group; and of the Oxford Emirates Data Lab.; Raphaelís research interests lie in optimisation algorithms; applied probability and statistics; distributed computing; and medical imaging. He is experienced in working on industrial collaborations involving real-world data and an inventor of several patents. At the Alan Turing Institute he will work on distributed algorithms for convex optimisation; extending an approach he developed for large scale principal component analysis.; ,Raphaelís research interests lie in optimisation algorithms; applied probability and statistics; distributed computing; and medical imaging. He is experienced in working on industrial collaborations involving real-world data and an inventor of several patents. At the Alan Turing Institute he will work on distributed algorithms for convex optimisation; extending an approach he developed for large scale principal component analysis.; ,['N/A'],Professor Raphael Hauser 
renaud-lambiotte,Dynamical systems & differential equations; Multi-agent systems; Multi-agent reasoning; Nonlinear dynamics; Pattern formation; Graph theory; Social psychology; ,Renaud Lambiotte has a PhD in physics from the Université libre de Bruxelles. After postdocs at ENS Lyon; Université de Liège; UCLouvain and Imperial College London; and a professorship in Mathematics at the University of Namur; he is currently associate professor at the Mathematical Institute of Oxford University. His main research interests are the modelling and analysis of processes taking place on large networks; with a particular focus on social and brain networks.; Algorithms and models of large-scale networks; ,Algorithms and models of large-scale networks; ,['N/A'],Professor Renaud Lambiotte 
ricardo-silva,Algorithms; Machine learning; Uncertainty quantification; Causality; Monte Carlo methods; ,Ricardo Silva got his PhD at Carnegie Mellon in 2005; in the newly formed Machine Learning Department. He moved to UCL as a Senior Research Fellow in the Gatsby Computational Neuroscience Unit. After a year at the Statistical Laboratory at Cambridge as a postdoc; Ricardo returned to UCL to join the Department of Statistical Science as a Lecturer in 2008.; His research focuses on: 1. Algorithms for probabilistic inference: approximations for likelihoods and posterior distributions based mostly of variational approximations and Markov chain Monte Carlo. 2. Latent variable models:  measurement error models and generalisations of probabilistic principal component analysis; as well as the modelling of network data. 3. Machine learning for causal inference: identification and discovery of models with unmeasured confounding and measurement error. He has also recently focused on applied work on human movement modelling; including partners such as TfL (for traffic data); UCL Institute of Behavioural Neuroscience (human navigation strategies) and Stratagem Ltd (sports modelling).; ,His research focuses on: 1. Algorithms for probabilistic inference: approximations for likelihoods and posterior distributions based mostly of variational approximations and Markov chain Monte Carlo. 2. Latent variable models:  measurement error models and generalisations of probabilistic principal component analysis; as well as the modelling of network data. 3. Machine learning for causal inference: identification and discovery of models with unmeasured confounding and measurement error. He has also recently focused on applied work on human movement modelling; including partners such as TfL (for traffic data); UCL Institute of Behavioural Neuroscience (human navigation strategies) and Stratagem Ltd (sports modelling).; ,['N/A'],Dr Ricardo Silva 
richard-dawson,['N/A'],Richard Dawson is Professor of Earth Systems Engineering in the School of Engineering at Newcastle University. He gained his PhD from the University of Bristol. His research focuses on the analysis and management of environmental risks to infrastructure networks; catchments and urban areas.; Richard has managed research funding worth £14m and currently leads Newcastle University's involvement in the major UK Collaboratorium for Research in Infrastructure and Cities (UKCRIC) programme. This includes co-leading the Newcastle Urban Observatory (www.urbanobservatory.ac.uk) and new engineering laboratories.  ; Richard is interested in application of data analytics to advance civil engineering and urban design. This includes: ; ,Richard is interested in application of data analytics to advance civil engineering and urban design. This includes: ; ,['N/A'],Professor Richard Dawson 
richard-everson,['N/A'],Richard Everson graduated with a degree in Physics from Cambridge University and a PhD in Applied Mathematics from Leeds University in 1988. He worked at Brown and Yale Universities on fluid mechanics and data analysis problems until moving to Rockefeller University; New York to work on optical imaging and modelling of the visual cortex. After working at Imperial College; London; he was appointed lecturer at Exeter University in 1999; where he is now a Professor of Machine Learning and Director of the Institute for Data Science and AI.; His current research interests are in machine learning and optimisation; and the interplay between them.  Current research projects include: surrogate modelling and Bayesian optimisation for optimisation of expensive-to-evaluate functions; such as computational fluid dynamics design problems; use of novel data; such as social media; in predicting damage from high winds; tracking-by-parts in video; automatic analysis of video for sewer inspection; linking of accelerometer data to physical activity; and clustering to understand dementia diagnoses.  He works closely with industrial partners on topics ranging from air traffic control to detecting lameness in cows.; ,His current research interests are in machine learning and optimisation; and the interplay between them.  Current research projects include: surrogate modelling and Bayesian optimisation for optimisation of expensive-to-evaluate functions; such as computational fluid dynamics design problems; use of novel data; such as social media; in predicting damage from high winds; tracking-by-parts in video; automatic analysis of video for sewer inspection; linking of accelerometer data to physical activity; and clustering to understand dementia diagnoses.  He works closely with industrial partners on topics ranging from air traffic control to detecting lameness in cows.; ,['N/A'],Professor Richard Everson 
richard-mann,Dynamical systems & differential equations; Multi-agent systems; Multi-agent reasoning; Game theory; Cognitive science; Social psychology; Uncertainty quantification; Simulation; ,Richard Mann is a University Academic Fellow in the School of Mathematics; University of Leeds. Previously he held postdoctoral positions at Uppsala University; The Institute for Futures Studies in Stockholm and ETH Zurich. He completed his DPhil at the University of Oxford in 2010.; Dr Mann's research is focused on collective behaviour and cognitive reasoning. He is interested in how individuals learn about the world by observing the actions and decisions of others; and how this in turn affects their own behaviour and ultimately the behaviour of whole groups. His goal is to understanding the basis for individual choices and their collective consequences; and through this knowledge to discover how collective behaviour can be predicted; controlled and optimised.; ,Dr Mann's research is focused on collective behaviour and cognitive reasoning. He is interested in how individuals learn about the world by observing the actions and decisions of others; and how this in turn affects their own behaviour and ultimately the behaviour of whole groups. His goal is to understanding the basis for individual choices and their collective consequences; and through this knowledge to discover how collective behaviour can be predicted; controlled and optimised.; ,['N/A'], Richard Mann 
richard-samworth,['N/A'],Richard Samworth is Professor of Statistics in the Statistical Laboratory at the University of Cambridge and a Fellow of St John's College.  He received his PhD; also from the University of Cambridge; in 2004; and currently holds an EPSRC Early Career Fellowship.;  ; His main research interests are in developing methodology and theory for high-dimensional and nonparametric statistical inference.  He is currently particularly interested in techniques for handling statistical challenges in Big Data that rely on perturbations of the data and aggregation.  Examples include random projection ensembles for high-dimensional classification and subsampling for variable selection. Other interests include shape-constrained and other nonparametric function estimation problems; Independent Component Analysis; and the bootstrap and its variants (e.g. bagging).; ,His main research interests are in developing methodology and theory for high-dimensional and nonparametric statistical inference.  He is currently particularly interested in techniques for handling statistical challenges in Big Data that rely on perturbations of the data and aggregation.  Examples include random projection ensembles for high-dimensional classification and subsampling for variable selection. Other interests include shape-constrained and other nonparametric function estimation problems; Independent Component Analysis; and the bootstrap and its variants (e.g. bagging).; ,['N/A'],Professor Richard Samworth 
rob-procter,['N/A'],Rob Procter is Professor of Social Informatics and deputy head (research) in the department of Computer Science; Warwick University. Previously he has held positions at Manchester and Edinburgh universities. His research interests are strongly inter-disciplinary; and include social media analytics and social data science.  ;  ; Rob’s current data science-related research includes the use of machine learning to: predict the veracity of content posted in social media and; analyse factors that influence the propagation of inflammatory postings. He is also interested in a number of other areas related to data science. These include methodological and ethical issues in the use of new forms of data in social research. More generally; he is interested: in applications of big data analytics in domains such as smart cities and healthcare; investigating social and technical issues that may influence the adoption of data science across academic; commercial and public sectors.; ,Rob’s current data science-related research includes the use of machine learning to: predict the veracity of content posted in social media and; analyse factors that influence the propagation of inflammatory postings. He is also interested in a number of other areas related to data science. These include methodological and ethical issues in the use of new forms of data in social research. More generally; he is interested: in applications of big data analytics in domains such as smart cities and healthcare; investigating social and technical issues that may influence the adoption of data science across academic; commercial and public sectors.; ,['N/A'],Professor Rob Procter 
robert-foley,Information retrieval; Evolution & adaptation; Applications (Machine learning); ,Robert Foley is Leverhulme Professor of Human Evolution at the University of Cambridge; and a Fellow of King's College. He obtained his PhD from Cambridge; and was subsequently a lecturer in biological anthropology at the University of Durham. Since 1986 he has been at Cambridge; and in 2001 co-founded the Leverhulme Centre for Human Evolutionary Studies; an inter-disciplinary research centre focused on developing multi-disciplinary and integrated approaches to human evolution. He was elected a Fellow of the British Academy in 2007.; Robert Foley's research has focused on the evolution and ecology of humans; especially their behaviour and adaptations. Much of this work has concentrated on understanding humans in terms of general Darwinian patterns and processes; and relating human evolution to more general models of evolution. Among his contributions are the development of off-site archaeology; community ecology and co-evolutionary approaches to hominin evolution; ecological models for human evolution; phylogenetic methods for analysing technological; cultural; social and linguistic evolution; the multiple dispersal model of human origins; and multi-disciplinary approaches to the evolution of human diversity.; His research combines traditional palaeoanthropological investigations with emerging approaches such as ancient DNA; agent-based modelling and human behavioural and cognitive experiments. This research has included early African hominins; the evolution of modern humans; and more recent prehistory and anthropology. Current research is focused on developing comparative and quantitative approaches to human evolution; including genotype-phenotype relationships. He has carried out field projects in Africa and Melanesia. He is currently involved in major field projects in northern and central Kenya. His books include Off-Site Archaeology; Another Unique Species: Patterns in Human Evolutionary Ecology; Humans before Humanity; and Principles of Human Evolution.; ,Robert Foley's research has focused on the evolution and ecology of humans; especially their behaviour and adaptations. Much of this work has concentrated on understanding humans in terms of general Darwinian patterns and processes; and relating human evolution to more general models of evolution. Among his contributions are the development of off-site archaeology; community ecology and co-evolutionary approaches to hominin evolution; ecological models for human evolution; phylogenetic methods for analysing technological; cultural; social and linguistic evolution; the multiple dispersal model of human origins; and multi-disciplinary approaches to the evolution of human diversity.; His research combines traditional palaeoanthropological investigations with emerging approaches such as ancient DNA; agent-based modelling and human behavioural and cognitive experiments. This research has included early African hominins; the evolution of modern humans; and more recent prehistory and anthropology. Current research is focused on developing comparative and quantitative approaches to human evolution; including genotype-phenotype relationships. He has carried out field projects in Africa and Melanesia. He is currently involved in major field projects in northern and central Kenya. His books include Off-Site Archaeology; Another Unique Species: Patterns in Human Evolutionary Ecology; Humans before Humanity; and Principles of Human Evolution.; ,['N/A'],Professor Robert Foley FBA
robert-mackay,['N/A'],Robert MacKay FRS CPhys FInstP CMath FIMA is a Professor in the Mathematics Institute of the University of Warwick and Director of Mathematical Interdisciplinary Research at Warwick. He was Director of Warwick’s Centre for Complexity Science from 2007-15 and President of the (UK) Institute of Mathematics and its Applications for 2012-13. He has made many contributions to the theory and applications of Nonlinear Dynamics. He has extensive research leadership and management experience.;  ; Robert proposes to develop (i) Gaussian processes to detect underdamped oscillations; with particular reference to AC electricity networks; (ii) models of financial systems to address their stability and regulation; (iii) design of assessor-assessee graphs for panel assessments to optimise the use of expertise and calibration.; ,Robert proposes to develop (i) Gaussian processes to detect underdamped oscillations; with particular reference to AC electricity networks; (ii) models of financial systems to address their stability and regulation; (iii) design of assessor-assessee graphs for panel assessments to optimise the use of expertise and calibration.; ,['N/A'],Professor Robert MacKay 
robert-piechocki,Compression (Algorithms); Information theory (Applied mathematics); Multi-agent systems; Robotics; Control theory; Neural networks; Communications; Applications (Machine learning); Deep learning; Reinforcement learning; Semi-supervised learning; Supervised learning; Unsupervised learning; Cryptography (Privacy & trust); Uncertainty quantification; Causality; Simulation; Information theory (Statistical methods & theory); ,Robert J. Piechocki is a Professor of Wireless Systems in the School of Computer Science; Electrical and Electronic Engineering and Engineering Maths. Robert received his PhD in Electrical and Electronic Engineering from Bristol in 2002.; Robert’s research interests span the areas of Connected Intelligent Systems; Wireless Networks; Information and Communication Theory; Statistics and AI. In his research work; he strives to develop solutions for decision making and inference in networked systems which communicate over resource constrained and unreliable links. Example of such networks are fleets of Connected and Automated Vehicles (CAV); which demand ultra-reliable and low-latency connectivity; despite very rapid changes in network topology and individual wireless links being subject to severe interference. Additional interests include contextual sensing in energy constraint IoT systems; such as battery powered IoT systems in residential healthcare applications.; ,Robert’s research interests span the areas of Connected Intelligent Systems; Wireless Networks; Information and Communication Theory; Statistics and AI. In his research work; he strives to develop solutions for decision making and inference in networked systems which communicate over resource constrained and unreliable links. Example of such networks are fleets of Connected and Automated Vehicles (CAV); which demand ultra-reliable and low-latency connectivity; despite very rapid changes in network topology and individual wireless links being subject to severe interference. Additional interests include contextual sensing in energy constraint IoT systems; such as battery powered IoT systems in residential healthcare applications.; ,['N/A'],Professor Robert Piechocki 
ross-king,['N/A'],Ross D. King obtained a B.Sc. Hons. Microbiology from the University of Aberdeen; and a Ph.D. in Computer Science from the Turing Institute. He is currently Professor of Machine Intelligence at the University of Manchester. His main research interests are in the interface between computer science and biology/chemistry. The research achievement he is most proud of is originating the idea of a “Robot Scientist”: using laboratory robotics to physically implement a closed-loop scientific discovery system.; His Robot Scientist “Adam” was the first machine to hypothesise and experimentally confirm scientific knowledge. His new robot “Eve” is searching for drugs against neglected tropical diseases. His work on this subject has been published in the top scientific journals; Science and Nature; and has received wide publicity. He is also very interested in building nondeterministic universal Turing machines using DNA; computational aesthetics; and computational economics. He spends his weekends walking along the Welsh coast.; ,['N/A'],['N/A'],Professor Ross King 
roy-ruddle,Data structures; Human computer interface; Cognitive science; Ethics; ,Roy Ruddle is a Professor of Computing at the University of Leeds; and Deputy Director (Research Technology) of the Leeds Institute for Data Analytics (LIDA). He has worked in both academia and industry; and researches visualization; visual analytics and human-computer interaction in spaces that range from high-dimensional data to virtual reality. In a 12-year collaboration with pathologists at the Leeds Teaching Hospitals NHS Trust (LTHT); he developed the Leeds Virtual Microscope (LVM) for visualizing tera-pixel image collections on Powerwall and ultra-high definition displays; leading to its use for pathology training in NHS hospitals and commercialisation by Roche.; The LVM has already won awards for its research (ACM ToCHI Best Paper 2016) and application (Yorkshire & Humber NHS Innovation Award for Medical Devices and Diagnostics; 2014). His industry-sponsored petrophysics research led to the PETMiner software; which integrates objective and subjective data in visualizations for interactive data analysis; and has been commercialised by the spin-out Petriva Ltd. He currently researches novel visualization methods for data profiling; investigating data quality; and analysing complex coded data such as electronic health records and retail transactions. That research involves collaborations with NHS Digital; Leeds City Council; LTHT; Bradford Institute for Health Research; Sainsbury's; and other organisations.; Professor Roy Ruddle designs; develops and evaluates visual analytic methods that address two neglected but challenging topics: (a) data quality; and (b) broken workflows. Both are essential if data analysis pipelines and models are to be rigorously designed; and hence are fundamental to data science. Visual analytics places users in the driving seat during the analysis of complex data; by combining the unique power of humans for detecting and reasoning about patterns (via interactive visualisation tools) and of machines for handling scale (via sophisticated models and powerful on-the-fly computation).; Data quality spans everything from data that is missing ('completeness') to data that is invalid; inconsistent; incompatible; implausible or clearly an error (these are all types of 'correctness'). Broken workflow occurs because it is rare that users rigorously investigate the knock-on consequences of choices made in one analysis step on the output of the next.; ,Professor Roy Ruddle designs; develops and evaluates visual analytic methods that address two neglected but challenging topics: (a) data quality; and (b) broken workflows. Both are essential if data analysis pipelines and models are to be rigorously designed; and hence are fundamental to data science. Visual analytics places users in the driving seat during the analysis of complex data; by combining the unique power of humans for detecting and reasoning about patterns (via interactive visualisation tools) and of machines for handling scale (via sophisticated models and powerful on-the-fly computation).; Data quality spans everything from data that is missing ('completeness') to data that is invalid; inconsistent; incompatible; implausible or clearly an error (these are all types of 'correctness'). Broken workflow occurs because it is rare that users rigorously investigate the knock-on consequences of choices made in one analysis step on the output of the next.; ,['N/A'],Professor Roy Ruddle 
ruben-sanchez-garcia,Geometry & topology; Applied mathematics; Graph theory; ,Ruben Sanchez Garcia is Associate Professor in Pure and Applied Mathematics at the University of Southampton. He did an undergraduate degree in Mathematics at the Universidad de Málaga (Spain); has a PhD from the University of Southampton; and held postdoctoral positions at the University of Sheffield  and Düsseldorf (Germany).; He has a pure mathematics background in Algebraic Topology; although he is working in the interface between pure and applied mathematics; particularly network science and applied topology. His interdisciplinary work includes collaborations with other mathematicians; physicists; power engineers; medics; biologists; and academics in business and risk management.; He is founding member of TopMD; which uses topological methods to identify pathological molecular pathways from gene expression data.; In his Turing Pilot Project; Dr Sanchez Garcia combines recent advances in data analytics and network modelling to identify combinations of genes that oscillate in a coordinated way (gene oscillators) from noisy experimental data. Genetic oscillators have important roles in regulating the timing of cellular dynamics and detecting them is essential to correctly identify cell types and transitions between cell states.; ,In his Turing Pilot Project; Dr Sanchez Garcia combines recent advances in data analytics and network modelling to identify combinations of genes that oscillate in a coordinated way (gene oscillators) from noisy experimental data. Genetic oscillators have important roles in regulating the timing of cellular dynamics and detecting them is essential to correctly identify cell types and transitions between cell states.; ,['N/A'],Dr Ruben Sanchez Garcia 
ruth-ahnert,Research methods; Social networks; ,Ruth Ahnert is Principal Investigator on the flagship Turing project 'Living With Machines'; and a Professor of Literary History and Digital Humanities at Queen Mary University of London. She gained her PhD from the Department of English at the University of Cambridge; but more recently her work has focused on the intersection between literary history and data science. She has held fellowships and grants funded by the AHRC; Stanford Humanities Center; the Folger Shakespeare Library; and the National Endowment of the Humanities (US).; In addition to her Turing-based work she is also Co-I on the AHRC-funded 'Networking the Archives: Assembling and Analysing Early Modern Correspondence'. With Elaine Treharne she is editor of the Stanford University Press book series Stanford Text Technologies.; Ruth's research at the Turing is dedicated to the 'Living with Machines' project. This project is a bold proposal for a new research paradigm. In this ground-breaking partnership between the Turing; the British Library; and partner universities (the University of Cambridge; the University of East Anglia; the University of Exeter; and Queen Mary University of London); historians; data scientists; geographers; computational linguists; and curators have been brought together to examine the human impact of industrial revolution.It is widely recognised that Britain was the birthplace of the world's first industrial revolution; yet there is still much to learn about the human; social; and cultural consequences of this historical moment. Focusing on the long nineteenth century (c.1780-1918); the Living with Machines project aims to harness the combined power of massive digitised archives and computational analytical tools to examine the ways in which technology altered the very fabric of human existence on a hitherto unprecedented scale.; The central theme - the mechanisation of work practices - speaks directly to present debates about how society can accommodate the revolutionary consequences of AI and robotics in what has become known as the fourth industrial revolution. To understand the fraught co-existence of human and machine; this project contends that we need research methods that combine technological innovation and human expertise.; ,Ruth's research at the Turing is dedicated to the 'Living with Machines' project. This project is a bold proposal for a new research paradigm. In this ground-breaking partnership between the Turing; the British Library; and partner universities (the University of Cambridge; the University of East Anglia; the University of Exeter; and Queen Mary University of London); historians; data scientists; geographers; computational linguists; and curators have been brought together to examine the human impact of industrial revolution.It is widely recognised that Britain was the birthplace of the world's first industrial revolution; yet there is still much to learn about the human; social; and cultural consequences of this historical moment. Focusing on the long nineteenth century (c.1780-1918); the Living with Machines project aims to harness the combined power of massive digitised archives and computational analytical tools to examine the ways in which technology altered the very fabric of human existence on a hitherto unprecedented scale.; The central theme - the mechanisation of work practices - speaks directly to present debates about how society can accommodate the revolutionary consequences of AI and robotics in what has become known as the fourth industrial revolution. To understand the fraught co-existence of human and machine; this project contends that we need research methods that combine technological innovation and human expertise.; ,['N/A'],Professor Ruth Ahnert 
ruth-king,Statistical methods & theory; Uncertainty quantification; Monte Carlo methods; Time series; Modelling (Statistical methods & theory); ,Ruth King is the Thomas Bayes' Chair of Statistics at the University of Edinburgh. She was awarded her PhD in 2001 from the University of Bristol; supervised by Steve Brooks. She then held positions at the Universities of Cambridge (PDRA; 2001-3) and St Andrews (EPSRC post-doctoral fellow in Mathematics 2003-5; lecturer 2003-10; reader 2010-15) before taking up her current appointment at the University of Edinburgh in 2015. Ruth was elected a Fellow of the Learned Society of Wales in 2017; and to the Royal Society of Edinburgh in 2018. ; Ruth’s research lies in the development of statistical methodology and their applications; primarily within the fields of ecology and epidemiology. A significant part of her research has involved dealing with intractable likelihoods; and the associated tools to fitting such models using a variety of different approaches; including Bayesian approaches; approximate likelihoods; imputation approaches and numerical integration. State-space models and hidden (semi-)Markov models have proven to be a very powerful tool in modelling a vast range of different systems; separating the true underlying system from the associated observation process are of particular interest; including the associated model-fitting tools. Further interest lies in integrating different forms of data within a single robust analysis; dealing with missing data and incorporating different forms of heterogeneity within a variety of application areas. ; ,Ruth’s research lies in the development of statistical methodology and their applications; primarily within the fields of ecology and epidemiology. A significant part of her research has involved dealing with intractable likelihoods; and the associated tools to fitting such models using a variety of different approaches; including Bayesian approaches; approximate likelihoods; imputation approaches and numerical integration. State-space models and hidden (semi-)Markov models have proven to be a very powerful tool in modelling a vast range of different systems; separating the true underlying system from the associated observation process are of particular interest; including the associated model-fitting tools. Further interest lies in integrating different forms of data within a single robust analysis; dealing with missing data and incorporating different forms of heterogeneity within a variety of application areas. ; ,['N/A'],Professor Ruth King FLSW; FRSE
sabina-leonelli,Robotics; Multi-agent reasoning; Communications; Databases; Human computer interface; Information retrieval; Data science of government & politics; Ethics; Research methods; ,Sabina Leonelli is a professor in philosophy and history of science at the University of Exeter; where she co-directs the Centre for the Study of the Life Sciences (Egenis). She gained her PhD at the Vrije Universiteit Amsterdam; following an MSc in history and philosophy of science at the London School of Economics and a BSc (hons) in history; philosophy and social studies of science at University College London.; Her research focuses on the methods and assumptions involved in the use of big data for discovery; the challenges involved in the extraction of knowledge from digital infrastructures; and the implications of choices in data curation for the outputs and uses of science and technology; the role of the open science movement within current landscapes of knowledge production; including concerns around inequality; and the status and history of experimental organisms as scientific models and data sources. She published widely in a variety of disciplines including philosophy; history; social studies of science; data science and biology; and is active in science policy; particularly as adviser on Open Science implementation for the European Commission and the steering boards of various research data infrastructures.; A key task for data science is to develop classification systems through which diverse types of data can be aligned to provide common ground for data mining and discovery. Developing such systems constitutes a challenge in the biological; biomedical and environmental sciences; where the methods and vocabulary used to classify data are often unique to the interests of those involved in data generation. ; On the one hand; linking disparate data together requires agreeing on common standards and keywords without losing information that may prove crucial to data interpretation. On the other hand; the choice of data classification systems requires a high degree of domain-specific knowledge and familiarity with the object and processes that data are aimed to document. These systems determine which claims – and about what – data are taken as evidence for; whose knowledge is legitimised or excluded by analytic tools; and whose perspective is incorporated within data-driven knowledge systems. ; What keywords should be used to bring data together or apart; how should these keywords be defined; who should make such decisions; and what are their implications for knowledge production and its impact on global society? This project addresses these questions through philosophical; historical and social scientific methods; aiming to provide a systematic analysis of (1) international initiatives aimed to facilitate data linkage; (2) the historical roots and motivations underpinning their classifications; and (3) the ways in which they map onto contemporary social challenges; such as the pursuit of the Sustainable Development Goals set out by the United Nations.; ,A key task for data science is to develop classification systems through which diverse types of data can be aligned to provide common ground for data mining and discovery. Developing such systems constitutes a challenge in the biological; biomedical and environmental sciences; where the methods and vocabulary used to classify data are often unique to the interests of those involved in data generation. ; On the one hand; linking disparate data together requires agreeing on common standards and keywords without losing information that may prove crucial to data interpretation. On the other hand; the choice of data classification systems requires a high degree of domain-specific knowledge and familiarity with the object and processes that data are aimed to document. These systems determine which claims – and about what – data are taken as evidence for; whose knowledge is legitimised or excluded by analytic tools; and whose perspective is incorporated within data-driven knowledge systems. ; What keywords should be used to bring data together or apart; how should these keywords be defined; who should make such decisions; and what are their implications for knowledge production and its impact on global society? This project addresses these questions through philosophical; historical and social scientific methods; aiming to provide a systematic analysis of (1) international initiatives aimed to facilitate data linkage; (2) the historical roots and motivations underpinning their classifications; and (3) the ways in which they map onto contemporary social challenges; such as the pursuit of the Sustainable Development Goals set out by the United Nations.; ,['N/A'],Professor Sabina Leonelli 
samuel-johnson,Dynamical systems & differential equations; Neural networks; Neuroscience; Nonlinear dynamics; Deep learning; Graph theory; Stochastic (Mathematical modelling); Data science of government & politics; Social networks; Combinatorics; ,Samuel Johnson received a PhD in physics from the University of Granada in 2011. He went on to work as a postdoc at the University of Oxford; a Marie Curie research fellow at Imperial College London; and a Zeeman lecturer at the University of Warwick; before taking up a lectureship in applied mathematics at the University of Birmingham in 2017.; Sam works on several topics related to complex systems; focusing particularly on the relationship between network structure and dynamics:; ,Sam works on several topics related to complex systems; focusing particularly on the relationship between network structure and dynamics:; ,['N/A'],Dr Samuel Johnson 
sandra-wachter,['N/A'],Dr. Sandra Wachter is a Researcher in Data Ethics and Algorithms at the Oxford Internet Institute; she is a member of the Ethics and Philosophy of Information research cluster and the Digital Ethics Lab. Sandra is also a Turing Research Fellow at the Alan Turing Institute in London. Her research focuses on the legal and ethical implications of Big Data; AI; and robotics as well as governmental surveillance; predictive policing; and human rights online.; Prior to joining the OII; Sandra worked the Royal Academy of Engineering on topics such as connectivity; AI; and autonomous systems. Sandra holds a Master’s and PhD in law specialising on European; International; and human rights law as well as technology and data protection law. In her PhD; she explored the concept of democracy according to the European Court of Human Rights and tested whether democracy is compatible with mass surveillance methods such as the European Data Retention Directive. Sandra also holds a Master’s of Science from the Oxford Internet Institute. Her thesis looked at tensions between freedom of speech and the right to privacy on social networks.; Her immediate research focuses on ethical design of algorithms; including the development of standards and methods to ensure fairness; accountability; transparency; interpretability; and group privacy in complex algorithmic systems. Sandra’s research also addresses legal and ethical aspects of robotics (e.g. surgical; domestic and social robots) and autonomous systems (e.g. autonomous and connected cars); including liability; accountability; and privacy issues as well as international policies and regulatory responses to the social and ethical consequences of automation. Research interests: Data Ethics; Big Data; AI; machine learning; algorithms; robotics; privacy; data protection and technology law; European; -International-; and human rights law; governmental algorithmic surveillance; and emotion detection; predictive policing .; ,Her immediate research focuses on ethical design of algorithms; including the development of standards and methods to ensure fairness; accountability; transparency; interpretability; and group privacy in complex algorithmic systems. Sandra’s research also addresses legal and ethical aspects of robotics (e.g. surgical; domestic and social robots) and autonomous systems (e.g. autonomous and connected cars); including liability; accountability; and privacy issues as well as international policies and regulatory responses to the social and ethical consequences of automation. Research interests: Data Ethics; Big Data; AI; machine learning; algorithms; robotics; privacy; data protection and technology law; European; -International-; and human rights law; governmental algorithmic surveillance; and emotion detection; predictive policing .; ,['N/A'],Professor Sandra Wachter 
sarah-filippi,['N/A'],Dr. Sarah Filippi joined the Department of Statistics of Oxford University in June 2014. She previously held a Medical Research Council Fellowship (2011-2104) in the Theoretical Systems Biology group at Imperial College London where she worked on a range of topics in computational statistics focused on understanding biological processes and their relation to disease. Prior to this she studied mathematics and stochastic processes at the University Denis Diderot in Paris (France) and completed her PhD in 2010 in reinforcement learning and parametric bandit models at LTCI; a joint lab of TELECOM ParisTech and CNRS.  ;  ; Dr. Filippi’s main research interests are related to the use of mathematical modelling; statistical machine learning and computational statistics to gain insight into biological processes and their role in diseases. She has a particular interest in addressing computational and statistical challenges in the analysis of genomic; transcriptomic and proteomic data at a single-cell level.; ,Dr. Filippi’s main research interests are related to the use of mathematical modelling; statistical machine learning and computational statistics to gain insight into biological processes and their role in diseases. She has a particular interest in addressing computational and statistical challenges in the analysis of genomic; transcriptomic and proteomic data at a single-cell level.; ,['N/A'],Dr Sarah Filippi 
sarah-meiklejohn,['N/A'],Sarah is a Reader in Cryptography and Security at University College London. She has worked on topics such as anonymity and criminal abuses in cryptocurrencies; privacy-enhancing technologies; and bringing transparency to shared systems.; Sarah has worked on empirical measurements of cryptocurrencies in order to quantify various security-related aspects; most notably; she has looked at anonymity and the extent to which real deployments achieve their stated anonymity guarantees.; Sarah is a Turing fellow through her co-supervision of Andrew Burnie; a doctoral student at the Turing.; ,Sarah has worked on empirical measurements of cryptocurrencies in order to quantify various security-related aspects; most notably; she has looked at anonymity and the extent to which real deployments achieve their stated anonymity guarantees.; Sarah is a Turing fellow through her co-supervision of Andrew Burnie; a doctoral student at the Turing.; ,['N/A'],Dr Sarah Meiklejohn 
sarah-morgan,Cognitive science; Time series; Neuroscience; Graph theory; Natural language processing; ,Sarah Morgan is a Research Fellow at Cambridge University. She completed her PhD at the Cambridge Physics department; in the Theory of Condensed Matter group (TCM). She then moved to the Brain Mapping Unit in the Cambridge Psychiatry department to begin her postdoctoral work. In 2017; Sarah won a Henslow Research Fellowship at Lucy Cavendish College; Cambridge. In 2019; she was awarded a Turing Fellowship to investigate whether speech can be used to detect psychotic disorders.; Sarah is a co-organiser of the Cambridge Networks Network; which brings together researchers from in and around Cambridge with an interest in complex networks (http://www.cnn.group.cam.ac.uk/). She is also passionate about making STEM more diverse; and co-founded the Cambridge group for women and non-binary people in physics (https://www.cavendishinspiringwomxn.co.uk/).; Sarah's research applies data science approaches- including network science; machine learning and bioinformatics- to better understand and predict mental health.; A main research focus is using MRI brain imaging to study schizophrenia and other mental health disorders. In particular; MRI brain images can be used to investigate brain connectivity; by calculating MRI brain networks where nodes represent large scale brain regions and edges represent connectivity between brain regions. MRI brain networks from patients with schizophrenia often show altered connectivity patterns compared to healthy volunteers. Sarah's research explores both whether we can use these connectivity patterns to predict individual patients' disease trajectories; and what they can teach us about the biological mechanisms underlying schizophrenia.; Sarah is also interested in using data science to investigate other aspects of mental health; for example using network science and natural language processing methods to study patients' speech. In schizophrenia in particular; patients often exhibit altered speech patterns; but to date there are no quantitative measures of altered speech available to clinicians. Recent work has also suggested that patients' speech might be able to predict their disease outcomes. Data science approaches therefore offer an exciting opportunity to provide quantitative speech markers which could revolutionise healthcare of this debilitating disorder.; ,Sarah's research applies data science approaches- including network science; machine learning and bioinformatics- to better understand and predict mental health.; A main research focus is using MRI brain imaging to study schizophrenia and other mental health disorders. In particular; MRI brain images can be used to investigate brain connectivity; by calculating MRI brain networks where nodes represent large scale brain regions and edges represent connectivity between brain regions. MRI brain networks from patients with schizophrenia often show altered connectivity patterns compared to healthy volunteers. Sarah's research explores both whether we can use these connectivity patterns to predict individual patients' disease trajectories; and what they can teach us about the biological mechanisms underlying schizophrenia.; Sarah is also interested in using data science to investigate other aspects of mental health; for example using network science and natural language processing methods to study patients' speech. In schizophrenia in particular; patients often exhibit altered speech patterns; but to date there are no quantitative measures of altered speech available to clinicians. Recent work has also suggested that patients' speech might be able to predict their disease outcomes. Data science approaches therefore offer an exciting opportunity to provide quantitative speech markers which could revolutionise healthcare of this debilitating disorder.; ,['N/A'],Dr Sarah Morgan 
sarvapali-ramchurn,Multi-agent systems; Artificial intelligence; Human computer interface; Optimisation; Privacy & trust; ,Prof. Sarvapali Ramchurn is a Professor of Artificial Intelligence in the Agents; Interaction; and Complexity research group where he carries out research into the design of autonomous agents and multi-agents for real-world socio-technical applications including energy systems; disaster management; and crowdsourcing. He has won multiple best paper awards for his work and is a recipient of the prestigious AXA Research Fund Award for Responsible Artificial Intelligence. He works closely with industry and his research touches on a number of fields including Machine Learning; Data Science; and Game Theory.  Specifically; he has pioneered the development of agent-based coordination algorithms for distributed task allocation that have been deployed on real-world unmanned aerial vehicles and in the Premier League’s Fantasy Football game where his approach has been shown to outperform more than 5M human players. His papers have been cited more than 6000 times (according to Google scholar) and his work has featured in various media including BBC News; New Scientist; Sky News; BBC Click; and Wired.;  ; To support the management of large numbers of robots; Artificial Intelligence (AI) algorithms have been developed to automate the actions of such robot swarms and enable to act in a cohesive and coordinated way. AI thus allows swarms to allocate tasks to each other; react to losses in communication or resources (e.g.; other members of the swarm) and therefore reduce the workload of their human counterparts. However; it has been shown that; in some situations; operators are overwhelmed with information coming from robots; may not completely trust their decisions; and therefore override them; by doing so; they may cause the system to fail. In other situations; humans may completely rely on the automation and fail to notice obvious errors in the system. Moreover; to manage such large fleets of robots in a safe manner; previous studies suggest there should be shifts in autonomy levels to allow humans to take corrective action to recover from failures or to optimize task performance. Understanding when such shifts should occur without losing out on the fault-tolerance benefits of a decentralized swarm; what levels of workload these shifts induce; and how the team of operators should enact such shifts are key questions that will be addressed in this project.; ,To support the management of large numbers of robots; Artificial Intelligence (AI) algorithms have been developed to automate the actions of such robot swarms and enable to act in a cohesive and coordinated way. AI thus allows swarms to allocate tasks to each other; react to losses in communication or resources (e.g.; other members of the swarm) and therefore reduce the workload of their human counterparts. However; it has been shown that; in some situations; operators are overwhelmed with information coming from robots; may not completely trust their decisions; and therefore override them; by doing so; they may cause the system to fail. In other situations; humans may completely rely on the automation and fail to notice obvious errors in the system. Moreover; to manage such large fleets of robots in a safe manner; previous studies suggest there should be shifts in autonomy levels to allow humans to take corrective action to recover from failures or to optimize task performance. Understanding when such shifts should occur without losing out on the fault-tolerance benefits of a decentralized swarm; what levels of workload these shifts induce; and how the team of operators should enact such shifts are key questions that will be addressed in this project.; ,,Professor Sarvapali Ramchurn 
saul-jacka,['N/A'],After a PhD from Cambridge Statistical Laboratory; Saul trained as an actuary and retains strong links with the profession. He has been a Professor of Statistics at Warwick since 2003. He won two Merrill Lynch Best Paper prizes in Mathematical Finance; is a member of Scientific Steering Committee of the Isaac Newton Institute and editor of the probability journal; Stochastics.; Saul specialises in probability; stochastic control and finance. He is currently working on stochastic control; optimal stopping and applications to finance; anomaly detection and numerics for non-linear pdes and multi-level Monte Carlo (MLMC) simulation. He is increasingly focusing on high-dimensional and analytically intractable control problems via the Policy Improvement Algorithm; approximations thereof and related numerical tools.; ,Saul specialises in probability; stochastic control and finance. He is currently working on stochastic control; optimal stopping and applications to finance; anomaly detection and numerics for non-linear pdes and multi-level Monte Carlo (MLMC) simulation. He is increasingly focusing on high-dimensional and analytically intractable control problems via the Policy Improvement Algorithm; approximations thereof and related numerical tools.; ,['N/A'],Professor Saul Jacka 
scott-hale,['N/A'],Dr Scott A. Hale is a Faculty Fellow with expertise in both the social sciences and computer science. His research focuses on knowledge discovery; data mining; and the visualization of human behaviour in three substantive areas: multilingualism and user experience; mobilization/collective action; and human mobility.; New technologies generate unprecedented quantities of digital trace data about human behaviour and provide opportunities to study complex social systems in frameworks similar to those of the natural sciences. His work concentrates on empirical observation of patterns in large-scale data and experiments. These approaches form part of a new field --- computational social science or social data science --- and can generate theory-informed predictive models and change the way we understand and solve social problems. The vast majority of the data about human behaviour; however; is unstructured; and my research therefore seeks to develop new tools to take full advantage of these data.; ,New technologies generate unprecedented quantities of digital trace data about human behaviour and provide opportunities to study complex social systems in frameworks similar to those of the natural sciences. His work concentrates on empirical observation of patterns in large-scale data and experiments. These approaches form part of a new field --- computational social science or social data science --- and can generate theory-informed predictive models and change the way we understand and solve social problems. The vast majority of the data about human behaviour; however; is unstructured; and my research therefore seeks to develop new tools to take full advantage of these data.; ,['N/A'],Dr Scott Hale 
sean-fox,Multi-agent systems; Multi-agent reasoning; Evolution & adaptation; Game theory; Visualisation (Computer systems & architectures); Applications (Machine learning); Deep learning; Pattern recognition; Semi-supervised learning; Supervised learning; Unsupervised learning; Data science of government & politics; Modelling (Statistical methods & theory); ,Sean Fox is a Senior Lecturer in Global Development at the University of Bristol. Prior to joining Bristol he was a Teaching Fellow in the Department of International Development at the London School of Economics; where he completed his PhD. He is co-author of the book Cities and Development (Routledge 2016) and is currently PI of the Quantifying Cities for Sustainable Development project; which is funded by the ESRC Transformative Research scheme. He has served as an advisor on urban development issues for organisations such as DfID; UN-HABITAT; UNECA; and the World Bank and is currently a Research Associate at the Overseas Development Institute.; Dr Fox's research for The Alan Turing Institute focuses on developing methods to identify urban areas using remotely sensed images and then linking this data to information on political and administrative institutions and processes. The goals is to develop a global data set of cities and urban regions defined by de facto boundaries rather than the formal ones reflected in administrative data. This will facilitate research into urban governance processes; particularly in data sparse environments.; ,Dr Fox's research for The Alan Turing Institute focuses on developing methods to identify urban areas using remotely sensed images and then linking this data to information on political and administrative institutions and processes. The goals is to develop a global data set of cities and urban regions defined by de facto boundaries rather than the formal ones reflected in administrative data. This will facilitate research into urban governance processes; particularly in data sparse environments.; ,['N/A'],Dr Sean Fox 
sebastian-axbard,Data science of government & politics; Research methods; Causality; ,Sebastian Axbard is Lecturer of Economics at Queen Mary; University of London. His research is empirically oriented and lies at the intersection of development economics and political economy. Sebastian received his PhD from Uppsala University in 2016.; Sebastian combines econometrics techniques with big data (e.g. satellite data with high temporal and spatial resolution) to answer questions of relevance for welfare in developing countries. His previous work has studied the determinants and consequences of illegal activities by individuals and state actors; such as the role of the judicial system in preventing corruption in the Philippines and the determinants of sea piracy in Indonesia. He is currently working on pollution monitoring in China and the determinants of responses to distress calls in the Mediterranean.; ,Sebastian combines econometrics techniques with big data (e.g. satellite data with high temporal and spatial resolution) to answer questions of relevance for welfare in developing countries. His previous work has studied the determinants and consequences of illegal activities by individuals and state actors; such as the role of the judicial system in preventing corruption in the Philippines and the determinants of sea piracy in Indonesia. He is currently working on pollution monitoring in China and the determinants of responses to distress calls in the Mediterranean.; ,['N/A'],Dr Sebastian Axbard 
ser-huang-poon,Data structures; Robotics; Game theory; Neural networks; Nonlinear dynamics; Pattern formation; Communications; Databases; Parallel computing; Human computer interface; Information retrieval; Computer vision; Deep learning; Natural language processing; Pattern recognition; Reinforcement learning; Speech recognition; Differential privacy; Identity management; Verification; Software framework development; Data science of government & politics; Ethics; Social media; Uncertainty quantification; Causality; ,Ser-Huang Poon; a professor at the Alliance Manchester Business School; specialises in FinTech and AI. She has published and working papers in the areas of corporate social responsibility; modern slavery; BigData; NLP analysis of corporate reporting and news; and on blockchain. She is experienced in leading large consortium having managed three EU FP consortia worth about €5 million in total.; The research involves building a blockchain that can provide the foundation to advance and innovative forced labour intelligence both nationally and internationally. Apart from the blockchain component; one significant area of interest lies in the ability to use artificial intelligence (AI) techniques to support the various parties from the victim support network who use the system. In particular the software developed should have the appropriate API's supporting information submissions and queries at different permission levels for analysis while also respecting the need to maintain the security and anonymity of the data in the system.; It might be possible to apply AI techniques in multiple areas; for instance the use of google maps to help fill in incomplete geographical information in reports of potential violations; perhaps ultimately spreading to using non-human image recognition to identify likely geographical locations from images captured on a mobile phone. The research will involve investigating the use of Google translate or other language detection and translation API to help managing foreign language telephone and text messages.; Other potential includes the use of AI and machine learning techniques for judging / ranking the anticipated severity and reliability of reported incidents using historical incidents and survey data together with patterns recognition from all current reported incidents; and the use of Smart Contracts to terminate data access according to GDPR requirements in order to meet the legal and ethical requirements within the UK or other international laws.; Dr Ser-Huang Poon became a professor at Manchester University in 2003. She has written three books and 63 papers; having published 34 of them; one of the published papers was cited on the Nobel website as a reference reading in volatility. Dr Poon received two best paper awards; one of which is a joint work with the late Nobel Laurette; Prof Clive Granger. She has co-authored a commissioned report for the HMT Foresight Program on the impact of high frequency trading. She has received nine grant awards for research and research training; most of which she was the principal investigator/applicant.; She has managed three Marie Curie ITN grants; two of which (€1million for FP6 and €3.7million for FP7) she served as host. She was appointed a visiting professor at National University of Singapore (NUS) and Victoria University of Wellington at New Zealand; and a distinguished visiting professor at University of Technology; Sydney. She has supervised nine research fellows and 15 PhD students; one of the PhD students received a best doctoral paper at MFS conference last year. Her current research interests evolve around blockchain; FinTech; AI and BigData.; ,The research involves building a blockchain that can provide the foundation to advance and innovative forced labour intelligence both nationally and internationally. Apart from the blockchain component; one significant area of interest lies in the ability to use artificial intelligence (AI) techniques to support the various parties from the victim support network who use the system. In particular the software developed should have the appropriate API's supporting information submissions and queries at different permission levels for analysis while also respecting the need to maintain the security and anonymity of the data in the system.; It might be possible to apply AI techniques in multiple areas; for instance the use of google maps to help fill in incomplete geographical information in reports of potential violations; perhaps ultimately spreading to using non-human image recognition to identify likely geographical locations from images captured on a mobile phone. The research will involve investigating the use of Google translate or other language detection and translation API to help managing foreign language telephone and text messages.; Other potential includes the use of AI and machine learning techniques for judging / ranking the anticipated severity and reliability of reported incidents using historical incidents and survey data together with patterns recognition from all current reported incidents; and the use of Smart Contracts to terminate data access according to GDPR requirements in order to meet the legal and ethical requirements within the UK or other international laws.; ,Dr Ser-Huang Poon became a professor at Manchester University in 2003. She has written three books and 63 papers; having published 34 of them; one of the published papers was cited on the Nobel website as a reference reading in volatility. Dr Poon received two best paper awards; one of which is a joint work with the late Nobel Laurette; Prof Clive Granger. She has co-authored a commissioned report for the HMT Foresight Program on the impact of high frequency trading. She has received nine grant awards for research and research training; most of which she was the principal investigator/applicant.; She has managed three Marie Curie ITN grants; two of which (€1million for FP6 and €3.7million for FP7) she served as host. She was appointed a visiting professor at National University of Singapore (NUS) and Victoria University of Wellington at New Zealand; and a distinguished visiting professor at University of Technology; Sydney. She has supervised nine research fellows and 15 PhD students; one of the PhD students received a best doctoral paper at MFS conference last year. Her current research interests evolve around blockchain; FinTech; AI and BigData.; ,Professor Ser-Huang Poon 
serge-guillas,Numerical (Algorithms); Numerical analysis; Visualisation (Computer systems & architectures); Deep learning; Supervised learning; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Information theory (Statistical methods & theory); ,"Serge Guillas obtained his PhD (Paris 6 Pierre-et-Marie-Curie; France) in 2001. Then he was Postdoctoral Research Associate (University of Chicago; USA) 2002-2004; Assistant Professor (Georgia Institute of Technology; USA) 2004-2008; lecturer (UCL) 2007-2009; Reader (UCL) 2009-2016; Professor since 2016. He has recently led a NERC-AHRC-ESRC GCRF project about tsunami risk for the Western coast of India. He also collaborates with the (re)-insurance industry and scientific experts on satellite data to improve modelling and early warnings.; Prof. Serge Guillas is leading the DCE project at the Turing called ""Uncertainty Quantification of complex computer models. Applications to tsunami and climate"". Co-investigators are Suhaib Fahmy (Warwick); Michael Giles (Oxford); Gihan R. Mudalige (Warwick); Daniel Williamson (Exeter).; ","Prof. Serge Guillas is leading the DCE project at the Turing called ""Uncertainty Quantification of complex computer models. Applications to tsunami and climate"". Co-investigators are Suhaib Fahmy (Warwick); Michael Giles (Oxford); Gihan R. Mudalige (Warwick); Daniel Williamson (Exeter).; ",['N/A'],Professor Serge Guillas 
shaogang-gong,Neural networks; Information retrieval; Computer vision; Deep learning; Pattern recognition; Reinforcement learning; Uncertainty quantification; ,Shaogang Gong is a Professor of Visual Computation at Queen Mary University of London and the Head of the Computer Vision Group at QMUL. He is a Fellow of IEE (now IET); a Fellow of the British Computer Society; and a Member of the UK Computing Research Committee. His research is in computer vision; machine learning and video analysis; with applications in visual surveillance and video forensic analysis. He has developed mathematical models; algorithms; patents; and commercial software for video behaviour recognition and multi-object multi-camera tracking; in particular person re-identification.; He is interested in variational graph models including Bayesian graphs; dynamic Bayesian networks; hidden Markov models; and probabilistic latent semantic analysis. His recent work includes domain transfer learning; unsupervised and semi-supervised deep learning; imbalanced data deep learning; reinforcement learning; zero-shot learning; and human-in-the-loop active learning.; The amount of video data of public urban space is growing exponentially from 24/7 operating urban camera infrastructures; online social media sources; self-driving cars; and smart city intelligent transportation systems; e.g. 1.4 trillion hours CCTV video in 2017; growing to 3.3 trillion hours by 2020. The scale and diversity of such video data make it very difficult to filter and extract useful critical information in a timely manner. There is a fundamental challenge to develop data analysis tools for large scale video semantic search by exploring the huge quantity of video data using deep learning; critical for smart city on public security; safety and intelligent transport.; To that end; robust and scalable algorithms are required for human recognition; of both individuals and populations; and searching individual people and/or categories of people and objects (semantic search) in large scale video data from diverse multiple sources city wide. This Turing research project aims to develop both scalable algorithms and software for semi-supervised and unsupervised deep learning for domain transfer object search; attribute-based latent semantic embedding space inference; and deep learning knowledge distillation.; ,The amount of video data of public urban space is growing exponentially from 24/7 operating urban camera infrastructures; online social media sources; self-driving cars; and smart city intelligent transportation systems; e.g. 1.4 trillion hours CCTV video in 2017; growing to 3.3 trillion hours by 2020. The scale and diversity of such video data make it very difficult to filter and extract useful critical information in a timely manner. There is a fundamental challenge to develop data analysis tools for large scale video semantic search by exploring the huge quantity of video data using deep learning; critical for smart city on public security; safety and intelligent transport.; To that end; robust and scalable algorithms are required for human recognition; of both individuals and populations; and searching individual people and/or categories of people and objects (semantic search) in large scale video data from diverse multiple sources city wide. This Turing research project aims to develop both scalable algorithms and software for semi-supervised and unsupervised deep learning for domain transfer object search; attribute-based latent semantic embedding space inference; and deep learning knowledge distillation.; ,['N/A'],Professor Shaogang Gong 
shimon-whiteson,['N/A'],Professor Shimon Whiteson studied English and Computer Science at Rice University before completing my doctorate in Computer Science at the University of Texas at Austin in 2007.  He then spent eight years as an Assistant and then an Associate Professor at the University of Amsterdam before joining Oxford as an Associate Professor in 2015.  He was awarded an ERC Starting Grant from the European Research Council in 2014.  ; Assoc. Prof. Shimon Whiteson’s research focuses on artificial intelligence. His goal is to design; analyse; and evaluate the algorithms that enable computational systems to acquire and execute intelligent behaviour. He’s particularly interested in machine learning; with which computers can learn from experience; and decision-theoretic planning; with which they can reason about their goals and deduce behavioural strategies that maximise their utility. In addition to theoretical work on these topics; he has in recent years also focused on applying them to practical problems in robotics and search engine optimisation.; ,['N/A'],['N/A'],Professor Shimon Whiteson 
siddharth-narayanaswamy,['N/A'],['N/A'],['N/A'],['N/A'],Dr Siddharth Narayanaswamy 
silvia-liverani,Uncertainty quantification; High dimensional inference; Time series; ,Dr Silvia Liverani is a Senior Lecturer in Statistics in the School of Mathematical Sciences. She gained her PhD in Statistics at the University of Warwick in 2009 and has held positions at the University of Bristol; Imperial College London and Brunel University London.; Her research focuses on Bayesian modelling and clustering methods. In particular she works on Dirichlet process mixture models and has also conducted research on spatio-temporal models and survival response models. She has worked in very diverse areas of application including genetics; epidemiology; road traffic accidents; football; environmental health and cardiac electrophysiology.; ,Her research focuses on Bayesian modelling and clustering methods. In particular she works on Dirichlet process mixture models and has also conducted research on spatio-temporal models and survival response models. She has worked in very diverse areas of application including genetics; epidemiology; road traffic accidents; football; environmental health and cardiac electrophysiology.; ,['N/A'],Dr Silvia Liverani 
simon-cotter,Dynamical systems & differential equations; Mathematical physics; Numerical analysis; Parallel computing; Uncertainty quantification; High dimensional inference; Monte Carlo methods; Simulation; Time series; Probability; ,Simon Cotter is a senior lecturer in applied mathematics in the School of Mathematics at the University of Manchester. He attained his PhD in infinite dimensional Bayesian inverse problems from the University of Warwick in 2010 under Andrew Stuart. He then worked with Radek Erban at Oxford University on stochastic models of chemical reactions; before taking up a lectureship at the University of Manchester as a member of the numerical analysis group. He was promoted to senior lecturer in 2016. Simon is interested in working on problems in the interface between applied maths; statistics; probability and often with biological applications. In particular he is interested in a range of computational methods for characterising probability distributions; including Markov chain Monte Carlo methods.; Underlying almost all aspects of data science and artificial intelligence; is a need for the characterisation of probability distributions arising from epistemic uncertainties. Robust and efficient numerical methods for the sampling of such distributions is a key component of many approaches to understanding data; incorporating it into mechanistic models; and informing decision makers. This can range from sampling from the distributions arising from Bayesian inverse problems in data analytics within engineering and science; through to the distributions which form part of the machinery that allows AI algorithms to work; such as within Gaussian process construction. Within his research; Simon has focused; amongst other things; on the design; analysis and implementation of efficient Markov chain Monte Carlo (MCMC) methodologies for data analytics.; This powerful and increasingly important family of methods allow us to sample from complex probability distributions. There are a range of challenges in this arena; including the development of methods which can tackle problems in high dimensions; both in terms of the state-space; and in terms of the size of data. There are also challenges surrounding the efficient sampling of distributions with complex structures; for example multimodality; or the concentration of the probability density on lower dimensional curved manifolds. Each of these scenarios requires careful consideration; with specific methods being optimal in each case.; ,Underlying almost all aspects of data science and artificial intelligence; is a need for the characterisation of probability distributions arising from epistemic uncertainties. Robust and efficient numerical methods for the sampling of such distributions is a key component of many approaches to understanding data; incorporating it into mechanistic models; and informing decision makers. This can range from sampling from the distributions arising from Bayesian inverse problems in data analytics within engineering and science; through to the distributions which form part of the machinery that allows AI algorithms to work; such as within Gaussian process construction. Within his research; Simon has focused; amongst other things; on the design; analysis and implementation of efficient Markov chain Monte Carlo (MCMC) methodologies for data analytics.; This powerful and increasingly important family of methods allow us to sample from complex probability distributions. There are a range of challenges in this arena; including the development of methods which can tackle problems in high dimensions; both in terms of the state-space; and in terms of the size of data. There are also challenges surrounding the efficient sampling of distributions with complex structures; for example multimodality; or the concentration of the probability density on lower dimensional curved manifolds. Each of these scenarios requires careful consideration; with specific methods being optimal in each case.; ,['N/A'],Dr Simon Cotter 
simon-lucas,Multi-agent systems; Operations research; Robotics; Multi-agent reasoning; Evolution & adaptation; Game theory; Neural networks; Neural & evolutionary computing; Visualisation (Computer systems & architectures); Applications (Machine learning); Deep learning; Reinforcement learning; Semi-supervised learning; Unsupervised learning; Stochastic (Mathematical modelling); Stochastic optimisation; Data science of government & politics; Monte Carlo methods; Simulation; ,Simon Lucas is a professor of Artificial Intelligence and Head of the School of Electronic Engineering and Computer Science at Queen Mary University of London where he also heads the Game AI Research Group. He holds a PhD degree (1991) in Electronics and Computer Science from the University of Southampton. He is the founding Editor-in-Chief of the IEEE Transactions on Games and co-founded the IEEE Conference on Games. His research involves developing and applying computational intelligence techniques to build better game AI; use AI to design better games; provide deep insights into the nature of intelligence and work towards Artificial General Intelligence. He is the QMUL lead for the EPSRC-funded CDT in Intelligent Games and Game Intelligence (IGGI).; For his Turing fellowship Simon will be exploring new ways to apply game AI to help make better real-world decisions. Games always pose the problem of: given the current state of the system; what should one do next?Recent developments in game AI have clearly demonstrated that machines can learn complex and effective decision making policies often to super-human levels of intelligence. However; to achieve has so far required that we have a model of the game or real-world problem; often referred to as a forward model. For complex real-world problems we often need to deal with flawed models and incomplete data.The research will explore better ways to learn models; and also how to cope better with model inaccuracies and incomplete data. This is seen as a pressing challenge in AI. Promising approaches involve deep reinforcement learning; statistical forward planning and evolutionary algorithms (especially model-based ones designed to be sample-efficient and cope with high levels of noise).; ,For his Turing fellowship Simon will be exploring new ways to apply game AI to help make better real-world decisions. Games always pose the problem of: given the current state of the system; what should one do next?Recent developments in game AI have clearly demonstrated that machines can learn complex and effective decision making policies often to super-human levels of intelligence. However; to achieve has so far required that we have a model of the game or real-world problem; often referred to as a forward model. For complex real-world problems we often need to deal with flawed models and incomplete data.The research will explore better ways to learn models; and also how to cope better with model inaccuracies and incomplete data. This is seen as a pressing challenge in AI. Promising approaches involve deep reinforcement learning; statistical forward planning and evolutionary algorithms (especially model-based ones designed to be sample-efficient and cope with high levels of noise).; ,['N/A'],Professor Simon Lucas 
simon-mcintosh-smith,Numerical (Algorithms); Dynamical systems & differential equations; Mathematical physics; Communications; Parallel computing; Neural & evolutionary computing; Real time computing; Applications (Machine learning); Deep learning; Hardware optimisation (FPGA/GPU); Probabilistic programming; Software framework development; Monte Carlo methods; Simulation; ,Simon McIntosh-Smith is a full Professor of High Performance Computing at the University of Bristol in the UK. He began his career in industry as a microprocessor architect; first at Inmos and STMicroelectronics in the early 1990s; before co-designing the world's first fully programmable GPU at Pixelfusion in 1999. In 2002 he co-founded ClearSpeed Technology where; as Director of Architecture and Applications; he co-developed the first modern many-core HPC accelerators.; He now leads the High Performance Computing Research Group at the University of Bristol; where his research focuses on advanced computer architectures and performance portability. He plays a key role in designing and procuring supercomputers at the local; regional and national level; including the UK’s national HPC service; Archer. In 2016 he led the successful bid by the GW4 Alliance along with the UK’s Met Office and Cray; to design and build ‘Isambard’; the world’s first production; ARMv8-based supercomputer.; A major new trend starting to emerge in computer architecture is the move towards heterogeneous computing. Described as a new “golden age in computer architecture” by Hennessy and Patterson in their ACM/IEEE Turing award lecture at ISCA 2018 [1]; the driving forces behind this shift come from a simultaneous slowdown in the rate of innovation in both CPU architectures and semiconductor design processes. Indeed; Dennard Scaling (the ability to exploit lower voltages to keep chip-level power consumption in check) is regarded as having halted several years ago [2]; while the industry-wide ITRS roadmap for semiconductors now predicts a slowdown in “Moore’s Law”; taking three years rather than the historical two to yield a doubling in transistor density per chip [3]. Innovation in computer architecture is regarded as the only way to meet this challenge.; Professor McIntosh-Smith's Turing-related research aims to create new heterogeneous computer architecture designs which deliver significant steps forward in performance for key applications within the Turing's remit. These new architectures; which can be co-designed alongside the relevant algorithms; will be simulated using a new rapid processor simulator development framework currently being designed within McIntosh-Smith’s research group in Bristol [4]. This framework enables the development of cycle-accurate CPU simulators able to run real codes generated by real compilers. This simulator framework already supports the Arm instruction set; including the new Scalable Vector Extensions (SVE). Support for additional instruction sets; such as x86 and RISC-V; will be added in the future. The simulator enables the rapid development of hypothetical computer architectures; and exploration of design spaces such as heterogeneous co-processors; memory hierarchies; mixed precision arithmetic; massive multi-threading; and memory centric system designs.During this Turing-related project; Professor McIntosh-Smith's group will use the simulator to investigate new computer architectures optimised specifically for applications in the data science and AI at scale theme; with the goal of significantly outperforming today’s CPU and GPU architectures. Any number of “data science at scale” algorithms could motivate the investigation; but one area already being actively explored by the McIntosh-Smith is ultra large-scale weather and climate simulations involving hundreds of PetaBytes of data or more; the Met Office and ECMWF are partners in this work. The project will also explore the notion of “self-designing architectures”; where machine learning techniques can be applied to automatically design superior next-generation computer architectures; some early explorations in this area have shown promising results.References[1] https://www.youtube.com/watch?v=3LVeEjsn8Ts[2] Mark Bohr. A 30 year retrospective on Dennard’s MOSFET scaling paper. Solid-State Circuits Newsletter; IEEE; 12(1):11 –13; winter 2007.[3] http://www.itrs.net/Links/2011ITRS/2011Chapters/2011ExecSum.pdf [4] http://uob-hpc.github.io/[5] http://gw4.ac.uk/isambard/; ,A major new trend starting to emerge in computer architecture is the move towards heterogeneous computing. Described as a new “golden age in computer architecture” by Hennessy and Patterson in their ACM/IEEE Turing award lecture at ISCA 2018 [1]; the driving forces behind this shift come from a simultaneous slowdown in the rate of innovation in both CPU architectures and semiconductor design processes. Indeed; Dennard Scaling (the ability to exploit lower voltages to keep chip-level power consumption in check) is regarded as having halted several years ago [2]; while the industry-wide ITRS roadmap for semiconductors now predicts a slowdown in “Moore’s Law”; taking three years rather than the historical two to yield a doubling in transistor density per chip [3]. Innovation in computer architecture is regarded as the only way to meet this challenge.; Professor McIntosh-Smith's Turing-related research aims to create new heterogeneous computer architecture designs which deliver significant steps forward in performance for key applications within the Turing's remit. These new architectures; which can be co-designed alongside the relevant algorithms; will be simulated using a new rapid processor simulator development framework currently being designed within McIntosh-Smith’s research group in Bristol [4]. This framework enables the development of cycle-accurate CPU simulators able to run real codes generated by real compilers. This simulator framework already supports the Arm instruction set; including the new Scalable Vector Extensions (SVE). Support for additional instruction sets; such as x86 and RISC-V; will be added in the future. The simulator enables the rapid development of hypothetical computer architectures; and exploration of design spaces such as heterogeneous co-processors; memory hierarchies; mixed precision arithmetic; massive multi-threading; and memory centric system designs.During this Turing-related project; Professor McIntosh-Smith's group will use the simulator to investigate new computer architectures optimised specifically for applications in the data science and AI at scale theme; with the goal of significantly outperforming today’s CPU and GPU architectures. Any number of “data science at scale” algorithms could motivate the investigation; but one area already being actively explored by the McIntosh-Smith is ultra large-scale weather and climate simulations involving hundreds of PetaBytes of data or more; the Met Office and ECMWF are partners in this work. The project will also explore the notion of “self-designing architectures”; where machine learning techniques can be applied to automatically design superior next-generation computer architectures; some early explorations in this area have shown promising results.References[1] https://www.youtube.com/watch?v=3LVeEjsn8Ts[2] Mark Bohr. A 30 year retrospective on Dennard’s MOSFET scaling paper. Solid-State Circuits Newsletter; IEEE; 12(1):11 –13; winter 2007.[3] http://www.itrs.net/Links/2011ITRS/2011Chapters/2011ExecSum.pdf [4] http://uob-hpc.github.io/[5] http://gw4.ac.uk/isambard/; ,['N/A'],Professor Simon McIntosh-Smith 
song-liu,Numerical (Algorithms); Information theory (Applied mathematics); Numerical analysis; Neural networks; Computing networks; Applications (Machine learning); Computer vision; Deep learning; Pattern recognition; Reinforcement learning; Semi-supervised learning; Supervised learning; Unsupervised learning; Convex programming; Nonlinear programming; Stochastic optimisation; Differential privacy; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Non-parametric & semi-parametric methods; Simulation; Time series; Asymptotic (Statistical methods & theory); Estimation theory; Information theory (Statistical methods & theory); Modelling (Statistical methods & theory); Probability; Algebra; ,Song Liu is a lecturer in University of Bristol. Before; he was a Project Assistant Professor in The Institute of Statistical Mathematics; Tokyo; Japan. he got his Doctor of Engineering degree from Tokyo Institute of Technology supervised by Professor Masashi Sugiyama and was awarded the DC2 Fellowship from Japan Society for the Promotion of Science.; Song Liu focuses on statistical machine learning algorithms identifying and interpreting differences from structured datasets. The datasets collected from real world systems are rarely static and usually evolve over time. Comparing two datasets collected at two different time points or spatial locations may highlight the patterns that change with these physical properties. For example; by comparing twitter keywords patterns before and after a social event; we can see that how the online community responds to a major incident which further helps us understand the change of sentiment toward this event. His main theoretical research focuses on understanding the statistical properties of difference learning algorithms; such as change-point detection algorithms; and creating statistical methods capturing various patterns that evolve over time/space. His main application research focuses on applying change detection algorithms on different datasets and reveal specific patterns that may help us understand the underlying system. For example; learning changes between two fMRI images helps us understand how our brain changes its behaviour when given two different tasks.; Song Liu was the recipient of the DC2 Fellowship from Japan Society for the Promotion of Science.; ,Song Liu focuses on statistical machine learning algorithms identifying and interpreting differences from structured datasets. The datasets collected from real world systems are rarely static and usually evolve over time. Comparing two datasets collected at two different time points or spatial locations may highlight the patterns that change with these physical properties. For example; by comparing twitter keywords patterns before and after a social event; we can see that how the online community responds to a major incident which further helps us understand the change of sentiment toward this event. His main theoretical research focuses on understanding the statistical properties of difference learning algorithms; such as change-point detection algorithms; and creating statistical methods capturing various patterns that evolve over time/space. His main application research focuses on applying change detection algorithms on different datasets and reveal specific patterns that may help us understand the underlying system. For example; learning changes between two fMRI images helps us understand how our brain changes its behaviour when given two different tasks.; ,Song Liu was the recipient of the DC2 Fellowship from Japan Society for the Promotion of Science.; ,Dr Song Liu 
sophia-ananiadou,Deep learning; Natural language processing; Linguistics; ,Sophia Ananiadou is Professor in Computer Science at the University of Manchester. She gained her PhD from UMIST in the area of natural language processing. Her research is interdisciplinary and her main contributions are in biomedical text mining and NLP. She is also the Director of the National Centre for Text Mining (since 2005); providing tools; resources; systems and infrastructure for biomedicine. She has had several projects in areas such as reconstruction of biological pathways using text mining; information extraction from the literature; the development of methods for the automation of systematic reviews and semantic search systems. She has applied her research mostly in systems biology; medicine; public health; biodiversity and chemistry.; Sophia's research at the Turing will be focusing on methods to increase the speed of knowledge discovery; develop decision-support tools to improve mental health decision-making and help clinicians and health practitioners make sense of the burgeoning scientific literature. By developing novel text mining and machine learning research methods the aim is to support discovery; organisation and prediction of the most relevant evidence from the literature in a dynamic; incremental and interactive way that will create a paradigm shift in mental health. This will aid the discovery of research evidence to automate the development of clinical guidelines; systematic reviews; policy and practice guidelines.; Building on proven methods which prioritise screening based on active learning; reduce the screening time and reduce bias; this research will go a step further. By examining relevancy at a deeper content level; beyond document; the complex aetiology of mental health will be unearthed by discovering and analysing different associations.; ,Sophia's research at the Turing will be focusing on methods to increase the speed of knowledge discovery; develop decision-support tools to improve mental health decision-making and help clinicians and health practitioners make sense of the burgeoning scientific literature. By developing novel text mining and machine learning research methods the aim is to support discovery; organisation and prediction of the most relevant evidence from the literature in a dynamic; incremental and interactive way that will create a paradigm shift in mental health. This will aid the discovery of research evidence to automate the development of clinical guidelines; systematic reviews; policy and practice guidelines.; Building on proven methods which prioritise screening based on active learning; reduce the screening time and reduce bias; this research will go a step further. By examining relevancy at a deeper content level; beyond document; the complex aetiology of mental health will be unearthed by discovering and analysing different associations.; ,['N/A'],Professor Sophia Ananiadou 
sotirios-sabanis,Numerical (Algorithms); Numerical analysis; Stochastic optimisation; Statistical methods & theory; High dimensional inference; Monte Carlo methods; Asymptotic (Statistical methods & theory); Estimation theory; Probability; ,Sotirios Sabanis is a Reader (Associate Professor) at the School of Mathematics of the University of Edinburgh. He received his undergraduate training in Mathematics at the Aristotle University of Thessaloniki and was awarded a PhD from Strathclyde University (Department of Statistics and Modelling Science); Glasgow. As Programme Director; he has led the development of the suite of postgraduate programmes in Computational Mathematical Finance at the University of Edinburgh and collaborates with industry (mainly financial services) through a number of joint projects. His research has appeared in the Annals of Applied Probability; SIAM Journal of Numerical Analysis; Stochastic Partial Differential Equations and other leading journals.;  ; My research focus recently has been on explicit numerical algorithms for nonlinear random systems of (typically) high dimension and their interplay with data science techniques. Examples of these algorithms are explicit numerical schemes for Stochastic (Partial) Differential Equations; stochastic approximation methods in the context of recursive identification of system parameters and MCMC algorithms. Any application of these methodologies to financial data is of keen interest to me.; ,My research focus recently has been on explicit numerical algorithms for nonlinear random systems of (typically) high dimension and their interplay with data science techniques. Examples of these algorithms are explicit numerical schemes for Stochastic (Partial) Differential Equations; stochastic approximation methods in the context of recursive identification of system parameters and MCMC algorithms. Any application of these methodologies to financial data is of keen interest to me.; ,['N/A'],Dr Sotirios Sabanis 
sotirios-tsaftaris,['N/A'],Dr. Sotirios (Sotos) Tsaftaris received the M.Sc. and Ph.D. degrees in Electrical and Computer Engineering from Northwestern University; USA; in 2003 and 2006; respectively. He is currently a Chancellor's Fellow (at a Senior Lecturer level) at the University of Edinburgh. Previously he was on the faculty of Northwestern University; USA and IMT Lucca; Italy. He has published extensively; particularly in interdisciplinary ﬁelds; with more than 100 journal and conference papers in his active record. His research interests are machine learning; computer vision; image analysis; image processing; and distributed computing.; Sotirios is interested in machine learning methods that work with multimodal input sources arising in medical imaging (health & wellbeing) applications where multiple information sources are used. Merging information across different sources should allow us to better characterise disease and its evolution. Since key to this is our ability to learn appropriate multimodal representation spaces; representation learning is an important aspect of my research. His team work on how they can devise methods that can advantage of such diverse sources whilst learning such spaces. Equally interesting is the ability to transfer these methods to new data sources (transfer learning) and even new tasks (multitask learning). Recent successes include deep learning models that use multimodal sources for image synthesis and biomarker discovery.; ,Sotirios is interested in machine learning methods that work with multimodal input sources arising in medical imaging (health & wellbeing) applications where multiple information sources are used. Merging information across different sources should allow us to better characterise disease and its evolution. Since key to this is our ability to learn appropriate multimodal representation spaces; representation learning is an important aspect of my research. His team work on how they can devise methods that can advantage of such diverse sources whilst learning such spaces. Equally interesting is the ability to transfer these methods to new data sources (transfer learning) and even new tasks (multitask learning). Recent successes include deep learning models that use multimodal sources for image synthesis and biomarker discovery.; ,['N/A'],Dr Sotirios Tsaftaris 
stefan-grosskinsky,Numerical (Algorithms); Mathematical physics; Multi-agent systems; Evolution & adaptation; Pattern formation; Ensemble (Mathematical modelling); Graph theory; Stochastic (Mathematical modelling); Data science of government & politics; Monte Carlo methods; Simulation; Probability; ,Stefan Grosskinsky is a Reader at the Warwick Mathematics Institute and a member of the Centre for Complexity Science. He works on emergent phenomena in complex interacting systems with applications in social and biological sciences; using methods from probability; statistical mechanics and computational approaches. A particular focus of his research is the probabilistic modelling of aggregation and transport processes in the framework of stochastic particle systems; and their applications in physics; biology and economics.Stefan obtained his PhD in Mathematical Physics at TU Munich in 2004 under the supervision of Herbert Spohn. After a fixed-term appointment as Lecturer in Applicable Mathematics at the Statistical Laboratory in Cambridge and a teaching fellowship at Churchill College; he moved to Warwick in 2007. Since then he has been part of the management team of the Centre for Doctoral Training in Complexity Science and later in Mathematics for Real-World Systems; and is currently coordinator of the associated MSc programme.; Stefan’s research on dynamic large deviations in stochastic particle systems (SPS) is related to the programme in data-centric engineering. SPS are minimal models of complex systems; and rare fluctuations of dynamic quantities such as particle or energy currents in physical systems have recently attracted major research interest in statistical mechanics and beyond. Current simulation techniques in that field are based on the classical idea of evolutionary algorithms where a large number of realisations of the process is run in parallel; and subject to selection based on their performance. Optimising such algorithms and providing rigorous bounds on convergence properties continues to be a question of significant research interest across disciplines; and we recently completed a first contribution in collaboration with Letizia Angeli (PhD Enrichment Student); Adam Johansen (Data-Centric Engineering Group Leader) and Andrea Pizzoferrato (Research Associate).Stefan recently started working on the impact of the monetary system on wealth inequality; as a natural application area for agent-based modelling of aggregation phenomena. The aim is to build on foundational work from econophysics in combination with data from the UK economy; to gain a detailed understanding of monetary dynamics and its role during and since the financial crisis in 2008. This is related to the Turing's economic data science programme; and current collaborators are Alexander Karalis Isaac (Warwick Economics) and Samuel Forbes (PhD student at the CDT Mathematics for Real-World Systems in Warwick).; ,Stefan’s research on dynamic large deviations in stochastic particle systems (SPS) is related to the programme in data-centric engineering. SPS are minimal models of complex systems; and rare fluctuations of dynamic quantities such as particle or energy currents in physical systems have recently attracted major research interest in statistical mechanics and beyond. Current simulation techniques in that field are based on the classical idea of evolutionary algorithms where a large number of realisations of the process is run in parallel; and subject to selection based on their performance. Optimising such algorithms and providing rigorous bounds on convergence properties continues to be a question of significant research interest across disciplines; and we recently completed a first contribution in collaboration with Letizia Angeli (PhD Enrichment Student); Adam Johansen (Data-Centric Engineering Group Leader) and Andrea Pizzoferrato (Research Associate).Stefan recently started working on the impact of the monetary system on wealth inequality; as a natural application area for agent-based modelling of aggregation phenomena. The aim is to build on foundational work from econophysics in combination with data from the UK economy; to gain a detailed understanding of monetary dynamics and its role during and since the financial crisis in 2008. This is related to the Turing's economic data science programme; and current collaborators are Alexander Karalis Isaac (Warwick Economics) and Samuel Forbes (PhD student at the CDT Mathematics for Real-World Systems in Warwick).; ,['N/A'],Dr Stefan Grosskinsky 
stefan-guttel,Numerical analysis; Control theory; Neural networks; Systems theory; Real time computing; Graph theory; Social media; Time series; Algebra; Calculus & analysis; ,Stefan Güttel is Reader in Numerical Analysis at the University of Manchester. He obtained his PhD in applied mathematics at TU Bergakademie Freiberg in Germany. His main research interests are in the field of computational mathematics; in particular; in parallel algorithms for high-dimensional problems arising with differential equations and in data-driven applications.; Data science relies on the availability of robust and fast parallel linear algebra techniques; be it within the optimisation routines for neural network training; in spectral clustering algorithms; or in the computation of centrality measures for complex networks. Stefan looks forward to working with Turing researchers to develop new algorithms for these purposes and to apply them in collaboration with practitioners in industry and public sectors.; ,Data science relies on the availability of robust and fast parallel linear algebra techniques; be it within the optimisation routines for neural network training; in spectral clustering algorithms; or in the computation of centrality measures for complex networks. Stefan looks forward to working with Turing researchers to develop new algorithms for these purposes and to apply them in collaboration with practitioners in industry and public sectors.; ,['N/A'],Dr Stefan Güttel 
steffen-petersen,Data structures; Numerical analysis; Robotics; Neural networks; Pattern formation; Communications; Databases; Deep learning; Natural language processing; Pattern recognition; Reinforcement learning; Ethics; ,Steffen Petersen is a Professor of Cardiovascular Medicine at William Harvey Research Institute; Queen Mary University of London and a Consultant Cardiologist and Clinical Director for Research at Barts Heart Centre; Barts Health NHS Trust. He is also the Cardiovascular Programme Director of UCLPartners Academic Medical Centre. He is Chair-elect; Cardiovascular MRI (Dec 2016-Dec 2018) of the European Society of Cardiology's (ESC) European Association of Cardiovascular Imaging (EACVI). He holds an MBCHB and MDRES equivalent (Dr med.) from Johannes Gutenberg University Mainz; Germany; a DPHIL (OXON) from the Department of Cardiovascular Medicine; University of Oxford; an MPH from Harvard School of Public Health. He is level 3 certified for cardiovascular magnetic resonance (CMR).; He has been actively involved in cardiovascular magnetic resonance since 1998 and he reports over 1000 cardiac adult MRI scans each year. His research interests include clinical trials using cardiovascular magnetic resonance (CMR); cost-effectiveness analysis related to cardiac imaging and primary prevention; large-scale population-based studies using CMR (UK Biobank cardiac imaging lead) and electronic health record research that incorporates cardiac imaging data.; Several projects of Steffen Petersen's current research projects involved artificial intelligence and machine learning and will benefit from The Alan Turing Insitute environment. UK Biobank requires analysis of 100;000 cardiac MR scans: automated image segmentation and image quality control are current examples of ongoing work. The Barts BioResource (>17500 consented patients) allows research using multi-dimensional healthcare data research. He will develop and validate a number of AI applications in cardiovascular research.; Fellow of the European Society of Cardiology; Fellow of the American College of Cardiology; Fellow of the Society for Cardiovascular Magnetic Resonance.; ,Several projects of Steffen Petersen's current research projects involved artificial intelligence and machine learning and will benefit from The Alan Turing Insitute environment. UK Biobank requires analysis of 100;000 cardiac MR scans: automated image segmentation and image quality control are current examples of ongoing work. The Barts BioResource (>17500 consented patients) allows research using multi-dimensional healthcare data research. He will develop and validate a number of AI applications in cardiovascular research.; ,Fellow of the European Society of Cardiology; Fellow of the American College of Cardiology; Fellow of the Society for Cardiovascular Magnetic Resonance.; ,Professor Steffen Petersen MD; DPHIL; MPH; FSCMR; FACC; FESC
stephen-eglen,Dynamical systems & differential equations; Neural networks; Neuroscience; Nonlinear dynamics; Pattern formation; Neural & evolutionary computing; Deep learning; ,Stephen Eglen is currently Reader in Computational Neuroscience at the University of Cambridge; in the Department of Applied Mathematics and Theoretical Physics. His undergraduate degree was in cognitive science; psychology and computer science (Nottingham); followed by doctorate in computer science and artificial intellegence (Sussex). His research interests focus on understanding the development of the nervous system: How do neurons form connections with each other into structured networks? He works primarily on analysing and modelling of neuronal activity and development in the visual system. Recent work has applied these techniques to understanding networks dervived from human stem cells and for neurotoxicity testing.; 1. Application of deep learning to understanding development of visual receptive fields.; 2. Enabling open research practices in computational neuroscience and more broadly.; 3. Promoting neuroscience as a data scienc; ,1. Application of deep learning to understanding development of visual receptive fields.; 2. Enabling open research practices in computational neuroscience and more broadly.; 3. Promoting neuroscience as a data scienc; ,['N/A'],Dr Stephen Eglen 
stephen-hansen,['N/A'],Stephen Hansen is an Associate Professor of Economics at the University of Oxford and a Fellow of University College. He received his BSc Econometrics and Mathematical Economics and PhD in Economics from the London School of Economics. Previously he was Assistant then Associate Professor at Pompeu Fabra University in Barcelona; Spain.; Stephen is primarily interested in organisational economics; with a particular focus on central bank design. He has worked on projects that examine the optimal structure of monetary policy committees; the impact of reputational incentives on policy dynamics; and the behavioural effects of transparency. Recently he has begun to incorporate unstructured text data into his research; which has led to a cross-over interest in statistical learning for high-dimensional discrete data.; ,Stephen is primarily interested in organisational economics; with a particular focus on central bank design. He has worked on projects that examine the optimal structure of monetary policy committees; the impact of reputational incentives on policy dynamics; and the behavioural effects of transparency. Recently he has begun to incorporate unstructured text data into his research; which has led to a cross-over interest in statistical learning for high-dimensional discrete data.; ,['N/A'],Dr Stephen Hansen 
stephen-mcgough,Robotics; Neural networks; Parallel computing; Neural & evolutionary computing; Computer vision; Deep learning; Natural language processing; Pattern recognition; ,Dr Stephen McGough is a Senior Lecturer in the School of Computing at Newcastle University. He studied Mathematics at Durham University as an undergraduate (1993) before studying his MSc (1996) and PhD (2000) at Newcastle University in the School of Computing. Stephen has previously worked at Imperial College London (2000-2009); University College London (2009); Newcastle University (2009-2013) and Durham University (2013-2017).; Stephen has a keen interest in machine learning and specifically deep learning which has developed as a consequence of his prior interests in mathematics and parallel computing. His interests in optimisation has led to a focus on the optimisation of deep learning networks. Although deep learning has shown; in many cases; super-human performance; this often requires vast computational resources. This is not only due to the vast amounts of data which need to be used for training; but also the need to identify the 'best' deep learning network to use. Stephen's research in this area is in how we can automatically identify the 'best' deep learning network to achieve optimal results without having to expend vast amounts of computing time.; ,Stephen has a keen interest in machine learning and specifically deep learning which has developed as a consequence of his prior interests in mathematics and parallel computing. His interests in optimisation has led to a focus on the optimisation of deep learning networks. Although deep learning has shown; in many cases; super-human performance; this often requires vast computational resources. This is not only due to the vast amounts of data which need to be used for training; but also the need to identify the 'best' deep learning network to use. Stephen's research in this area is in how we can automatically identify the 'best' deep learning network to achieve optimal results without having to expend vast amounts of computing time.; ,['N/A'],Dr Stephen McGough 
stephen-roberts,['N/A'],Stephen Roberts is the RAEng/Man Professor of Machine Learning at the University of Oxford. Stephen is a Fellow of the Royal Academy of Engineering; the Royal Statistical Society; the IET and the Institute of Physics. Stephen is Director of the Oxford-Man Institute of Quantitative Finance and Director of the Oxford Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (AIMS).; Stephen’s interests lie in methods for machine learning & data analysis in complex problems; especially those in which noise and uncertainty abound. His current major interests include the application of machine learning to huge astrophysical data sets (for discovering exo-planets; pulsars and cosmological models); biodiversity monitoring (for detecting changes in ecology and spread of disease); smart networks (for reducing energy consumption and impact); sensor networks (to better acquire and model complex events) and finance (to provide timeseries and point process models and aggregate large numbers of information streams).; ,Stephen’s interests lie in methods for machine learning & data analysis in complex problems; especially those in which noise and uncertainty abound. His current major interests include the application of machine learning to huge astrophysical data sets (for discovering exo-planets; pulsars and cosmological models); biodiversity monitoring (for detecting changes in ecology and spread of disease); smart networks (for reducing energy consumption and impact); sensor networks (to better acquire and model complex events) and finance (to provide timeseries and point process models and aggregate large numbers of information streams).; ,['N/A'],Professor Stephen Roberts 
steve-uhlig,Data structures; Communications; Time series; ,Steve Uhlig obtained a Ph.D. degree in Applied Sciences from the University of Louvain; Belgium; in 2004. From 2004 to 2006; he was a Postdoctoral Fellow of the Belgian National Fund for Scientific Research (F.N.R.S.). His thesis won the annual IBM Belgium/F.N.R.S. Computer Science Prize 2005. Between 2004 and 2006; he was a visiting scientist at Intel Research Cambridge; UK; and at the Applied Mathematics Department of University of Adelaide; Australia. Between 2006 and 2008; he was with Delft University of Technology; the Netherlands. Prior to joining Queen Mary; he was a Senior Research Scientist with Technische Universität Berlin/Deutsche Telekom Laboratories; Berlin; Germany.; Starting in January 2012; he is the Professor of Networks and Head of the Networks Research group at Queen Mary; University of London. Between 2012 and 2016; he was a guest professor at the Institute of Computing Technology; Chinese Academy of Sciences; Beijing; China.; Steve's research will design novel reactive and distributed Internet monitoring techniques. The key idea is to couple network monitoring and management; by introducing a feedback loop; so as to automatically adapt network management practices; towards better use of network resources and more secure network infrastructures. The first challenge is to identify and select appropriate unsupervised machine learning techniques; capable of taking as input network-wide distributed network measurements based on efficient data structures; to be included in the envisioned feedback loop. These techniques must be compatible with the distributed feed of network measurements; coming in the form of streams.; The main challenge of this project is to find an optimal way to handle the vast amounts of data that is typically the outcome of network monitoring; and instead use learning to converge towards surgical and highly scalable network measurements useful for network engineering.; ,Steve's research will design novel reactive and distributed Internet monitoring techniques. The key idea is to couple network monitoring and management; by introducing a feedback loop; so as to automatically adapt network management practices; towards better use of network resources and more secure network infrastructures. The first challenge is to identify and select appropriate unsupervised machine learning techniques; capable of taking as input network-wide distributed network measurements based on efficient data structures; to be included in the envisioned feedback loop. These techniques must be compatible with the distributed feed of network measurements; coming in the form of streams.; The main challenge of this project is to find an optimal way to handle the vast amounts of data that is typically the outcome of network monitoring; and instead use learning to converge towards surgical and highly scalable network measurements useful for network engineering.; ,['N/A'],Professor Steve Uhlig 
steven-bishop,Dynamical systems & differential equations; Multi-agent systems; Multi-agent reasoning; Control theory; Game theory; Nonlinear dynamics; Pattern formation; Systems theory; Pattern recognition; Graph theory; Data science of government & politics; Simulation; ,Steven Bishop is a Professor of Mathematics at UCL where he has remained since arriving in 1984 as a post-doctoral researcher. He has published around 200 academic papers; edited books and has had appearances on television and radio. Historically his research investigated topics such as chaos theory; reducing vibrations of engineering structures and how sand dunes are formed; but has more recently considered the implications of big data and used his modelling skills to capture the dynamics of social systems. He previously held a prestigious 'Dream' Fellowship funded by the UK research council EPSRC allowing him to consider creative ways to arrive at scientific narratives.; He was influential in the formation of a European network of physical and social scientists in order to investigate how decision support systems can be developed to assist policy makers and; to drive this; has organised conferences and art events in the UK and European Parliaments. He has been involved in several European Commission funded projects and has helped to forge a research agenda which looks at behaviour of systems that cross policy. His current research students collect data to study patterns in social behaviour such as human migration; crime and car accidents.; The main area of Steven's research will involve activities on the city scale adding to theme of urban analytics. His research aims to establish the obvious or subtle relationship between different; possibly seemingly disconnected social issues that may; nonetheless; have serious consequences. For instance; conflict or crime and the fear of crime have caused the displacement of millions of people around the world and so keeping track of changes in behaviour is important. Furthermore; the spread of misinformation can create echo chambers which; once started; are difficult to change; leading to prejudice and segregation.; Building on expertise in mathematical modelling; the Fellowship hopes to create mathematical narratives of urban dynamics based on models for the complex behaviour of individuals and the emergence of social patterns. Three different types of data will be considered: firstly; information about the city (such as transportation networks); secondly; population-based data (such as population density); and finally; social data (such as media and social media). This will allow modelling parameters to be benchmarked and the models tested and then used to forecast trends. The quantitative models will thus provide a window into social implications and create tools for policy-making. But it is not all equations and data.; He will establish a variety of creative outputs which go beyond the usual scientific journal articles by considering how art can facilitate a better understanding of scientific advances; which may prove especially useful in the policy-making arena.; Steven received a D.Sc in Mathematics in 2001 and was Elected Foreign Member of the Estonian Academy of Sciences December 2012; ,The main area of Steven's research will involve activities on the city scale adding to theme of urban analytics. His research aims to establish the obvious or subtle relationship between different; possibly seemingly disconnected social issues that may; nonetheless; have serious consequences. For instance; conflict or crime and the fear of crime have caused the displacement of millions of people around the world and so keeping track of changes in behaviour is important. Furthermore; the spread of misinformation can create echo chambers which; once started; are difficult to change; leading to prejudice and segregation.; Building on expertise in mathematical modelling; the Fellowship hopes to create mathematical narratives of urban dynamics based on models for the complex behaviour of individuals and the emergence of social patterns. Three different types of data will be considered: firstly; information about the city (such as transportation networks); secondly; population-based data (such as population density); and finally; social data (such as media and social media). This will allow modelling parameters to be benchmarked and the models tested and then used to forecast trends. The quantitative models will thus provide a window into social implications and create tools for policy-making. But it is not all equations and data.; He will establish a variety of creative outputs which go beyond the usual scientific journal articles by considering how art can facilitate a better understanding of scientific advances; which may prove especially useful in the policy-making arena.; ,Steven received a D.Sc in Mathematics in 2001 and was Elected Foreign Member of the Estonian Academy of Sciences December 2012; ,Professor Steven Bishop 
subramanian-ramamoorthy,Dynamical systems & differential equations; Multi-agent systems; Robotics; Control theory; Neural networks; Human computer interface; Deep learning; Natural language processing; Reinforcement learning; Verification; Probabilistic programming; Uncertainty quantification; Causality; Geometry & topology; ,Dr Subramanian Ramamoorthy is a Reader (Associate Professor) in the School of Informatics; University of Edinburgh; where he has been on the faculty since 2007. He is an Executive Committee Member for the Edinburgh Centre for Robotics and at the Bayes Centre. He received his PhD in Electrical and Computer Engineering from The University of Texas at Austin in 2007. He is an elected Member of the Young Academy of Scotland at the Royal Society of Edinburgh; and has been a Visiting Professor at Stanford University and the University of Rome 'La Sapienza'.; He serves as Vice President - Prediction and Planning at FiveAI; a UK-based startup company focussed on developing a technology stack for autonomous vehicles. His research focus is on robot learning and decision-making under uncertainty; with particular focus on achieving safety and robustness in artificially intelligent systems.; In recent years; AI has enjoyed a sustained period of widespread adoption; with AI-based systems being deployed widely in domains ranging from information retrieval to home entertainment. While many early applications were in domains where errors were tolerable to an extent; e.g.; advertisement placement; we now see AI being deployed in safety-critical applications involving physical interaction between humans and machines. This raises several new challenges that must necessarily be addressed if these technologies are to realise their potential benefits. A first challenge is the need for robust decision making despite noisy sensing - providing assurances regarding their closed-loop behaviour; through novel tools for introspection and interrogation of models.; One approach that will be investigated in detail is program induction; to reinterpret or analyse complex models by casting them in a compositional and programmatic form that is compatible with tools for analysis and safety verification (including from control theory and formal methods). A second challenge pertains to ambiguity in models and specifications; requiring techniques for using a dialogue with the human user to iteratively expand the task and model specification; in order to better approximate the intended meaning/behaviour. We will develop the paradigm of programming by discussion; wherein the target of dialogue is the model or reward function that is then used in decision-making. These new tools will enable progress towards the larger goal of safety-critical AI; in the context of experimental efforts within the domain of healthcare; where the Co-I's involvement allows us to make significant inroads into the emerging area of surgical assistance.; ,In recent years; AI has enjoyed a sustained period of widespread adoption; with AI-based systems being deployed widely in domains ranging from information retrieval to home entertainment. While many early applications were in domains where errors were tolerable to an extent; e.g.; advertisement placement; we now see AI being deployed in safety-critical applications involving physical interaction between humans and machines. This raises several new challenges that must necessarily be addressed if these technologies are to realise their potential benefits. A first challenge is the need for robust decision making despite noisy sensing - providing assurances regarding their closed-loop behaviour; through novel tools for introspection and interrogation of models.; One approach that will be investigated in detail is program induction; to reinterpret or analyse complex models by casting them in a compositional and programmatic form that is compatible with tools for analysis and safety verification (including from control theory and formal methods). A second challenge pertains to ambiguity in models and specifications; requiring techniques for using a dialogue with the human user to iteratively expand the task and model specification; in order to better approximate the intended meaning/behaviour. We will develop the paradigm of programming by discussion; wherein the target of dialogue is the model or reward function that is then used in decision-making. These new tools will enable progress towards the larger goal of safety-critical AI; in the context of experimental efforts within the domain of healthcare; where the Co-I's involvement allows us to make significant inroads into the emerging area of surgical assistance.; ,['N/A'],Dr Subramanian Ramamoorthy 
suhaib-fahmy,Hardware optimisation (FPGA/GPU); Computer systems & architectures; ,Suhaib Fahmy leads the Connected Systems Research Group within the School of Engineering at Warwick; where he is Reader in Computer Engineering. He graduated from Imperial College London in 2003 with an MEng in Information Systems Engineering; and in 2008 with a PhD in Electrical and Electronic Engineering. From 2007 to 2009; he was a Postdoctoral Research Fellow at CTVR; Trinity College Dublin and Visiting Research Engineer at Xilinx Research Labs; Ireland. He was Assistant Professor in the School of Computer Engineering at Nanyang Technological University from 2009 to 2015.; His research explores the use of reconfigurable computing systems in a connected context; from embedded to the datacenter; across a range of application domains. Working with FPGAs for well over a decade; he has explored how they can be virtualised to support more flexible exploitation of their computational efficiency and how computational capability can be added at the network interface to enhance system capabilities. At the Turing Institute; he is investigating how this interplay between computation and communication can be exploited to address large scale problems; and how real-time analytics capability can be achieved at scale. ; Dr Fahmy was a recipient of the Best Paper Award at the IEEE Conference on Field Programmable Technology in 2012; the IBM Faculty Award in 2013 and 2017; and the ACM TODAES Best Paper Award in 2019. He is a senior member of the ACM and IEEE and Chartered Engineer and Member of the IET. ; ,His research explores the use of reconfigurable computing systems in a connected context; from embedded to the datacenter; across a range of application domains. Working with FPGAs for well over a decade; he has explored how they can be virtualised to support more flexible exploitation of their computational efficiency and how computational capability can be added at the network interface to enhance system capabilities. At the Turing Institute; he is investigating how this interplay between computation and communication can be exploited to address large scale problems; and how real-time analytics capability can be achieved at scale. ; ,Dr Fahmy was a recipient of the Best Paper Award at the IEEE Conference on Field Programmable Technology in 2012; the IBM Faculty Award in 2013 and 2017; and the ACM TODAES Best Paper Award in 2019. He is a senior member of the ACM and IEEE and Chartered Engineer and Member of the IET. ; ,Dr Suhaib Fahmy 
sumeetpal-singh,['N/A'],Sumeetpal S. Singh received the B.E. (with first-class honours) and Ph.D. degrees from the Dept. of Electrical Engineering; University of Melbourne; Australia; in 1997 and 2002; respectively. After having worked in Industry for a number of years; he joined the Cambridge University Engineering Department in 2004 as a Research Associate and is currently a University Reader in Engineering Statistics.; He is also a Fellow and Director of Studies at Churchill College; an Affiliated Lecturer of the Statistics Laboratory and an Associate Editor of Statistics and Computing. His research focus is on statistical signal processing; in particular; using Monte Carlo methods; covering algorithmic development for applications and theoretical analysis.  He has been recognised for his work on Multi-target Tracking (awarded the IEEE M. Barry Carlton Award in 2013.)  ; Broadly; his research develops core statistical methodology for Data Science with the view of using cloud computing techniques. Analysing heterogeneous data sets [which is necessary for data-driven decision making] is a major hurdle in Data Science [e.g. Ecology; Epidemiology] as the mathematical models that describe the data are increasingly more intricate and very high dimensional in most cases. Fortunately; statistical inference is still feasible with Monte Carlo methods. His research is inherently cross-disciplinary as he draw on tools from Statistics; Engineering; Computer Science and Probability Theory to design and analyse new computational procedures.; ,Broadly; his research develops core statistical methodology for Data Science with the view of using cloud computing techniques. Analysing heterogeneous data sets [which is necessary for data-driven decision making] is a major hurdle in Data Science [e.g. Ecology; Epidemiology] as the mathematical models that describe the data are increasingly more intricate and very high dimensional in most cases. Fortunately; statistical inference is still feasible with Monte Carlo methods. His research is inherently cross-disciplinary as he draw on tools from Statistics; Engineering; Computer Science and Probability Theory to design and analyse new computational procedures.; ,['N/A'],Dr Sumeetpal Singh 
susan-banducci,Social data science; Data science of government & politics; ,Susan is the director of the Exeter Q-Step Centre working; along with a £1.44million investment from Nuffield; ESRC and Hefce and four new lecturers; toward advancing quantitative methods in the social sciences. She is a member of the Centre for Elections; Media and Participation.; In general; Susan's research focuses on inequalities in political participation; bringing together individual and institutional explanations for inequalities into multi-level analyses using large-scale cross-national surveys that. These inequalities in political engagement and participation include differences between men and women; minorities and non-minorities as well as how events over the lifecycle can contribute to inequalities.; One question that motivates this research; and has significant policy implications; is which electoral rules; political institutions or policies are best at reducing political inequalities. In particular; a number of findings in her research suggest that electoral reforms meant to make participation more meaningful are better at reducing inequalities in diverse societies while reforms aimed at making participation less costly exacerbate these inequalities.; Another motivating question is how the news media contribute to or ameliorate inequalities in turnout and political engagement. This latter question has motivated her most recent research. Her recent publications appear in top disciplinary (British Journal of Political Science) and interdisciplinary journals (Public Opinion Quaterly).; ,In general; Susan's research focuses on inequalities in political participation; bringing together individual and institutional explanations for inequalities into multi-level analyses using large-scale cross-national surveys that. These inequalities in political engagement and participation include differences between men and women; minorities and non-minorities as well as how events over the lifecycle can contribute to inequalities.; One question that motivates this research; and has significant policy implications; is which electoral rules; political institutions or policies are best at reducing political inequalities. In particular; a number of findings in her research suggest that electoral reforms meant to make participation more meaningful are better at reducing inequalities in diverse societies while reforms aimed at making participation less costly exacerbate these inequalities.; Another motivating question is how the news media contribute to or ameliorate inequalities in turnout and political engagement. This latter question has motivated her most recent research. Her recent publications appear in top disciplinary (British Journal of Political Science) and interdisciplinary journals (Public Opinion Quaterly).; ,['N/A'], Susan Banducci 
susan-grant-muller,Social media; ,Susan is Professor of Technologies and Informatics at the Institute for Transport Studies; University of Leeds and a Fellow of The Alan Turing Institute. She leads a programme of research into large scale data analytics and the role of new forms of technology enabled data in developing sustainable transport policy.; A statistician by discipline; she researches within the Intelligent Transport Systems field. Recently this has concerned the use of new and emerging data forms; social media; mobility profiling and the role of web enabled technologies in the transport sector. Her current interests lie in understanding the wider impacts of ICT enabled mobility infrastructure (such as health; energy and carbon impacts); the governance of micro-level user-generated data; and understanding incentivisation as part of behaviourally orientated demand management.; Susan is a Co-Investigator for the ESRC Consumer Data Research Centre (CDRC) a member of the Leeds Institute for Data Analytics (LIDA) and the University of Leeds Centre for Integrated Energy Research (CIER) funded by a substantive tranche of University strategic funding ; ,['N/A'],['N/A'],Professor Susan Grant-Muller 
suzy-moat,['N/A'],Suzy Moat is Professor of Behavioural Science at Warwick Business School; where she co-directs the Data Science Lab. She is also a Turing Fellow of The Alan Turing Institute. Her research investigates whether data on our usage of the Internet; from sources such as Google; Twitter; Wikipedia and Flickr can help us measure and predict human behaviour in the real world. The results of her work have been featured by television; radio and press worldwide; by outlets such as CNN; BBC; The Guardian; Wall Street Journal; New Scientist and Wired; and published in journals such as Proceedings of the National Academy of Sciences. She has acted as an advisor to government and public bodies on related topics. Moat studied Computer Science at UCL and Psychology at Edinburgh; during which time she won prizes including the UCL Faculty of Engineering Medal.  ; Everyday usage of the Internet leaves huge volumes of text and images in its wake. Suzy's research draws on these new data sources; and asks: can we use online data to measure human behaviour and experience we couldn’t measure before? Can we generate quicker; cheaper indicators of the wellbeing of society? Can we use these new data sources to predict human behaviour? Her previous work has touched on problems as diverse as linking online behaviour to stock market moves (with Preis; Curme; Stanley; et al.); estimating crowd sizes (with Botta and Preis) and evaluating whether the beauty of the environment we live in might affect our health (with Seresinhe and Preis). She is interested in generating indicators to support decision making in a range of domains; including economics and health.; ,Everyday usage of the Internet leaves huge volumes of text and images in its wake. Suzy's research draws on these new data sources; and asks: can we use online data to measure human behaviour and experience we couldn’t measure before? Can we generate quicker; cheaper indicators of the wellbeing of society? Can we use these new data sources to predict human behaviour? Her previous work has touched on problems as diverse as linking online behaviour to stock market moves (with Preis; Curme; Stanley; et al.); estimating crowd sizes (with Botta and Preis) and evaluating whether the beauty of the environment we live in might affect our health (with Seresinhe and Preis). She is interested in generating indicators to support decision making in a range of domains; including economics and health.; ,['N/A'],Professor Suzy Moat 
sylvia-richardson,['N/A'],Professor Sylvia Richardson is Director of the MRC Biostatistics Unit and holds the Chair of Biostatistics in the University of Cambridge since 2012. Prior to this; she was Directeur de Recherches at the French National Institute for Medical Research. In 2000; she moved to the UK to take up the Chair of Biostatistics at Imperial College. She was awarded the Guy Medal in Silver from the Royal Statistical Society in 2009. She is a Fellow of the Institute of Mathematical Statistics; of the International Society for Bayesian Analysis and was elected Fellow of the Academy of Medical Sciences in 2016.; She has worked extensively on statistical methodology and applications to the health sciences. Her main interest is centred on modelling and computational aspects of Bayesian statistics applied to complex-highly structured biological and epidemiological data sets. Her recent research has focussed on the analysis of large data problems such as those arising in genomics and on developing methods and scalable algorithms for clustering; sparse regression and large scale hierarchical analysis of high dimensional biomedical and multi-omics data.; ,She has worked extensively on statistical methodology and applications to the health sciences. Her main interest is centred on modelling and computational aspects of Bayesian statistics applied to complex-highly structured biological and epidemiological data sets. Her recent research has focussed on the analysis of large data problems such as those arising in genomics and on developing methods and scalable algorithms for clustering; sparse regression and large scale hierarchical analysis of high dimensional biomedical and multi-omics data.; ,['N/A'],Professor Sylvia Richardson 
sylvie-delacroix,Robotics; Human computer interface; Natural language processing; Differential privacy; Cognitive science; Data science of government & politics; Ethics; Research methods; Social media; Social psychology; ,Sylvie Delacroix is professor in Law and Ethics at the University of Birmingham; which she joined in January 2018; coming from UCL where she was a reader in Legal Theory and Ethics; with a fractional appointment in UCL Computer Science. Prior to that Sylvie was the Evelyn Green Davis Fellow at the Radcliffe Institute for Advanced Study (Harvard University; 2004-05); a lecturer in Law in Kent University and a post-doctoral scholar in Trinity College; Cambridge University. While in UCL Sylvie Delacroix was the founding Director of the UCL Centre for Ethics and Law; as well as the UCL Virtual Environments and the Professions Group. Professor Delacroix's work has notably been funded by the Wellcome Trust; the NHS and the Leverhulme Trust; from whom she received the Leverhulme Prize. Professor Delacroix has recently been appointed to the Public Policy Commission on the use of algorithms in the justice system (Law Society of England and Wales).; Professor Delacroix focuses on the intersection between law and ethics; with a particular interest in Machine Ethics; Agency and the role of habit within moral decisions (Habitual Ethics?; Bloomsbury; 2019). Her current research focuses on the design of both decision-support and `autonomous  systems meant for morally-loaded contexts; with a focus on the processes underlying both legal and medical decisions.She also researches the impact of the profile-based optimisation of our environment on ethical agency. She is currently considering the potential inherent in 'bottom-up' Data Trusts as a mechanism to address power imbalances between data-subjects and data-controllers (in collaboration with Neil Lawrence).; ,Professor Delacroix focuses on the intersection between law and ethics; with a particular interest in Machine Ethics; Agency and the role of habit within moral decisions (Habitual Ethics?; Bloomsbury; 2019). Her current research focuses on the design of both decision-support and `autonomous  systems meant for morally-loaded contexts; with a focus on the processes underlying both legal and medical decisions.She also researches the impact of the profile-based optimisation of our environment on ethical agency. She is currently considering the potential inherent in 'bottom-up' Data Trusts as a mechanism to address power imbalances between data-subjects and data-controllers (in collaboration with Neil Lawrence).; ,['N/A'],Professor Sylvie Delacroix 
taha-yasseri,['N/A'],Taha Yasseri is a Research Fellow in Computational Social Science at the Oxford Internet Institute; University of Oxford. He graduated from the Department of Physics at the Sharif University of Technology; Tehran; Iran with an MSc in Physics (2006). Dr Yasseri has obtained his PhD in Complex Systems Physics (2010) from the Institute of Theoretical Physics at the University of Göttingen; Germany.  ; Our personal and social daily activities produce an unprecedented amount of data. These data provide a unique opportunity to tackle some of the most important sociological questions in a framework similar to the one of empirical natural sciences. Dr Yasseri in interested in developing new techniques and methods for and applied to this sort of big transactional datasets. Moreover; Dr Yasseri is interested in developing mechanistic models —similar to ones in statistical mechanics— to go beyond observing patterns and correlations in platform-specific data; and generate general theories that not only explain causalities; but also provide quantitative predictions.; ,Our personal and social daily activities produce an unprecedented amount of data. These data provide a unique opportunity to tackle some of the most important sociological questions in a framework similar to the one of empirical natural sciences. Dr Yasseri in interested in developing new techniques and methods for and applied to this sort of big transactional datasets. Moreover; Dr Yasseri is interested in developing mechanistic models —similar to ones in statistical mechanics— to go beyond observing patterns and correlations in platform-specific data; and generate general theories that not only explain causalities; but also provide quantitative predictions.; ,['N/A'],Dr Taha Yasseri 
terry-lyons,['N/A'],Professor Terry Lyons is the Wallis Professor of Mathematics; he was a founding member (2007) of; and then Director (2011-2015) of; the Oxford Man Institute of Quantitative Finance; he was the Director of the Wales Institute of Mathematical and Computational Sciences (WIMCS; 2008-2011). He came to Oxford in 2000 having previously been Professor of Mathematics at Imperial College London (1993-2000); and before that he held the Colin Maclaurin Chair at Edinburgh (1985-93).  ; Prof Lyons’ long-term research interests are all focused on Rough Paths; Stochastic Analysis; and applications – particularly to Finance and more generally to the summarsing of large complex data. More specifically; his interests are in developing mathematical tools that can be used to effectively model and describe high dimensional systems that exhibit randomness. Prof Lyons is involved in a wide range of problems from pure mathematical ones to questions of efficient numerical calculation.; ,Prof Lyons’ long-term research interests are all focused on Rough Paths; Stochastic Analysis; and applications – particularly to Finance and more generally to the summarsing of large complex data. More specifically; his interests are in developing mathematical tools that can be used to effectively model and describe high dimensional systems that exhibit randomness. Prof Lyons is involved in a wide range of problems from pure mathematical ones to questions of efficient numerical calculation.; ,['N/A'],Professor Terry Lyons 
thomas-gernon,Operations research; Databases; Machine learning; Applications (Machine learning); Uncertainty quantification; Causality; High dimensional inference; Probability; ,Thomas Gernon is Associate Professor of Earth Science at the University of Southampton; based at the National Oceanography Centre. He gained his PhD in volcanology at the University of Bristol; focussing on multi-phase flows in volcanic environments. He is particularly interested in the application of machine learning to understand a wide range of geological phenomena (e.g.; forecasting hazardous flows); and to inform regulation of manmade (induced) earthquakes associated with oil and gas operations. His pilot project with the Alan Turing Institute; with Dr Thea Hincks as Turing Researcher; uses Bayesian Networks to analyse the complex joint dependencies between operational and geological parameters and induced seismicity in hydraulic fracking and fluid injection regions. Partners include Energy Regulators and researchers in the UK; USA and Canada. The project will develop next generation geospatial data analysis tools to automatically identify changing susceptibility to induced earthquakes in space and time. His work has been published in leading journals including Science; PNAS; Nature Climate Change and Nature Geoscience.; Gernon’s research explores the complex interactions between geological processes operating on different spatial and temporal scales; and in different environments. Recent research has ranged from investigating the coupling between global tectonic and geochemical cycles deep in Earth history; to the regional controls on seismicity today. He applies a range of different techniques; including remote sensing; fieldwork; chemical analysis; analogue experiments; and modelling to address these problems.; He has recently been involved in developing advanced Bayesian networks to determine the interplay between operational and geological variables in triggering man-made earthquakes. He has also been involved in an exciting study that utilised NASA spacecraft data and applied approximate Bayesian computation methods to understand and interpret the impact cratering record of the Earth and Moon. In the spirit of the Turing Institute; he is keen to apply data science and artificial intelligence to tackle major scientific challenges—especially those at the interface between environment; energy; and the economy.; ,Gernon’s research explores the complex interactions between geological processes operating on different spatial and temporal scales; and in different environments. Recent research has ranged from investigating the coupling between global tectonic and geochemical cycles deep in Earth history; to the regional controls on seismicity today. He applies a range of different techniques; including remote sensing; fieldwork; chemical analysis; analogue experiments; and modelling to address these problems.; He has recently been involved in developing advanced Bayesian networks to determine the interplay between operational and geological variables in triggering man-made earthquakes. He has also been involved in an exciting study that utilised NASA spacecraft data and applied approximate Bayesian computation methods to understand and interpret the impact cratering record of the Earth and Moon. In the spirit of the Turing Institute; he is keen to apply data science and artificial intelligence to tackle major scientific challenges—especially those at the interface between environment; energy; and the economy.; ,['N/A'],Dr Thomas Gernon 
thomas-hills,['N/A'],Thomas Hills is a Professor of Psychology at the University of Warwick. His initial training was in the neurogenetics and mathematical characterisation of animal foraging at the University of Utah Biology Department.  He subsequently worked with the University of Texas at Austin College of Education and Indiana University Department of Psychological and Brain Sciences; most recently working with the University of Basel before moving to his present position.; My research focuses on the quantitative description of cognitive information; such as the semantic structure of memory; the evolution of language; psychological attitudes towards concepts such as risk and immigration; and patterns of behaviour and language symptomatic of beliefs and underlying pathology.; ,My research focuses on the quantitative description of cognitive information; such as the semantic structure of memory; the evolution of language; psychological attitudes towards concepts such as risk and immigration; and patterns of behaviour and language symptomatic of beliefs and underlying pathology.; ,['N/A'],Professor Thomas Hills 
thomas-house,Numerical (Algorithms); Dynamical systems & differential equations; Deterministic (Mathematical modelling); Graph theory; Stochastic (Mathematical modelling); Social networks; Uncertainty quantification; Monte Carlo methods; Simulation; Modelling (Statistical methods & theory); Probability; Algebra; ,Thomas House is a Reader in the School of Mathematics at the University of Manchester.; Thomas' research interests relate to mathematical epidemiology - particularly how we can make the advances in data science and AI required to drive improvements in population health.; ,Thomas' research interests relate to mathematical epidemiology - particularly how we can make the advances in data science and AI required to drive improvements in population health.; ,['N/A'],Dr Thomas House 
thomas-irvine,Data structures; Social networks; Applications (Machine learning); ,Dr Thomas Irvine is an Associate Professor in Music at the University of Southampton and the Director of Programmes in Music for the academic year 2019-2020.; After studying viola at conservatoire (at the Shepherd School of Rice University and Indiana University Jacobs School of Music) he moved to Germany and played professionally; mostly in Early Music ensembles but also in symphony orchestras. He also taught for a year at the Frankfurt International School and worked as a manager for a large Early Music organisation.In 1999 he found his way to musicology and back to the US; studying performance practice and musicology at Cornell University; where he took his PhD in 2005. In 2002 he crossed the Atlantic again as a DAAD scholar at the University of Würzburg Institute of Musicology; where he stayed on as a postdoctoral fellow in 2005/06. He has lived and worked in Southampton since 2006.He is a Non-Executive Director of the Southampton Web Science Institute and currently serves as an external examiner at the Royal Academy of Music. He co-chairs the American Musicological Society study group ‘Global East Asia.’ Outside of his teaching and research he is trying to learn Chinese and follow Southampton FC. Both can be challenging! He also sings a little.; ,['N/A'],['N/A'],Dr Thomas Irvine 
thomas-lukasiewicz,['N/A'],Thomas Lukasiewicz is a Professor of Computer Science at the University of Oxford's Department of Computer Science since 2010. Prior to this; he was holding a prestigious Heisenberg Fellowship by the German Research Foundation; affiliated with the University of Oxford; TU Vienna; Austria; and Sapienza University of Rome; Italy. Among his recent awards are the Artificial Intelligence journal's Prominent Paper Award 2013 and the RuleML 2015 Best Paper Award. He is  associate editor for the Artificial Intelligence journal and the Journal of Artificial Intelligence Research; area editor for the journal ACM TOCL; and editor for the Semantic Web journal.; Thomas Lukasiewicz’s research interests are in artificial intelligence (AI) and information systems; including especially knowledge representation and reasoning; uncertainty in AI; machine learning; the (Social and/or Semantic) Web; and databases. His research at the Alan Turing Institute focuses on the following areas: (A) value added data systems to support users in discovering; extracting; integrating; accessing; and interpreting the data of relevance to their questions; (B) integrations of deep learning and symbolic knowledge representation and reasoning; (C) question answering over the Web; in plain text documents; and in videos; and (D) personalised search and recommender systems.; ,Thomas Lukasiewicz’s research interests are in artificial intelligence (AI) and information systems; including especially knowledge representation and reasoning; uncertainty in AI; machine learning; the (Social and/or Semantic) Web; and databases. His research at the Alan Turing Institute focuses on the following areas: (A) value added data systems to support users in discovering; extracting; integrating; accessing; and interpreting the data of relevance to their questions; (B) integrations of deep learning and symbolic knowledge representation and reasoning; (C) question answering over the Web; in plain text documents; and in videos; and (D) personalised search and recommender systems.; ,['N/A'],Professor Thomas Lukasiewicz 
thomas-monks,Management science; Simulation; Operations research; Mathematical modelling; Stochastic (Mathematical modelling); Machine learning; ,Dr Thomas Monks is exDirector of Data Science for the National Institute for Health Research (NIHR) Collaboration in Leadership and Applied Health Research (CLAHRC) Wessex. Thomas is a methodologist with expertise is in applying computer simulation methods; mathematical modelling and machine learning in health service delivery.; He joined University of Southampton in  2014 as part of NIHR CLAHRC Wessex. Prior to this he worked at University of Exeter Medical School as part of their healthcare Operational Research group. His early career was in computer science; software engineering and operational research in the private and public sectors. His work focuses on the translation of Data Science and Operational Research tools to health and social care.; Dr Monks’ research interests are related to the translation and application of data science and operational research in healthcare.; Real-time decision support systems: The onset of faster computer simulation and collection of real-time service delivery data to support machine learning means that the NHS can begin to adopt predictive and decision support tools for managing its demand. His research aims to explore how current developments in data science and operational research can be adapted and applied in healthcare (particularly in U&EC and community settings) and the gaps in current methodology for real-time prediction.; Implementation science: His research here is focussed on understanding the mechanisms the enable or prohibit the successful uptake of mathematical modelling and machine learning to improve service delivery in health and social care services.; Urgent and emergency care: This includes computer simulation; particularly discrete-event simulation of accident and emergency departments; computer simulation stroke services (particularly stroke thrombolysis); machine learning aiming to predict emergency demand and identifying intra-hospital barriers to patient flow.; Community services: His work in community services is concerned with logistics. Recent work has considered the provision and access of sexual health services in Hampshire; the development of routing and scheduling tools to coordinate how teams of nurses visit patients in their own homes; and the costs and consequences of different modes of delivering point of care testing for LRTI in the primary care.; Safe staffing of hospitals: His work with Professor Peter Griffiths is concerned with mathematical modelling of hospital staffing policies and how these decisions affect patient safety long term.; ,Dr Monks’ research interests are related to the translation and application of data science and operational research in healthcare.; Real-time decision support systems: The onset of faster computer simulation and collection of real-time service delivery data to support machine learning means that the NHS can begin to adopt predictive and decision support tools for managing its demand. His research aims to explore how current developments in data science and operational research can be adapted and applied in healthcare (particularly in U&EC and community settings) and the gaps in current methodology for real-time prediction.; Implementation science: His research here is focussed on understanding the mechanisms the enable or prohibit the successful uptake of mathematical modelling and machine learning to improve service delivery in health and social care services.; Urgent and emergency care: This includes computer simulation; particularly discrete-event simulation of accident and emergency departments; computer simulation stroke services (particularly stroke thrombolysis); machine learning aiming to predict emergency demand and identifying intra-hospital barriers to patient flow.; Community services: His work in community services is concerned with logistics. Recent work has considered the provision and access of sexual health services in Hampshire; the development of routing and scheduling tools to coordinate how teams of nurses visit patients in their own homes; and the costs and consequences of different modes of delivering point of care testing for LRTI in the primary care.; Safe staffing of hospitals: His work with Professor Peter Griffiths is concerned with mathematical modelling of hospital staffing policies and how these decisions affect patient safety long term.; ,['N/A'],Dr Thomas Monks 
thomas-reynolds,Dynamical systems & differential equations; Pattern recognition; Time series; ,Tom Reynolds is a Chancellor's Fellow at the University of Edinburgh. He has an MEng in engineering science from the University of Oxford and a PhD in civil engineering from the University of Bath. He has worked as a postdoctoral researcher at the University of Bath; researching dynamics of multi-storey timber buildings; and in the interdisciplinary Natural Material Innovation group at the University of Cambridge. He worked for four years in civil and structural engineering design consultancy with WYG and AKT (now AKTII); and is a chartered member of the Institution of Civil Engineers.; Tom's research aims to change the way structural engineers work. Engineers currently learn very little about the eventual performance of buildings they have designed; and so get little opportunity to incorporate that experience into future designs. Sensor technology exists to change this; since relatively low-cost sensors can be incorporated into buildings to provide data on the in-situ behaviour of the structure. The obstacle to creating this feedback is to turn this data into the information required by designers. Focusing on measurement of vibration due to wind and the movement of people; he has carried out in-situ vibration measurements on many buildings and structures over the past five years; ranging from timber attic frames to the stage for the London Olympic opening ceremony.; He has worked with structural engineers to learn from those in-situ measurements; to compare them with the results of their analysis; and to modify future designs on that basis. Particularly valuable information comes from measurements which show the change of structural properties over time; for example as the bare structural frame is fitted out and eventually becomes a functional building. This can show which secondary elements in the structure or environmental conditions may cause the behaviour to deviate from predictions. What is required to make this a mainstream way of working is to develop methods which can efficiently process and visualise the results of long-term monitoring; illuminating the change in properties with time.; ,Tom's research aims to change the way structural engineers work. Engineers currently learn very little about the eventual performance of buildings they have designed; and so get little opportunity to incorporate that experience into future designs. Sensor technology exists to change this; since relatively low-cost sensors can be incorporated into buildings to provide data on the in-situ behaviour of the structure. The obstacle to creating this feedback is to turn this data into the information required by designers. Focusing on measurement of vibration due to wind and the movement of people; he has carried out in-situ vibration measurements on many buildings and structures over the past five years; ranging from timber attic frames to the stage for the London Olympic opening ceremony.; He has worked with structural engineers to learn from those in-situ measurements; to compare them with the results of their analysis; and to modify future designs on that basis. Particularly valuable information comes from measurements which show the change of structural properties over time; for example as the bare structural frame is fitted out and eventually becomes a functional building. This can show which secondary elements in the structure or environmental conditions may cause the behaviour to deviate from predictions. What is required to make this a mainstream way of working is to develop methods which can efficiently process and visualise the results of long-term monitoring; illuminating the change in properties with time.; ,['N/A'],Dr Thomas Reynolds 
tiejun-ma,Parallel computing; Applications (Machine learning); Stochastic (Mathematical modelling); Management science; Modelling (Statistical methods & theory); ,Dr Tiejun Ma is currently a Reader in Business Informatics; theme leader for Business Application for Informatics; Centre for Intelligent Systems and Applications; University of Edinburgh.He was an Associate Professor in Risk Analysis within Centre for Risk Research and he worked at Imperial College London and University of Oxford. Dr Ma’s research focuses on risk analysis and decision-making using quantitative modelling of user’s behaviour with big data analysis techniques (e.g. individual's financial/cyber risk taking and decision behaviour) and fintech.Dr Ma is also a member of the Risk Management Research and Thought Leadership Committee; of Faculty of Actuaries (IFoA). Recently; Dr Ma’s research projects with leading city industry partners have been twice awarded ‘Certificate of Excellence’ by InnovationUK and he won the Lenovo AI on Fintech Innovation Award (SuperComputing 2017).; Dr Ma’s research focuses on risk analysis and decision-making using quantitative modelling and real-time Big data analysis techniques applies to fintech; cyber-risk; and resilience. He employs applied data science and mathematical modelling methodologies in the analysis/forecasts of risks; using an inter-disciplinary research strategy; via state-of-the-art computing; data analytics and behavioural analysis techniques.Dr Ma is a selected member of EPSRC’s New Economy Model Network and is a selected reviewer for EPSRC/ESRC proposals. Dr Ma has extensive experience in large-scale data analysis; risk modelling and real-time decision-making of finance and computing challenges and has been published on international leading journals; such as European Journal of Operations Research; International Journal of Forecasting; IEEE Transaction on Knowledge and Data Engineering and IEEE Transaction on Dependable and Secure Computing and Journal of Information Assurance and Security etc. Dr Ma has been successful in securing £4m+ in research grants on 30 projects (as PI or CI) from Research Councils (e.g. EPSRC/ESRC/Innovation UK/GCHQ/HEA etc.) and industry partners.; ,Dr Ma’s research focuses on risk analysis and decision-making using quantitative modelling and real-time Big data analysis techniques applies to fintech; cyber-risk; and resilience. He employs applied data science and mathematical modelling methodologies in the analysis/forecasts of risks; using an inter-disciplinary research strategy; via state-of-the-art computing; data analytics and behavioural analysis techniques.Dr Ma is a selected member of EPSRC’s New Economy Model Network and is a selected reviewer for EPSRC/ESRC proposals. Dr Ma has extensive experience in large-scale data analysis; risk modelling and real-time decision-making of finance and computing challenges and has been published on international leading journals; such as European Journal of Operations Research; International Journal of Forecasting; IEEE Transaction on Knowledge and Data Engineering and IEEE Transaction on Dependable and Secure Computing and Journal of Information Assurance and Security etc. Dr Ma has been successful in securing £4m+ in research grants on 30 projects (as PI or CI) from Research Councils (e.g. EPSRC/ESRC/Innovation UK/GCHQ/HEA etc.) and industry partners.; ,,Dr Tiejun Ma 
tim-bishop,Modelling (Statistical methods & theory); ,Tim holds a BSc in Mathematics and Computer Science from the University of Bristol and an MSc from the same University in Statistics. His PhD. was awarded in 1978 by the University of Sheffield in Probability and Statistics; the topic was the application of game theory to modelling animal conflicts.; In 1977;  he moved to the University of Utah; Salt Lake City; for a post-doc (followed by Faculty appointments) to work with Professor Mark Skolnick and the late Professor Roger Williams developing statistical approaches to study disease aggregation in families. As part of their activities; they worked on early approaches to use recombinant DNA technology to study and utilise genetic variation to understand human disease. As such; they were highly involved in the early phases of the Human Genome Project and in applying the rudimentary map to map and clone disease genes; notably genes associated with neurofibromatosis; haemochromatosis; breast cancer; bowel cancer and melanoma. The initial success with finding NF1 (the neurofibromatosis gene) and understanding the association between haemochromatosis and HLA showed the potential for the technology.; He was recruited to Leeds by the Imperial Cancer Research Fund (now Cancer Research UK) in 1989 to develop a Genetic Epidemiology Unit associated with the University of Leeds. Cancer Research UK has supported their activities since that time.  ; Tim's research aims to:; ,Tim's research aims to:; ,['N/A'],Professor Tim Bishop 
tim-cannings,Machine learning; Statistical methods & theory; ,Tim is a lecturer in statistics and data science at the University of Edinburgh. He completed his PhD in statistics with Prof Richard Samworth at the University of Cambridge in 2015. He has previously held postdoctoral research positions in the Statistical Laboratory; University of Cambridge and in the Marshall School of Business; University of Southern California.; Tim is interested in statistical methodology and theory for large-scale data problems. In particular; he has developed a classification algorithm for high-dimensional data; based on aggregating the results of applying many low-dimensional random projections to the original data. He is now extending this research to other tasks; such as clustering. Recently he has been working on predicting cancer tumour relapse rates using genomic data; the ultimate goal of this project is to help determine the best therapeutic strategy for cancer patients before they begin treatment.; ,Tim is interested in statistical methodology and theory for large-scale data problems. In particular; he has developed a classification algorithm for high-dimensional data; based on aggregating the results of applying many low-dimensional random projections to the original data. He is now extending this research to other tasks; such as clustering. Recently he has been working on predicting cancer tumour relapse rates using genomic data; the ultimate goal of this project is to help determine the best therapeutic strategy for cancer patients before they begin treatment.; ,['N/A'],Dr Tim Cannings 
tim-dodwell,Uncertainty quantification; Numerical analysis; Mathematical modelling; ,Dr Tim Dodwell is a senior lecturer in industrial applied mathematics within the Institute of Data Science and AI at the University of Exeter; in which he is the institute’s lead for ‘science; engineering and technology’. Tim is a Turing Fellow at the Alan Turing Institute; currently holds the Romberg Visiting Professor ship at Heidelberg and sits on the board member for Proceedings of Royal Society London A. At Exeter he leads an interdisciplinary group at the dynamic interface between applied mathematics; statistics and high-performance scientific computing; in the areas of: stochastic uncertainty quantification; multiscale modelling of complex materials and high-performance numerical solvers. Over the next 18 months his group will double to 16 researchers built from UKRI; EPSRC; Innovate UK and direct industrial awards with a combined value exceeding £6.5M.; He works at the dynamic interface between engineering; applied mathematics and high performance scientific computing. His research interests in modelling complex materials for industry-motivated problems; integrating understanding of engineering materials; mathematical models; numerical and Bayesian methods and their implementation across massive computer architectures.; A core aim of his research outputs is to develop robust; validated mathematical methods to tackle critical industrial challenges which require the close synergy of complex numerical models and rich convoluted data sets. His theoretical research has had direct impact in the high value manufacturing sector; by working closely with two of largest international composite manufactures Boeing and GKN Aerospace. Having said this; his theoretical research has much broader applications across the sciences; including new applications to other engineering systems; the environment and healthcare.; ,He works at the dynamic interface between engineering; applied mathematics and high performance scientific computing. His research interests in modelling complex materials for industry-motivated problems; integrating understanding of engineering materials; mathematical models; numerical and Bayesian methods and their implementation across massive computer architectures.; A core aim of his research outputs is to develop robust; validated mathematical methods to tackle critical industrial challenges which require the close synergy of complex numerical models and rich convoluted data sets. His theoretical research has had direct impact in the high value manufacturing sector; by working closely with two of largest international composite manufactures Boeing and GKN Aerospace. Having said this; his theoretical research has much broader applications across the sciences; including new applications to other engineering systems; the environment and healthcare.; ,['N/A'],Dr Tim Dodwell 
tim-lenton,Complexity (Algorithms); Numerical (Algorithms); Dynamical systems & differential equations; Information theory (Applied mathematics); Multi-agent systems; Numerical analysis; Control theory; Evolution & adaptation; Game theory; Nonlinear dynamics; Systems theory; Databases; Visualisation (Computer systems & architectures); Applications (Machine learning); Pattern recognition; Automata & algebraic (Mathematical modelling); Deterministic (Mathematical modelling); Dynamic/static (Mathematical modelling); Stochastic (Mathematical modelling); Stochastic optimisation; Software framework development; Social networks; Simulation; Time series; Modelling (Statistical methods & theory); ,Tim Lenton is Professor of Climate Change and Earth System Science at the University of Exeter and Director of the Global Systems Institute. He has over 20 years of research experience in studying the Earth as a system; and developing and using models to understand its behaviour.; He is particularly interested in how life has reshaped the planet in the past; and what lessons we can draw from this as we proceed to reshape the planet now. These topics are covered in his books ‘Earth System Science: A Very Short Introduction’ (OUP; 2016) and (with Andrew Watson) ’Revolutions that made the Earth’ (OUP; 2011).; Tim leads the University of Exeter’s Climate Change MOOC (Massive Open Online Course); which has attracted over 60;000 learners worldwide since its launch in 2014.; Tim is particularly interested in how data science and artificial intelligence can be used to help create a more sustainable future for 9-11 billion people on this finite planet. He recently introduced the possibility of ‘Gaia 2.0’ – humans adding some self-aware feedback to the Earth’s self-regulation. Sensor technology and machine learning will be crucial if we are to achieve that.; Tim’s work identifying climate tipping points and developing early warning methods for them won the Times Higher Education Award for Research Project of the Year. He has also received a Philip Leverhulme Prize 2004; a European Geosciences Union Outstanding Young Scientist Award 2006; the British Association Charles Lyell Award Lecture 2006 and the Geological Society of London William Smith Fund 2008. Tim is a Fellow of the Linnean Society; the Geological Society and the Society of Biology and he holds a Royal Society Wolfson Research Merit Award (2013).; ,Tim is particularly interested in how data science and artificial intelligence can be used to help create a more sustainable future for 9-11 billion people on this finite planet. He recently introduced the possibility of ‘Gaia 2.0’ – humans adding some self-aware feedback to the Earth’s self-regulation. Sensor technology and machine learning will be crucial if we are to achieve that.; ,Tim’s work identifying climate tipping points and developing early warning methods for them won the Times Higher Education Award for Research Project of the Year. He has also received a Philip Leverhulme Prize 2004; a European Geosciences Union Outstanding Young Scientist Award 2006; the British Association Charles Lyell Award Lecture 2006 and the Geological Society of London William Smith Fund 2008. Tim is a Fellow of the Linnean Society; the Geological Society and the Society of Biology and he holds a Royal Society Wolfson Research Merit Award (2013).; ,Professor Tim Lenton 
timothy-hospedales,Neural networks; Computer vision; Deep learning; Reinforcement learning; ,Timothy Hospedales is a Reader (Associate Professor) at the University of Edinburgh. He received his PhD from the University of Edinburgh; advised by Professor Sethu Vijayakumar. He was postdoctoral researcher and then lecturer at Queen Mary University of London prior to his current appointment.; His research focuses on improving data and annotation efficiency in machine learning methods. To this end he studies lifelong transfer learning; meta-learning; unsupervised; weakly-supervised; semi-supervised and active learning. His primarily application is computer vision; but has studied diverse areas including the vision-language interface; robot control; medical diagnosis; finance and computational neuroscience.; ,His research focuses on improving data and annotation efficiency in machine learning methods. To this end he studies lifelong transfer learning; meta-learning; unsupervised; weakly-supervised; semi-supervised and active learning. His primarily application is computer vision; but has studied diverse areas including the vision-language interface; robot control; medical diagnosis; finance and computational neuroscience.; ,['N/A'],Dr Timothy Hospedales 
tjeerd-van-staa,Data structures; Uncertainty quantification; Causality; Probability; ,Tjeerd Pieter van Staa is a physician (MD) and clinical epidemiologist with training in Pharmaco-epidemiology from McGill University (MSc) and Utrecht University (PhD) and training in Medical Ethics from Kings College (MA). He is Professor of Health eResearch at the Health eResearch Centre; University of Manchester. One of his current research interests is Efficient Trials; which aims to harness advanced health informatics and electronic health records to improve clinical trials. Another interest is the Learning HealthCare System in which routinely collected data are used to feedback actionable information to clinicians and patients. One research project is around optimising antibiotic prescribing in primary care. Recent work has focused on measuring the uncertainty and lack of generalisibility of risk prediction models that use routinely collected data (such as electronic health records from primary care). He has published over 230 peer-reviewed articles and is a well-recognised speaker in the field of pragmatic trials and clinical epidemiology.; The Turing work of Professor van Staa is focusing on the quantification of uncertainty in models that predict clinical risks for individual patients using routinely collected data (such as electronic health records [EHRs]). Machine learning techniques offer an interesting opportunity to further improve the predictive performance of clinical prediction models over conventional statistical models. Health care is increasingly guided by risk prediction models. Predictive modelling with electronic health records (EHRs) is anticipated to drive personalised medicine and improve healthcare quality. However; one of the challenges is that EHRs typically contain information that is recorded at the point of care by multiple clinicians and varied sites; with potential of major biases. The data are collected for the purposes of documenting clinical care rather than for the purposes of research.; It is well known that the quality and completeness of EHRs vary greatly but the impact on the accuracy on predicted individual risks is currently unknown. While clinical devices (such as blood pressure monitors) can only be used after evidence of minimal individual measurement errors; there is no such approach to risk prediction. A further challenge is the generalisibility of risk prediction models to other settings and EHR software settings (that store data differently). Research of van Staa and his group is investigating the generalisibility of transportability of risk prediction models across multiple heterogeneous clinical sites. The ultimate aim of this work is to implement approaches to minimise uncertainty in risk prediction models.; ,The Turing work of Professor van Staa is focusing on the quantification of uncertainty in models that predict clinical risks for individual patients using routinely collected data (such as electronic health records [EHRs]). Machine learning techniques offer an interesting opportunity to further improve the predictive performance of clinical prediction models over conventional statistical models. Health care is increasingly guided by risk prediction models. Predictive modelling with electronic health records (EHRs) is anticipated to drive personalised medicine and improve healthcare quality. However; one of the challenges is that EHRs typically contain information that is recorded at the point of care by multiple clinicians and varied sites; with potential of major biases. The data are collected for the purposes of documenting clinical care rather than for the purposes of research.; It is well known that the quality and completeness of EHRs vary greatly but the impact on the accuracy on predicted individual risks is currently unknown. While clinical devices (such as blood pressure monitors) can only be used after evidence of minimal individual measurement errors; there is no such approach to risk prediction. A further challenge is the generalisibility of risk prediction models to other settings and EHR software settings (that store data differently). Research of van Staa and his group is investigating the generalisibility of transportability of risk prediction models across multiple heterogeneous clinical sites. The ultimate aim of this work is to implement approaches to minimise uncertainty in risk prediction models.; ,['N/A'],Professor Tjeerd van Staa 
tobias-preis,['N/A'],Tobias Preis is Professor of Behavioural Science and Finance at the University of Warwick and a Turing Fellow of the Alan Turing Institute; the UK’s national institute for Data Science. Together with his colleague Dr. Suzy Moat; he directs the Data Science Lab at Warwick Business School. His recent research has aimed to analyse and predict real world behaviour with the volumes of data being generated by our interactions with technology; using data from Google; Wikipedia; Twitter; Flickr; Instagram and other sources. He has published over fifty research articles; book and book chapters in this area. Preis’ research is frequently featured in the news; by outlets including the BBC; the New York Times; the Financial Times; Science; Nature; Time Magazine; New Scientist and the Guardian. He has given a range of public talks including presentations at TEDx events in the UK and in Switzerland and he frequently advises governmental and commercial stakeholders around the globe.; Mammoth amounts of data are now being generated through society's extensive interactions with technological systems; automatically documenting collective human behaviour in a previously unimaginable fashion. Preis' research investigates whether data from sources such as Google; Wikipedia; Twitter; Flickr; Instagram and other sources can be used to:; ,Mammoth amounts of data are now being generated through society's extensive interactions with technological systems; automatically documenting collective human behaviour in a previously unimaginable fashion. Preis' research investigates whether data from sources such as Google; Wikipedia; Twitter; Flickr; Instagram and other sources can be used to:; ,['N/A'],Professor Tobias Preis 
tom-gaunt,Databases; Information retrieval; Applications (Machine learning); Natural language processing; Graph theory; Visualisation (Programming languages); Research methods; Causality; High dimensional inference; Simulation; ,Tom Gaunt is Professor of Health and Biomedical Informatics in the MRC Integrative Epidemiology Unit at the University of Bristol. Following an undergraduate degree in Biology he worked as a researcher at the University of Southampton; during which time he completed his PhD in Human Genetics. In 2005 he was awarded a British Heart Foundation Intermediate Fellowship and in 2006 secured a lectureship in bioinformatics and molecular genetics at the University of Bristol. In Bristol he has built a research team working in molecular epidemiology and biomedical informatics; co-leading a data mining and bioinformatics theme in the MRC Integrative Epidemiology Unit (IEU) from 2012-2017. In 2018 he was awarded a new MRC programme within the MRC IEU; with a focus on data mining approaches in population health science.; Tom leads a programme of research that applies informatics approaches to investigate the relationships between risk factors and health outcomes. His team uses population cohort data and software tools developed by the MRC IEU to build causal networks for thousands of risk factors and diseases. These causal networks are combined with an array of other biological knowledge; using literature mining; molecular pathways and drug target databases. His programme is building a comprehensive; open graph database (EpiGraphDB) containing these data. EpiGraphDB will be used to develop and apply novel data mining approaches to better understand the mechanisms of disease and identify novel intervention targets.; ,Tom leads a programme of research that applies informatics approaches to investigate the relationships between risk factors and health outcomes. His team uses population cohort data and software tools developed by the MRC IEU to build causal networks for thousands of risk factors and diseases. These causal networks are combined with an array of other biological knowledge; using literature mining; molecular pathways and drug target databases. His programme is building a comprehensive; open graph database (EpiGraphDB) containing these data. EpiGraphDB will be used to develop and apply novel data mining approaches to better understand the mechanisms of disease and identify novel intervention targets.; ,['N/A'],Professor Tom Gaunt 
tomas-petricek,['N/A'],"Tomas is an collaborating Turing Fellow at The Alan Turing institute; working on data-driven storytelling. He is building tools that integrate with modern data sources (open government data; data published by citizen initiatives) and let users easily create analyses and visualizations that are linked to the original data source; making the analyses more transparent; reproducible; but also easy to adapt. His early work on the project can be found at http://thegamma.net.; Tomas' many other interests include open-source and functional programming (he is an active contributor to the F# ecosystem); programming language theory (his PhD thesis on ""coeffects"" develops a theory of context-aware programming language language); but also understanding programming through the perspective of philosophy of science.; ","Tomas' many other interests include open-source and functional programming (he is an active contributor to the F# ecosystem); programming language theory (his PhD thesis on ""coeffects"" develops a theory of context-aware programming language language); but also understanding programming through the perspective of philosophy of science.; ",['N/A'],Dr Tomas Petricek 
trevor-graham,Dynamical systems & differential equations; Mathematical physics; Multi-agent systems; Control theory; Systems theory; Uncertainty quantification; Causality; High dimensional inference; Monte Carlo methods; Simulation; Time series; ,Trevor Graham is Professor of Cancer Evolution at the Barts Cancer Institute; QMUL; UK. His research focuses on measuring the evolutionary dynamics of cancer formation; and exploiting this knowledge to improve patient outcomes. His interdisciplinary laboratory combines expertise in genomics; bioinformatics and mathematical modelling. Trevor originally trained in mathematics before moving into cancer research during his doctoral and post-doctoral training.; Cancer research has entered a period of Big Data; foremost genomic sequencing and high resolution imaging. The goal of Trevor Graham's research is to find simple explanations for patterns in these data; through the data-driven evaluation of simple mechanistic mathematical models of cancer evolution.; ,Cancer research has entered a period of Big Data; foremost genomic sequencing and high resolution imaging. The goal of Trevor Graham's research is to find simple explanations for patterns in these data; through the data-driven evaluation of simple mechanistic mathematical models of cancer evolution.; ,['N/A'],Professor Trevor Graham 
tristan-bekinschtein,Complexity (Algorithms); Information theory (Applied mathematics); Neuroscience; Nonlinear dynamics; Human computer interface; Applications (Machine learning); Graph theory; Cognitive science; Uncertainty quantification; ,Tristan is a biologist; Master in Neurophysiology and PhD in Neuroscience; Buenos Aires University. He has been an EU Marie Curie Fellow and senior researcher at the MRC-Cognition and Brain Sciences Unit; Cambridge; and a Fyssen fellow at ICM; Paris. He has also been an Invited Researcher at QBI; Australia and INECO; Argentina. In 2011 he founded the Consciousness and Cognition Lab; now at the Department of Psychology; University of Cambridge. He is Wellcome Trust Fellow and now Turing Fellow.The interface between behavioural and brain data with health and wellbeing is finally starting to emerge as a discipline that can be integrated between different expertise; He aims to use brain and cognition mechanisms to guide the methods to find the interactions in that interface. His lab is primarily interested in the complexity of thought and what makes us human. He works on the cognitive neuroscience of consciousness; primarily on the fragmentation of cognition as we lose consciousness while falling asleep or getting sedated; on the cognitive and neural differences between conscious states; and on the interaction between attention and consciousness in health and disease.; At the Turing Tristan aims to collaborate to create projects at the crossroads between brain sciences; human behaviour; computational sciences and data sciences to understand mechanisms of health and disease; improve quality of life and efficiency of methods in society and medicine; and to help to unravel what makes us humans.; ,At the Turing Tristan aims to collaborate to create projects at the crossroads between brain sciences; human behaviour; computational sciences and data sciences to understand mechanisms of health and disease; improve quality of life and efficiency of methods in society and medicine; and to help to unravel what makes us humans.; ,['N/A'],Dr Tristan Bekinschtein 
ulrike-tillmann,['N/A'],Professor Ulrike Tillmann FRS has been at the University of Oxford since 1992. She is an algebraic topologist; known in particular for her work on Riemann surfaces and the homology of their moduli spaces. She has long standing research interest in homology stability questions. In 2011 she introduced an annual course (with Abramsky) in Computational Algebraic Topology at masters level. In the last year; Tillmann has co-organized four workshops on topological data analysis; as well as an CMI-LMS research school.; She held an EPSRC Advanced Fellowship 1997-2003. She was invited to present at the ICM in 2002 and was a member of the topology subject panel for both the 2010 and 2014 ICMs. In 2008 she was made a Fellow of the Royal Society and received the Bessel Forschungspreis from the Humboldt Gesellschaft. She is an inaugural Fellow of the AMS. Algebraic topology and its applications.; Algebraic topology is a very effective tool to study the global properties of geometric objects. For example; take the surface of a ball and divide it into triangles; now count the number of faces; add the number of vertices and subtract the number of edges; no matter how you choose your triangles; the result will always be 2. Do the same with the surface of a donut and the result is always 0. These numbers were already known by Euler and are foreshadows of homology developed in the 20th century. By now the basic ideas of algebraic topology have permeated nearly every branch of research in mathematics. Her own research has been motivated by questions in quantum physics and string theory. In particular; she has contributed to our understanding of the 'space of surfaces'.; ,Algebraic topology is a very effective tool to study the global properties of geometric objects. For example; take the surface of a ball and divide it into triangles; now count the number of faces; add the number of vertices and subtract the number of edges; no matter how you choose your triangles; the result will always be 2. Do the same with the surface of a donut and the result is always 0. These numbers were already known by Euler and are foreshadows of homology developed in the 20th century. By now the basic ideas of algebraic topology have permeated nearly every branch of research in mathematics. Her own research has been motivated by questions in quantum physics and string theory. In particular; she has contributed to our understanding of the 'space of surfaces'.; ,['N/A'],Professor Ulrike Tillmann 
vaishak-belle,['N/A'],Vaishak Belle is a Chancellor’s Fellow at the School of Informatics; University of Edinburgh; UK. Vaishak’s research is in artificial intelligence; machine learning and knowledge representation. Previously; he was at KU Leuven; the University of Toronto; and the Aachen University of Technology. He has co-authored several articles in AI-related venues; and his work has won the Microsoft best paper award at UAI; the Machine learning journal best student paper award at ECML-PKDD; and the Kurt Goedel silver medal.; His research is at the intersection of machine learning and symbolic systems (logics; programs); in service of the science and technology of artificial intelligence. He is motivated by the need to augment learning and perception with high-level structured; commonsensical knowledge; to enable systems to learn faster and more accurate models of the world. He is most keen on computational frameworks that are able to explain their decisions; modular; re-usable; and robust to variations in problem description. Concretely; he works on themes such as probabilistic inference and learning; probabilistic programming; automated planning; statistical relational learning and epistemic reasoning.; ,His research is at the intersection of machine learning and symbolic systems (logics; programs); in service of the science and technology of artificial intelligence. He is motivated by the need to augment learning and perception with high-level structured; commonsensical knowledge; to enable systems to learn faster and more accurate models of the world. He is most keen on computational frameworks that are able to explain their decisions; modular; re-usable; and robust to variations in problem description. Concretely; he works on themes such as probabilistic inference and learning; probabilistic programming; automated planning; statistical relational learning and epistemic reasoning.; ,['N/A'],Dr Vaishak Belle 
varun-kanade,['N/A'],Varun Kanade is a Research Lecturer in the Department of Computer Science; Oxford University. He has been a Simons Postdoctoral Fellow at the University of California; Berkeley and a FSMP postdoctoral fellow at ENS; Paris. He obtained his Ph.D. from Harvard University in 2012. His research interests are in machine learning and theoretical computer science.; Varun Kanade's research interests lie at the intersection of machine learning and theoretical computer science. At the ATI; his research will focus on mathematical foundations of machine learning; in particular statistical and computational tradeoffs; as well as improving our theoretical understanding of methods successful in practice. His work will also focus on randomized algorithms particularly for analysing large networks.; ,Varun Kanade's research interests lie at the intersection of machine learning and theoretical computer science. At the ATI; his research will focus on mathematical foundations of machine learning; in particular statistical and computational tradeoffs; as well as improving our theoretical understanding of methods successful in practice. His work will also focus on randomized algorithms particularly for analysing large networks.; ,['N/A'],Professor Varun Kanade 
vasco-m-carvalho,['N/A'],"Vasco M. Carvalho is currently Professor of Macroeconomics and Senior Keynes Fellow at the Faculty of Economics of the University of Cambridge.  He is also a co-ordinator of the Cambridge-INET institute and a Fellow of Jesus College. Having gained his PhD at the University of Chicago; he was a Junior Researcher at CREI in Barcelona and Affiliated Assistant Professor at University Pompeu Fabra before joining the University of Cambridge in 2013. He was awarded the 2014 Wiley Prize in Economics by the British Academy for ""achievement in research by an outstanding early career economist"".  He is the Principal Investigator of the European Research Council Grant ""MacroNets: Production Networks in Macroeconomics"" and of a Leverhulme Prize Fellowship.; Professor Carvalho's research focuses on the origins of business cycles. His research has shown how the organization of production around supply-chain networks exposes the aggregate economy to disruptions in critical nodes along these chains. Ongoing research seeks to further our understanding of how production networks affect macroeconomic outcomes by (i) providing empirical evidence on both the mechanisms and impact of such supply chain disruptions; (ii) analysing the dynamics of supply-chain formation and input diffusion; (iii) showing how production networks affect the size and direction of innovation and long-run growth.; ",Professor Carvalho's research focuses on the origins of business cycles. His research has shown how the organization of production around supply-chain networks exposes the aggregate economy to disruptions in critical nodes along these chains. Ongoing research seeks to further our understanding of how production networks affect macroeconomic outcomes by (i) providing empirical evidence on both the mechanisms and impact of such supply chain disruptions; (ii) analysing the dynamics of supply-chain formation and input diffusion; (iii) showing how production networks affect the size and direction of innovation and long-run growth.; ,['N/A'],Professor Vasco M. Carvalho 
vidit-nanda,['N/A'],Dr Vidit Nanda is an applied and computational algebraic topologist. Before starting at the Alan Turing Institute; he worked a post-doctoral research fellow at the University of Pennsylvania and as a PhD candidate in mathematics at Rutgers University.; Dr Nanda develops algebraic-topological theories; algorithms and software for the analysis of non-linear data and complex systems arising in various scientific contexts. In particular; he employs discrete Morse-theoretic techniques to substantially compress cell complexes built around the input data without modifying core topological properties. His recent work has involved excursions into computational geometry; cellular sheaf theory and higher-categorical localisation.; ,Dr Nanda develops algebraic-topological theories; algorithms and software for the analysis of non-linear data and complex systems arising in various scientific contexts. In particular; he employs discrete Morse-theoretic techniques to substantially compress cell complexes built around the input data without modifying core topological properties. His recent work has involved excursions into computational geometry; cellular sheaf theory and higher-categorical localisation.; ,['N/A'],Dr Vidit Nanda 
vili-lehdonvirta,['N/A'],Prof. Vili Lehdonvirta is an Associate Professor; Senior Research Fellow; and DPhil Programme Director at the Oxford Internet Institute; University of Oxford; and a Faculty Fellow of the Alan Turing Institute. Previously he worked at London School of Economics; University of Tokyo; and Helsinki Institute for Information Technology. Before his academic career; he worked as a game programmer. He has advised companies; startups; and policy makers in the United States; Europe; and Japan; including Rovio; Warner Brothers; and the World Bank.  ; His research deals with the design and socioeconomic implications of digital marketplaces and platforms; using conventional social research methods and novel data science approaches. He has a PhD in Economic Sociology from Turku School of Economics (2009) and an MSc in Information Networks from Helsinki University of Technology (2005).; ,His research deals with the design and socioeconomic implications of digital marketplaces and platforms; using conventional social research methods and novel data science approaches. He has a PhD in Economic Sociology from Turku School of Economics (2009) and an MSc in Information Networks from Helsinki University of Technology (2005).; ,['N/A'],Professor Vili Lehdonvirta 
vincent-c-muller,Systems theory; Differential privacy; Cognitive science; Ethics; ,"Vincent C. Müller studied philosophy with cognitive science; linguistics and history at the universities of Marburg; Hamburg; London and Oxford. Since his PhD he is teaching full time at Anatolia College/ACT (4-4; minus buyouts). He was Stanley J. Seeger Fellow at Princeton University and James Martin Research Fellow at the University of Oxford. He is now is Professor of philosophy at Anatolia College/ACT and University Academic Fellow at the University of Leeds - as well as President of the European Society for Cognitive Systems and chair of the euRobotics topics group on 'ethical; legal and socio-economic issues'.; Müller's research focuses on theory and ethics of disruptive technologies; particularly artificial intelligence. He has published ca. 40 academic papers as well as 14 edited volumes on the philosophy of computing; philosophy of AI and cognitive science; philosophy of language; applied ethics; etc. (citations: h-11). He has organised ca. 25 conferences or workshops and presents about one paper each month; usually by invitation. Müller organizes a conference series on the Theory and Philosophy of AI and is principal investigator of a EU-funded project on ""Inclusive Robotics for a Better Society"" (INBOTS). He has generated ca. 3.9 mil.€ research income for his institutions. At the Alan Turing Institute; his work focuses on the ethics of big data and AI.; ","Müller's research focuses on theory and ethics of disruptive technologies; particularly artificial intelligence. He has published ca. 40 academic papers as well as 14 edited volumes on the philosophy of computing; philosophy of AI and cognitive science; philosophy of language; applied ethics; etc. (citations: h-11). He has organised ca. 25 conferences or workshops and presents about one paper each month; usually by invitation. Müller organizes a conference series on the Theory and Philosophy of AI and is principal investigator of a EU-funded project on ""Inclusive Robotics for a Better Society"" (INBOTS). He has generated ca. 3.9 mil.€ research income for his institutions. At the Alan Turing Institute; his work focuses on the ethics of big data and AI.; ",['N/A'],Dr Vincent C Müller 
vito-latora,['N/A'],Vito Latora is Professor of Applied Mathematics and Chair of Complex Systems at Queen Mary University of London. Noted for his research in statistical physics and in complex networks; his current interests include time-varying and multiplex networks; and their applications to socio-economic systems and to the human brain.; Complex systems. Structure and dynamics of complex networks. Multiplex networks. Temporal networks. Social networks. Quantitative urbanism. Brain networks. The science of success. Innovation ecosystems; ,Complex systems. Structure and dynamics of complex networks. Multiplex networks. Temporal networks. Social networks. Quantitative urbanism. Brain networks. The science of success. Innovation ecosystems; ,['N/A'],Professor Vito Latora 
walid-magdy,Multi-agent reasoning; Natural language processing; Data science of government & politics; Social media; Social psychology; ,Walid Magdy is a lecturer at the School of Informatics; the University of Edinburgh (UoE); where he leads the social media analysis group. His main research interests include computational social science; data mining; and Arabic natural language processing. He holds his PhD from the School of Computing at Dublin City University (DCU); Ireland. He has an extensive industrial background from working earlier for IBM; Microsoft; and QCRI.; Walid has two main lines of research: 1) Computational social science; where he applies quantitative and qualitative analysis methods to study online users' behaviour. This includes social media analysis for political events; online racism; e-health; and privacy. 2) NLP for social data science; where he develops the required technology for analysing big social data; including sentiment analysis; stance detection; and demographic prediction; with additional focus on English and Arabic languages. Through The Alan Turing Institute; Walid is contributing to the research priority on technology and health & wellbeing through understanding human behaviour. He is currently working with the Turing defence & security programme on building better technologies for better analysis for social media.; ,Walid has two main lines of research: 1) Computational social science; where he applies quantitative and qualitative analysis methods to study online users' behaviour. This includes social media analysis for political events; online racism; e-health; and privacy. 2) NLP for social data science; where he develops the required technology for analysing big social data; including sentiment analysis; stance detection; and demographic prediction; with additional focus on English and Arabic languages. Through The Alan Turing Institute; Walid is contributing to the research priority on technology and health & wellbeing through understanding human behaviour. He is currently working with the Turing defence & security programme on building better technologies for better analysis for social media.; ,['N/A'],Dr Walid Magdy 
weisi-guo,Dynamical systems & differential equations; Communications; Social networks; Information theory (Statistical methods & theory); ,"Dr. Weisi Guo is an Associate Professor at Warwick and graduated from the University of Cambridge with degrees in General Engineering and Computer Science. He is a Turing Fellow and a Group Leader in the Turing's programme in Data-Centric Engineering. ; Dr Guo's research expertise is in information and networks. In terms of applications; he is particularly interested in interdependent critical infrastructures and urban analytics for defence and security. His pioneering research in molecular signalling and complex networks has won him a number of awards & nominations. At the Turing; he focuses on using complexity and data science techniques to improve engineering safety and reduce conflict likelihood [1].; [1] ""Retool AI to Forecast and Limit Wars;"" Nature; 562; 331-333 (2018); IET: Innovation Award 2015; Bell Labs Prize: Finalist 2014; Semi-Finalist 2016; IEEE: Best Paper and Exemplary Reviewer; ","Dr Guo's research expertise is in information and networks. In terms of applications; he is particularly interested in interdependent critical infrastructures and urban analytics for defence and security. His pioneering research in molecular signalling and complex networks has won him a number of awards & nominations. At the Turing; he focuses on using complexity and data science techniques to improve engineering safety and reduce conflict likelihood [1].; [1] ""Retool AI to Forecast and Limit Wars;"" Nature; 562; 331-333 (2018); ",IET: Innovation Award 2015; Bell Labs Prize: Finalist 2014; Semi-Finalist 2016; IEEE: Best Paper and Exemplary Reviewer; ,Dr Weisi Guo PhD
wilfrid-kendall,['N/A'],After a Mathematics DPhil from Oxford; Wilfrid worked first at University of Hull Statistics; then University of Strathclyde Mathematics; then University of Warwick Statistics; where he served a period as head of department and is now Professor of Statistics. He also served as Scientific Secretary and later as President of the Bernoulli Society for Mathematical Statistics and Probability; and was a founding co-director of the APTS scheme for training first-year UK Statistics students. He co-authored the primary monograph on stochastic geometry (with Stoyan and Mecke and now Chiu); currently in its 3rd edition.; Wilfrid is interested in research themes concerning interactions between probability; geometry and computation. These include: contributions to the theory of Markov chain Monte Carlo (most recently in applying Dirichlet form theory to study optimal scaling); developing new theoretical models for scale-invariant random spatial networks; and nascent projects concerning (a) a substantial and heterogeneous GIS database related to hypotheses concerning Anglo-Saxon building design; and (b) building and assessing a broad database on degradation of CT detector screens.; ,Wilfrid is interested in research themes concerning interactions between probability; geometry and computation. These include: contributions to the theory of Markov chain Monte Carlo (most recently in applying Dirichlet form theory to study optimal scaling); developing new theoretical models for scale-invariant random spatial networks; and nascent projects concerning (a) a substantial and heterogeneous GIS database related to hypotheses concerning Anglo-Saxon building design; and (b) building and assessing a broad database on degradation of CT detector screens.; ,['N/A'],Professor Wilfrid Kendall 
william-browne,Robotics; Human computer interface; Applications (Machine learning); Software framework development; Cognitive science; Research methods; Uncertainty quantification; Simulation; Modelling (Statistical methods & theory); ,Bill Browne is Professor of Statistics in the School of Education; University of Bristol. He studied all his degrees at the University of Bath finishing his PhD in the maths school in 1998. He has been a professor at Bristol for 11.5 years having spent seven years at the School of Veterinary Science as Professor of Biostatistics before moving to Education. He also was the founding director of the Jean Golding Institute for Data Intensive research from 2015-2017. These days he co-directs the Centre for Multilevel Modelling and his research interests are in statistical methods; software and applications. He developed the Bayesian MCMC functionality of the MLwiN software package and has published methodology papers on multilevel modelling as well as applied papers in a variety of disciplines. More recently he has become interested in teaching of statistics and the role of automation in both statistical analysis and teaching which is the topic of his fellowship.; Bill Browne's Turing fellowship is looking at how we might bridge the statistical skills gap through automation and improved statistical training. This works builds on a recently completed British Academy grant that created functionality in the StatJR software package to automate the production of teaching materials that use the SPSS software package.; Within the fellowship there are several strands: looking into translation across software packages; translation across different disciplines; adaptation of materials for assessment purposes; investigating what we can learn about teaching from automation and conversely what we can learn about data science training from education research. Bill hopes to work with Ben Murton at the Turing to bring together researchers and funders from different disciplines to share ideas about statistical training.; ,Bill Browne's Turing fellowship is looking at how we might bridge the statistical skills gap through automation and improved statistical training. This works builds on a recently completed British Academy grant that created functionality in the StatJR software package to automate the production of teaching materials that use the SPSS software package.; Within the fellowship there are several strands: looking into translation across software packages; translation across different disciplines; adaptation of materials for assessment purposes; investigating what we can learn about teaching from automation and conversely what we can learn about data science training from education research. Bill hopes to work with Ben Murton at the Turing to bring together researchers and funders from different disciplines to share ideas about statistical training.; ,['N/A'],Professor William Browne 
william-marsh,Artificial intelligence; Applications (Machine learning); Supervised learning; Probabilistic programming; Data science of government & politics; Ethics; Uncertainty quantification; Causality; Probability; ,William Marsh is a Senior Lecturer in Computer Science at Queen Mary University of London and a member of the Risk and Information research group. He has played a major role in the group's previous work on decision support for medical applications; working with Bayesian networks (BNs) and causal modelling. He has also pioneered the use of BNs in railway risk analysis. He enjoys collaborating with clinicians and has worked on applications in trauma care; forensic psychiatry; musculoskeletal injury; diabetes and rheumatology. His research interests are in better ways to build Bayesian networks suitable for decision support; effective ways to combine knowledge and data and ways to improve the usefulness of predictions to decision makers (e.g. explanation; explicit knowledge).Before joining Queen Mary he worked in software development and assessment and led software and safety assessment projects in avionics and railway engineering. He has a PhD from Southampton University.; Dr Marsh collaborates with clinical professionals to develop; test and deploy practical decision-making tools. His current projects focus on chronic medical conditions and the potential for decision support to enable greater patient autonomy (PamBayesian); improved triage in musculoskeletal injury (RealmAI) and trauma care (Trauma Models).Prediction in medicine (diagnosis; prognosis; the effect of treatment) is expected by many to transform health care. However; so far the practical impact has been limited (except perhaps in relation to diagnostic imaging). There are many practical challenges to overcome to change this; with questions such as: How do we know which predictions are likely to have the biggest impact? How do we capture and model medical knowledge efficiently? How can the very complex clinical data records in the Electronic Health Record (EHR) systems now used by hospitals and GPs be used routinely so that a prediction system improves with experience (like a real doctor)?; ,Dr Marsh collaborates with clinical professionals to develop; test and deploy practical decision-making tools. His current projects focus on chronic medical conditions and the potential for decision support to enable greater patient autonomy (PamBayesian); improved triage in musculoskeletal injury (RealmAI) and trauma care (Trauma Models).Prediction in medicine (diagnosis; prognosis; the effect of treatment) is expected by many to transform health care. However; so far the practical impact has been limited (except perhaps in relation to diagnostic imaging). There are many practical challenges to overcome to change this; with questions such as: How do we know which predictions are likely to have the biggest impact? How do we capture and model medical knowledge efficiently? How can the very complex clinical data records in the Electronic Health Record (EHR) systems now used by hospitals and GPs be used routinely so that a prediction system improves with experience (like a real doctor)?; ,['N/A'],Dr William Marsh 
xuan-vinh-doan,Operations research; Game theory; Information retrieval; Convex programming; Nonlinear programming; Stochastic optimisation; Management science; Probability; ,Dr Xuan Vinh Doan is an Associate Professor at Warwick Business School; the University of Warwick. Before joining Warwick Business School; he was a postdoctoral fellow in the Department of Combinatorics and Optimization at University of Waterloo (Canada) from November 2009 to September 2011. He completed his PhD from the Operations Research Center at MIT (USA); his master's study from the MIT-Singapore Aliance at NUS (Singapore); and his undergraduate study from RMIT (Australia).; His research interests are optimisation under uncertainty; which supports decision making given imperfect information or incomplete data; and optimisation-based machine learning; which concerns the development of optimisation techniques to solve data-mining problems; especially low-rank and sparse optimisation problems. He is also interested in prescriptive analytics which focuses on the interplay between data-driven optimisation and machine learning.; ,His research interests are optimisation under uncertainty; which supports decision making given imperfect information or incomplete data; and optimisation-based machine learning; which concerns the development of optimisation techniques to solve data-mining problems; especially low-rank and sparse optimisation problems. He is also interested in prescriptive analytics which focuses on the interplay between data-driven optimisation and machine learning.; ,['N/A'],Dr Xuan Vinh Doan 
yannick-wurm,Data structures; Robotics; Multi-agent reasoning; Neuroscience; Databases; Parallel computing; Human computer interface; Information retrieval; Pattern recognition; Software framework development; Simulation; ,Yannick Wurm is Senior Lecturer in Bioinformatics at Queen Mary University of London; a fellow of The Alan Turing Institute and the Software Sustainability Institute.; His lab takes advantage of the 50;000-fold drop in DNA sequencing cost over the past 10 years to study the evolution of social behaviour in ants and the effects of environmental challenges on pollinators. Working on such species; for which modern molecular and genomic resources are scarce; requires careful balancing of analytical efforts for robustness and productivity.; Yannick's background is highly computational. His first degree was as an engineer in bioinformatics and modelling from the French National Institute of Applied Sciences (INSA de Lyon). He subsequently obtained PhD in sociogenetics of ants at University of Lausanne in collaboration with the Swiss Institute of Bioinformatics.; Yannick aims to combine his lab's expertise with that of the Turing by developing methodologies to:; ,Yannick aims to combine his lab's expertise with that of the Turing by developing methodologies to:; ,['N/A'],Dr Yannick Wurm 
yanos-zylberberg,Databases; Applications (Machine learning); Pattern recognition; Social data science; Research methods; ,Yanos Zylberberg is a senior lecturer at the University of Bristol; has a PhD in Economics from Paris School of Economics and an undergraduate degree in Mathematics. His research interests span a range of economic fields; including labour economics; macroeconomics; development economics; economic history and urban economics. His current research agenda includes: the long-term effects of historical pollution on the structure of cities; migration and the organisation of production in developing economies; job search and unemployment insurance.; Yanos's research project at the Turing aims to study the patterns of urbanisation in China over the past 35 years; and the role of rural-urban migration in shaping such urbanisation. Rural-to-urban migration is central to the structural transformation of an economy; there is however very little empirical evidence on its contribution to urban development and the well-known empirical regularities observed in developing economies: urban sprawl; unequal access to urban amenities; unequal exposure to environmental disamenities; and residential segregation. The research will exploit the combination of (i) large agricultural shocks in rural areas; (ii) satellite imagery and supervised recognition algorithms to classify land use; and (iii) high-quality geo-located data on manufacturing establishments. ; ,Yanos's research project at the Turing aims to study the patterns of urbanisation in China over the past 35 years; and the role of rural-urban migration in shaping such urbanisation. Rural-to-urban migration is central to the structural transformation of an economy; there is however very little empirical evidence on its contribution to urban development and the well-known empirical regularities observed in developing economies: urban sprawl; unequal access to urban amenities; unequal exposure to environmental disamenities; and residential segregation. The research will exploit the combination of (i) large agricultural shocks in rural areas; (ii) satellite imagery and supervised recognition algorithms to classify land use; and (iii) high-quality geo-located data on manufacturing establishments. ; ,['N/A'],Dr Yanos Zylberberg 
yarin-gal,['N/A'],Yarin is an Associate Professor of Machine Learning at the Computer Science department at University of Oxford; Tutorial Fellow in Computer Science at Christ Church; Oxford and part-time fellow at the Alan Turing Institute. Prior to taking his position at Oxford he was a Research Fellow in Computer Science at St Catharine's College; Cambridge. He obtained his PhD from the Cambridge machine learning group; working with Zoubin Ghahramani and funded by the Google Europe Doctoral Fellowship. Prior to that he studied Computer Science at Oxford for a Master's degree under the supervision of Phil Blunsom.; Yarin's interests lie in the fields of linguistics; applied maths; and computer science. Most of his work is motivated by problems found at the intersections of these fields. In his current research he develops Bayesian techniques for deep learning; with applications to reinforcement learning. In the past he has worked on Bayesian modelling; approximate inference; and natural language processing. This work included Bayesian nonparametrics; Gaussian processes; inference algorithms for big data; and work on machine translation.; ,Yarin's interests lie in the fields of linguistics; applied maths; and computer science. Most of his work is motivated by problems found at the intersections of these fields. In his current research he develops Bayesian techniques for deep learning; with applications to reinforcement learning. In the past he has worked on Bayesian modelling; approximate inference; and natural language processing. This work included Bayesian nonparametrics; Gaussian processes; inference algorithms for big data; and work on machine translation.; ,['N/A'],Professor Yarin Gal 
yee-whye-teh,['N/A'],Yee Whye Teh is a Professor of Statistical Machine Learning at the University of Oxford; an Alan Turing Institute Faculty Fellow and a Research Scientist at Google DeepMind.  He obtained his Ph.D. at the University of Toronto; and did postdoctoral work at the University of California at Berkeley and National University of Singapore (as Lee Kuan Yew Postdoctoral Fellow).  He was a Lecturer then a Reader at the Gatsby Computational Neuroscience Unit; UCL prior to his current appointment. ; At The Alan Turing Institute; he is interested in investigating novel scalable machine learning and computational statistics methods; in particular in the frameworks of probabilistic models; Bayesian nonparametrics and deep learning. He is interested in applications to machine perception; recommendation systems; genetics and genomics.; ,At The Alan Turing Institute; he is interested in investigating novel scalable machine learning and computational statistics methods; in particular in the frameworks of probabilistic models; Bayesian nonparametrics and deep learning. He is interested in applications to machine perception; recommendation systems; genetics and genomics.; ,['N/A'],Professor Yee Whye Teh 
yi-yu,Social networks; Estimation theory; Time series; Probability; Modelling (Statistical methods & theory); Non-parametric & semi-parametric methods; Information theory (Statistical methods & theory); Asymptotic (Statistical methods & theory); High dimensional inference; Neuroscience; Applications (Machine learning); ,Yi Yu is an Associate Professor in the Department of Statistics; University of Warwick and a Turing Fellow at the Alan Turing Institute; previously a Lecturer in the University of Bristol; a postdoc of Professor Richard Samworth and a graduate student of Professor Zhiliang Ying. She obtained my academic degrees from Fudan University (B.S. in Mathematics; July 2009 and Ph.D. in Mathematical Statistics; July 2013). ; Yi's research interests are high-dimensional statistics and network studies. She has been working on theoretical; methodological and computational aspects of variable selection; post-selection inference and tuning parameter selection in regression problems; survival analysis; network analysis; and time series analysis.; ,Yi's research interests are high-dimensional statistics and network studies. She has been working on theoretical; methodological and computational aspects of variable selection; post-selection inference and tuning parameter selection in regression problems; survival analysis; network analysis; and time series analysis.; ,['N/A'],Dr Yi Yu 
zoe-kourtzi,Machine learning; Cognitive science; ,Zoe received her BSc in Experimental Psychology from the University of Crete; Greece and my PhD in Cognitive Neuroscience from Rutgers University; USA. She continued her postdoctoral work at MIT and Harvard in cognitive neuroimaging focusing on the brain computations that support our ability to interpret sensory information and make successful decisions. Upon the award of a Mc Donnell Pew fellowship; she moved to the Max Planck Institute for Biological Cybernetics; Tuebingen; Germany and then to the University of Birmingham; as a Chair in Brain Imaging. In 2013; she was appointed Chair of Experimental Psychology at the University of Cambridge.; Predicting individual socio-cognitive health is of high priority for health economies; given the translational potential for early diagnosis and personalised treatment. Yet; predicting intervention outcomes in health and society is challenged by variability across individuals. Zoe's work addresses this challenge by developing predictive models based on machine learning approaches that synthesise large-scale multivariate and longitudinal data to characterise individualised profiles of health across the lifespan. This work has strong translational applications in education and healthcare for the design of a) practical cost-effective tools for early diagnosis of decline and disease progression in clinical practice; b) training programmes tailored to individual needs.; ,Predicting individual socio-cognitive health is of high priority for health economies; given the translational potential for early diagnosis and personalised treatment. Yet; predicting intervention outcomes in health and society is challenged by variability across individuals. Zoe's work addresses this challenge by developing predictive models based on machine learning approaches that synthesise large-scale multivariate and longitudinal data to characterise individualised profiles of health across the lifespan. This work has strong translational applications in education and healthcare for the design of a) practical cost-effective tools for early diagnosis of decline and disease progression in clinical practice; b) training programmes tailored to individual needs.; ,['N/A'],Professor Zoe Kourtzi 
zoubin-ghahramani,Artificial intelligence; Machine learning; Uncertainty quantification; ,Zoubin Ghahramani is Professor of Information Engineering at the University of Cambridge and Chief Scientist at Uber. He is also Deputy Director of the Leverhulme Centre for the Future of Intelligence; and a Fellow of St John's College. He was a founding Cambridge Director of the Alan Turing Institute; the UK’s national institute for data science. He has worked and studied at the University of Pennsylvania; MIT; the University of Toronto; the Gatsby Unit at University College London; and Carnegie Mellon University. His research focuses on probabilistic approaches to machine learning and artificial intelligence; and he has published over 250 research papers on these topics. He was co-founder of Geometric Intelligence (now Uber AI Labs) and advises a number of AI and machine learning companies. In 2015; he was elected a Fellow of the Royal Society for his contributions to machine learning.; His current research interests include Bayesian approaches to machine learning; artificial intelligence; statistics; and data science. Statistics provides the mathematical foundations for handling uncertainty; making decisions; and designing learning systems. I have recently worked on Gaussian processes; non-parametric Bayesian methods; clustering; approximate inference algorithms; graphical models; Monte Carlo methods; semi-supervised learning; probabilistic programming; and building an Automatic Statistician.; ,His current research interests include Bayesian approaches to machine learning; artificial intelligence; statistics; and data science. Statistics provides the mathematical foundations for handling uncertainty; making decisions; and designing learning systems. I have recently worked on Gaussian processes; non-parametric Bayesian methods; clustering; approximate inference algorithms; graphical models; Monte Carlo methods; semi-supervised learning; probabilistic programming; and building an Automatic Statistician.; ,['N/A'],Professor Zoubin Ghahramani 
